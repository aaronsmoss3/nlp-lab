{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymongo in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (3.8.0)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bson import json_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('s3:nlp-kc-learning/train.csv', names=[\"Label\", \"Title\", \"Review\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1 = train[train.Label==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2 = train1['Review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train3 = train2.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('s3:nlp-kc-learning/test.csv', names=[\"Label\", \"Title\", \"Review\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = test.Review[test.Label==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2=test1.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Title</th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Batteries died within a year ...</td>\n",
       "      <td>I bought this charger in Jul 2003 and it worke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>DVD Player crapped out after one year</td>\n",
       "      <td>I also began having the incorrect disc problem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>Incorrect Disc</td>\n",
       "      <td>I love the style of this, but after a couple y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>DVD menu select problems</td>\n",
       "      <td>I cannot scroll through a DVD menu that is set...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>Not an \"ultimate guide\"</td>\n",
       "      <td>Firstly,I enjoyed the format and tone of the b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>Not!</td>\n",
       "      <td>If you want to listen to El Duke , then it is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>A complete Bust</td>\n",
       "      <td>This game requires quicktime 5.0 to work...if ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>didn't run off of USB bus power</td>\n",
       "      <td>Was hoping that this drive would run off of bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>Don't buy!</td>\n",
       "      <td>First of all, the company took my money and se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>Long and boring</td>\n",
       "      <td>I've read this book with much expectation, it ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>Dont like it</td>\n",
       "      <td>This product smells when you open the package ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>Don't Take the Chance - Get the SE Branded Cable</td>\n",
       "      <td>If you purchase this data cable, you need to k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>Waste of money!</td>\n",
       "      <td>Like many of the Barbie CD Roms, the playtime ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>Has No Range</td>\n",
       "      <td>I suppose if you were going to sit in the same...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>Three Days of Use and It Broke</td>\n",
       "      <td>Very disappointed in this product. It worked p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1</td>\n",
       "      <td>Not as expected...</td>\n",
       "      <td>My children get easily bored with this video. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1</td>\n",
       "      <td>Doublecharged for shipping because merchant wa...</td>\n",
       "      <td>Merchant was out of stock on the second pair o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1</td>\n",
       "      <td>Light reading, light in substance</td>\n",
       "      <td>A clich√©d storyline based on the classic Cyran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1</td>\n",
       "      <td>Great Book--unacceptable condition</td>\n",
       "      <td>I was looking forward to receiving this book, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1</td>\n",
       "      <td>More Romance Please and Less Mystery!!</td>\n",
       "      <td>the reason i gave two stars when I usually giv...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Label                                              Title  \\\n",
       "2       1                   Batteries died within a year ...   \n",
       "5       1              DVD Player crapped out after one year   \n",
       "6       1                                     Incorrect Disc   \n",
       "7       1                           DVD menu select problems   \n",
       "9       1                            Not an \"ultimate guide\"   \n",
       "11      1                                               Not!   \n",
       "12      1                                    A complete Bust   \n",
       "14      1                    didn't run off of USB bus power   \n",
       "15      1                                         Don't buy!   \n",
       "20      1                                    Long and boring   \n",
       "21      1                                       Dont like it   \n",
       "24      1   Don't Take the Chance - Get the SE Branded Cable   \n",
       "25      1                                    Waste of money!   \n",
       "27      1                                       Has No Range   \n",
       "29      1                     Three Days of Use and It Broke   \n",
       "35      1                                 Not as expected...   \n",
       "37      1  Doublecharged for shipping because merchant wa...   \n",
       "39      1                  Light reading, light in substance   \n",
       "41      1                 Great Book--unacceptable condition   \n",
       "46      1             More Romance Please and Less Mystery!!   \n",
       "\n",
       "                                               Review  \n",
       "2   I bought this charger in Jul 2003 and it worke...  \n",
       "5   I also began having the incorrect disc problem...  \n",
       "6   I love the style of this, but after a couple y...  \n",
       "7   I cannot scroll through a DVD menu that is set...  \n",
       "9   Firstly,I enjoyed the format and tone of the b...  \n",
       "11  If you want to listen to El Duke , then it is ...  \n",
       "12  This game requires quicktime 5.0 to work...if ...  \n",
       "14  Was hoping that this drive would run off of bu...  \n",
       "15  First of all, the company took my money and se...  \n",
       "20  I've read this book with much expectation, it ...  \n",
       "21  This product smells when you open the package ...  \n",
       "24  If you purchase this data cable, you need to k...  \n",
       "25  Like many of the Barbie CD Roms, the playtime ...  \n",
       "27  I suppose if you were going to sit in the same...  \n",
       "29  Very disappointed in this product. It worked p...  \n",
       "35  My children get easily bored with this video. ...  \n",
       "37  Merchant was out of stock on the second pair o...  \n",
       "39  A clich√©d storyline based on the classic Cyran...  \n",
       "41  I was looking forward to receiving this book, ...  \n",
       "46  the reason i gave two stars when I usually giv...  "
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2a.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "container = get_image_uri(boto3.Session().region_name, 'ntm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "sess = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set location s3://nlp-kc-learning/moss/val\n",
      "Trained model will be saved at s3://nlp-kc-learning/moss/output\n"
     ]
    }
   ],
   "source": [
    "bucket = 'nlp-kc-learning'\n",
    "prefix = 'moss'\n",
    "\n",
    "val_prefix = os.path.join(prefix, 'val')\n",
    "output_prefix = os.path.join(prefix, 'output')\n",
    "\n",
    "s3_val_data = os.path.join('s3://', bucket, val_prefix)\n",
    "output_path = os.path.join('s3://', bucket, output_prefix)\n",
    "print('Validation set location', s3_val_data)\n",
    "print('Trained model will be saved at', output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2.to_csv(\"train_neg.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '956526727A76863D',\n",
       "  'HostId': '1lQyJMhfiP4sxMEhxObnzgvPG+ngeDjv47DnLn3J6fcnG/sv3z2IZIA+mVRfUKw5zc+DBFmZGy8=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': '1lQyJMhfiP4sxMEhxObnzgvPG+ngeDjv47DnLn3J6fcnG/sv3z2IZIA+mVRfUKw5zc+DBFmZGy8=',\n",
       "   'x-amz-request-id': '956526727A76863D',\n",
       "   'date': 'Thu, 25 Apr 2019 15:44:05 GMT',\n",
       "   'etag': '\"d41d8cd98f00b204e9800998ecf8427e\"',\n",
       "   'content-length': '0',\n",
       "   'server': 'AmazonS3'},\n",
       "  'RetryAttempts': 0},\n",
       " 'ETag': '\"d41d8cd98f00b204e9800998ecf8427e\"'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from io import StringIO\n",
    "bucket = 'nlp-kc-learning'\n",
    "csv_buffer=StringIO()\n",
    "s3_resource = boto3.resource('s3')\n",
    "s3_resource.Object(bucket, 'train_neg.csv').put(Body=csv_buffer.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comprehend experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comprehend = boto3.client('comprehend')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentiment = comprehend.start_topics_detection_job(InputDataConfig=str(train.Review.values))\n",
    "#detect_sentiment(Text=str(sample.content.values), LanguageCode='en')\n",
    "print(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_s3_url = \"s3://nlp-kc-learning/train_neg.csv\"\n",
    "input_doc_format = \"ONE_DOC_PER_LINE\"\n",
    "output_s3_url = \"s3://nlp-kc-learning/moss/\"\n",
    "data_access_role_arn = \"arn:aws:iam::023375022819:role/service-role/AmazonComprehendServiceRole-mark\"\n",
    "number_of_topics = 10\n",
    "\n",
    "input_data_config = {\"S3Uri\": input_s3_url, \"InputFormat\": input_doc_format}\n",
    "output_data_config = {\"S3Uri\": output_s3_url}\n",
    "\n",
    "start_topics_detection_job_result = \\\n",
    "    comprehend.start_topics_detection_job(NumberOfTopics=number_of_topics,\n",
    "                                          JobName=\"moss_topics_detection\",\n",
    "                                          InputDataConfig=input_data_config,\n",
    "                                          OutputDataConfig=output_data_config,\n",
    "                                          DataAccessRoleArn=data_access_role_arn)\n",
    "\n",
    "print('start_topics_detection_job_result: ' + json.dumps(start_topics_detection_job_result))\n",
    " \n",
    "job_id = start_topics_detection_job_result[\"JobId\"]\n",
    " \n",
    "print('job_id: ' + job_id)\n",
    "print('\\n\\n\\n')\n",
    " \n",
    "describe_topics_detection_job_result = comprehend.describe_topics_detection_job(JobId=job_id)\n",
    " \n",
    "print('describe_topics_detection_job_result: ' + json.dumps(describe_topics_detection_job_result, default=json_util.default))\n",
    "print('\\n\\n\\n')\n",
    "\n",
    "list_topics_detection_jobs_result = comprehend.list_topics_detection_jobs()\n",
    " \n",
    "print('list_topics_detection_jobs_result: ' + json.dumps(list_topics_detection_jobs_result, default=json_util.default))\n",
    "print('\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_topics_detection_jobs_result.items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (3.3)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from nltk) (1.11.0)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import re\n",
    "token_pattern = re.compile(r\"(?u)\\b\\w\\w+\\b\")\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc) if len(t) >= 2 and re.match(\"[a-z].*\",t) \n",
    "                and re.match(token_pattern, t)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing and counting, this may take a few minutes...\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vocab_size = 2000\n",
    "print('Tokenizing and counting, this may take a few minutes...')\n",
    "start_time = time.time()\n",
    "vectorizer = CountVectorizer(input='content', analyzer='word', stop_words='english',\n",
    "                             tokenizer=LemmaTokenizer(), max_features=vocab_size, max_df=0.95, min_df=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 2000\n",
      "Done. Time elapsed: 1871.16s\n"
     ]
    }
   ],
   "source": [
    "## training set\n",
    "vectors = vectorizer.fit_transform(train3)\n",
    "vocab_list = vectorizer.get_feature_names()\n",
    "print('vocab size:', len(vocab_list))\n",
    "\n",
    "print('Done. Time elapsed: {:.2f}s'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random shuffle\n",
    "idx = np.arange(vectors.shape[0])\n",
    "np.random.shuffle(idx)\n",
    "vectors = vectors[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'> float32\n"
     ]
    }
   ],
   "source": [
    "import scipy.sparse as sparse\n",
    "vectors = sparse.csr_matrix(vectors, dtype=np.float32)\n",
    "print(type(vectors), vectors.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 2000\n",
      "Done. Time elapsed: 2275.61s\n"
     ]
    }
   ],
   "source": [
    "## test set\n",
    "vectors1 = vectorizer.fit_transform(test2)\n",
    "vocab_list1 = vectorizer.get_feature_names()\n",
    "print('vocab size:', len(vocab_list1))\n",
    "\n",
    "print('Done. Time elapsed: {:.2f}s'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200000"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors1.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'> float32\n"
     ]
    }
   ],
   "source": [
    "vectors_tst = sparse.csr_matrix(vectors1, dtype=np.float32)\n",
    "print(type(vectors_tst), vectors_tst.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = int(0.8 * vectors.shape[0])\n",
    "\n",
    "# split train and test\n",
    "train_vectors = vectors[:n_train, :]\n",
    "test_vectors = vectors[n_train:, :]\n",
    "\n",
    "# further split test set into validation set (val_vectors) and test  set (test_vectors)\n",
    "n_test = test_vectors.shape[0]\n",
    "val_vectors = test_vectors[:n_test//2, :]\n",
    "test_vectors = test_vectors[n_test//2:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1440000, 2000) (180000, 2000) (180000, 2000)\n"
     ]
    }
   ],
   "source": [
    "print(train_vectors.shape, test_vectors.shape, val_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'nlp-kc-learning'\n",
    "prefix = 'moss'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set location s3://nlp-kc-learning/moss/train\n",
      "Validation set location s3://nlp-kc-learning/moss/val\n",
      "Trained model will be saved at s3://nlp-kc-learning/moss/output\n"
     ]
    }
   ],
   "source": [
    "train_prefix = os.path.join(prefix, 'train')\n",
    "\n",
    "s3_train_data = os.path.join('s3://', bucket, train_prefix)\n",
    "\n",
    "print('Training set location', s3_train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_convert_upload(sparray, bucket, prefix, fname_template='data_part{}.pbr', n_parts=2):\n",
    "    import io\n",
    "    import boto3\n",
    "    import sagemaker.amazon.common as smac\n",
    "    \n",
    "    chunk_size = sparray.shape[0]// n_parts\n",
    "    for i in range(n_parts):\n",
    "\n",
    "        # Calculate start and end indices\n",
    "        start = i*chunk_size\n",
    "        end = (i+1)*chunk_size\n",
    "        if i+1 == n_parts:\n",
    "            end = sparray.shape[0]\n",
    "        \n",
    "        # Convert to record protobuf\n",
    "        buf = io.BytesIO()\n",
    "        smac.write_spmatrix_to_sparse_tensor(array=sparray[start:end], file=buf, labels=None)\n",
    "        buf.seek(0)\n",
    "        \n",
    "        # Upload to s3 location specified by bucket and prefix\n",
    "        fname = os.path.join(prefix, fname_template.format(i))\n",
    "        boto3.resource('s3').Bucket(bucket).Object(fname).upload_fileobj(buf)\n",
    "        print('Uploaded data to s3://{}'.format(os.path.join(bucket, fname)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded data to s3://nlp-kc-learning/moss/train/train_part0.pbr\n",
      "Uploaded data to s3://nlp-kc-learning/moss/train/train_part1.pbr\n",
      "Uploaded data to s3://nlp-kc-learning/moss/train/train_part2.pbr\n",
      "Uploaded data to s3://nlp-kc-learning/moss/train/train_part3.pbr\n",
      "Uploaded data to s3://nlp-kc-learning/moss/train/train_part4.pbr\n",
      "Uploaded data to s3://nlp-kc-learning/moss/train/train_part5.pbr\n",
      "Uploaded data to s3://nlp-kc-learning/moss/train/train_part6.pbr\n",
      "Uploaded data to s3://nlp-kc-learning/moss/train/train_part7.pbr\n",
      "Uploaded data to s3://nlp-kc-learning/moss/val/val_part0.pbr\n"
     ]
    }
   ],
   "source": [
    "split_convert_upload(train_vectors, bucket=bucket, prefix=train_prefix, fname_template='train_part{}.pbr', n_parts=8)\n",
    "split_convert_upload(val_vectors, bucket=bucket, prefix=val_prefix, fname_template='val_part{}.pbr', n_parts=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "container = get_image_uri(boto3.Session().region_name, 'ntm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntm = sagemaker.estimator.Estimator(container,\n",
    "                                    role, \n",
    "                                    base_job_name= 'aaronmossjob',\n",
    "                                    train_instance_count=3, \n",
    "                                    train_instance_type='ml.c4.xlarge',\n",
    "                                    output_path=output_path,\n",
    "                                    sagemaker_session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 10\n",
    "ntm.set_hyperparameters(num_topics=num_topics, feature_dim=vocab_size, mini_batch_size=128, \n",
    "                        epochs=25, num_patience_epochs=5, tolerance=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.session import s3_input\n",
    "s3_train = s3_input(s3_train_data, distribution='ShardedByS3Key') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: aaronmossjob-2019-04-25-16-30-31-576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-25 16:30:31 Starting - Starting the training job...\n",
      "2019-04-25 16:30:33 Starting - Launching requested ML instances......\n",
      "2019-04-25 16:31:45 Starting - Preparing the instances for training.........\n",
      "2019-04-25 16:33:24 Downloading - Downloading input data..\n",
      "\u001b[32mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[31mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:33:45 INFO 140590214039360] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/default-input.json: {u'num_patience_epochs': u'3', u'clip_gradient': u'Inf', u'encoder_layers': u'auto', u'optimizer': u'adadelta', u'_kvstore': u'auto_gpu', u'rescale_gradient': u'1.0', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'learning_rate': u'0.01', u'_data_format': u'record', u'sub_sample': u'1.0', u'epochs': u'50', u'weight_decay': u'0.0', u'_num_kv_servers': u'auto', u'encoder_layers_activation': u'sigmoid', u'mini_batch_size': u'256', u'tolerance': u'0.001', u'batch_norm': u'false'}\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:33:45 INFO 140590214039360] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'num_patience_epochs': u'5', u'num_topics': u'10', u'epochs': u'25', u'feature_dim': u'2000', u'mini_batch_size': u'128', u'tolerance': u'0.001'}\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:33:45 INFO 140590214039360] Final configuration: {u'optimizer': u'adadelta', u'rescale_gradient': u'1.0', u'_tuning_objective_metric': u'', u'learning_rate': u'0.01', u'clip_gradient': u'Inf', u'feature_dim': u'2000', u'encoder_layers_activation': u'sigmoid', u'_num_kv_servers': u'auto', u'weight_decay': u'0.0', u'num_patience_epochs': u'5', u'epochs': u'25', u'mini_batch_size': u'128', u'num_topics': u'10', u'_num_gpus': u'auto', u'_data_format': u'record', u'sub_sample': u'1.0', u'_kvstore': u'auto_gpu', u'encoder_layers': u'auto', u'tolerance': u'0.001', u'batch_norm': u'false'}\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:33:45 INFO 140590214039360] nvidia-smi took: 0.0252339839935 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:33:45 INFO 139933765801792] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/default-input.json: {u'num_patience_epochs': u'3', u'clip_gradient': u'Inf', u'encoder_layers': u'auto', u'optimizer': u'adadelta', u'_kvstore': u'auto_gpu', u'rescale_gradient': u'1.0', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'learning_rate': u'0.01', u'_data_format': u'record', u'sub_sample': u'1.0', u'epochs': u'50', u'weight_decay': u'0.0', u'_num_kv_servers': u'auto', u'encoder_layers_activation': u'sigmoid', u'mini_batch_size': u'256', u'tolerance': u'0.001', u'batch_norm': u'false'}\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:33:45 INFO 139933765801792] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'num_patience_epochs': u'5', u'num_topics': u'10', u'epochs': u'25', u'feature_dim': u'2000', u'mini_batch_size': u'128', u'tolerance': u'0.001'}\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:33:45 INFO 139933765801792] Final configuration: {u'optimizer': u'adadelta', u'rescale_gradient': u'1.0', u'_tuning_objective_metric': u'', u'learning_rate': u'0.01', u'clip_gradient': u'Inf', u'feature_dim': u'2000', u'encoder_layers_activation': u'sigmoid', u'_num_kv_servers': u'auto', u'weight_decay': u'0.0', u'num_patience_epochs': u'5', u'epochs': u'25', u'mini_batch_size': u'128', u'num_topics': u'10', u'_num_gpus': u'auto', u'_data_format': u'record', u'sub_sample': u'1.0', u'_kvstore': u'auto_gpu', u'encoder_layers': u'auto', u'tolerance': u'0.001', u'batch_norm': u'false'}\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:33:45 INFO 139933765801792] nvidia-smi took: 0.0252001285553 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:33:45 INFO 139933765801792] Launching parameter server for role scheduler\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:33:45 INFO 139933765801792] {'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/386aef2b-c1ed-4e45-8cb3-530e486d096a', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python2.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'ethwe', 'SHLVL': '1', 'AWS_REGION': 'us-east-1', 'NVIDIA_VISIBLE_DEVICES': 'all', 'TRAINING_JOB_NAME': 'aaronmossjob-2019-04-25-16-30-31-576', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '2', 'HOSTNAME': 'aws', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/1442ceb8-c9ef-4c09-b789-4eae407e1145', 'PWD': '/', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:33:45 INFO 139933765801792] envs={'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/386aef2b-c1ed-4e45-8cb3-530e486d096a', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'DMLC_NUM_WORKER': '3', 'DMLC_PS_ROOT_PORT': '9000', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python2.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'ethwe', 'SHLVL': '1', 'DMLC_PS_ROOT_URI': '10.40.0.4', 'AWS_REGION': 'us-east-1', 'NVIDIA_VISIBLE_DEVICES': 'all', 'TRAINING_JOB_NAME': 'aaronmossjob-2019-04-25-16-30-31-576', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '2', 'HOSTNAME': 'aws', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/1442ceb8-c9ef-4c09-b789-4eae407e1145', 'DMLC_ROLE': 'scheduler', 'PWD': '/', 'DMLC_NUM_SERVER': '3', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:33:45 INFO 139933765801792] Launching parameter server for role server\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:33:45 INFO 139933765801792] {'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/386aef2b-c1ed-4e45-8cb3-530e486d096a', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python2.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'ethwe', 'SHLVL': '1', 'AWS_REGION': 'us-east-1', 'NVIDIA_VISIBLE_DEVICES': 'all', 'TRAINING_JOB_NAME': 'aaronmossjob-2019-04-25-16-30-31-576', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '2', 'HOSTNAME': 'aws', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/1442ceb8-c9ef-4c09-b789-4eae407e1145', 'PWD': '/', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:33:45 INFO 139933765801792] envs={'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/386aef2b-c1ed-4e45-8cb3-530e486d096a', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'DMLC_NUM_WORKER': '3', 'DMLC_PS_ROOT_PORT': '9000', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python2.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'ethwe', 'SHLVL': '1', 'DMLC_PS_ROOT_URI': '10.40.0.4', 'AWS_REGION': 'us-east-1', 'NVIDIA_VISIBLE_DEVICES': 'all', 'TRAINING_JOB_NAME': 'aaronmossjob-2019-04-25-16-30-31-576', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '2', 'HOSTNAME': 'aws', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/1442ceb8-c9ef-4c09-b789-4eae407e1145', 'DMLC_ROLE': 'server', 'PWD': '/', 'DMLC_NUM_SERVER': '3', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:33:45 INFO 139933765801792] Environment: {'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/386aef2b-c1ed-4e45-8cb3-530e486d096a', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'DMLC_PS_ROOT_PORT': '9000', 'DMLC_NUM_WORKER': '3', 'SAGEMAKER_HTTP_PORT': '8080', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python2.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'ethwe', 'SHLVL': '1', 'DMLC_PS_ROOT_URI': '10.40.0.4', 'AWS_REGION': 'us-east-1', 'NVIDIA_VISIBLE_DEVICES': 'all', 'TRAINING_JOB_NAME': 'aaronmossjob-2019-04-25-16-30-31-576', 'HOME': '/root', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '2', 'HOSTNAME': 'aws', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/1442ceb8-c9ef-4c09-b789-4eae407e1145', 'DMLC_ROLE': 'worker', 'PWD': '/', 'DMLC_NUM_SERVER': '3', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[31mProcess 40 is a shell:scheduler.\u001b[0m\n",
      "\u001b[31mProcess 41 is a shell:server.\u001b[0m\n",
      "\u001b[31mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:33:45 INFO 139933765801792] Using default worker.\u001b[0m\n",
      "\u001b[31m[2019-04-25 16:33:46.020] [tensorio] [info] batch={\"data_pipeline\": \"/opt/ml/input/data/train\", \"num_examples\": 128, \"features\": [{\"name\": \"values\", \"shape\": [2000], \"storage_type\": \"CSR\"}]}\u001b[0m\n",
      "\u001b[31m[2019-04-25 16:33:46.038] [tensorio] [warning] TensorIO is already initialized; ignoring the initialization routine.\u001b[0m\n",
      "\u001b[31m[2019-04-25 16:33:46.056] [tensorio] [info] batch={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"num_examples\": 128, \"features\": [{\"name\": \"values\", \"shape\": [2000], \"storage_type\": \"CSR\"}]}\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:33:46 INFO 139933765801792] Initializing\u001b[0m\n",
      "\u001b[31m/opt/amazon/lib/python2.7/site-packages/ai_algorithms_sdk/config/config_helper.py:122: DeprecationWarning: deprecated\n",
      "  warnings.warn(\"deprecated\", DeprecationWarning)\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:33:46 INFO 139933765801792] None\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:33:46 INFO 139933765801792] vocab.txt\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:33:46 INFO 139933765801792] Vocab file is not provided\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:33:46 INFO 139933765801792] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:33:46 INFO 139933765801792] Create Store: dist_async\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:33:45 INFO 140590214039360] Launching parameter server for role server\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:33:45 INFO 140590214039360] {'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/12b5e05a-0826-4f2d-81a2-d1324204b25b', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python2.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'ethwe', 'SHLVL': '1', 'AWS_REGION': 'us-east-1', 'NVIDIA_VISIBLE_DEVICES': 'all', 'TRAINING_JOB_NAME': 'aaronmossjob-2019-04-25-16-30-31-576', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '2', 'HOSTNAME': 'aws', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/8a0b9660-9885-40d2-9ce0-d34b77909648', 'PWD': '/', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:33:45 INFO 140590214039360] envs={'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/12b5e05a-0826-4f2d-81a2-d1324204b25b', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'DMLC_NUM_WORKER': '3', 'DMLC_PS_ROOT_PORT': '9000', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python2.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'ethwe', 'SHLVL': '1', 'DMLC_PS_ROOT_URI': '10.40.0.4', 'AWS_REGION': 'us-east-1', 'NVIDIA_VISIBLE_DEVICES': 'all', 'TRAINING_JOB_NAME': 'aaronmossjob-2019-04-25-16-30-31-576', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '2', 'HOSTNAME': 'aws', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/8a0b9660-9885-40d2-9ce0-d34b77909648', 'DMLC_ROLE': 'server', 'PWD': '/', 'DMLC_NUM_SERVER': '3', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:33:45 INFO 140590214039360] Environment: {'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/12b5e05a-0826-4f2d-81a2-d1324204b25b', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'DMLC_PS_ROOT_PORT': '9000', 'DMLC_NUM_WORKER': '3', 'SAGEMAKER_HTTP_PORT': '8080', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python2.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'ethwe', 'SHLVL': '1', 'DMLC_PS_ROOT_URI': '10.40.0.4', 'AWS_REGION': 'us-east-1', 'NVIDIA_VISIBLE_DEVICES': 'all', 'TRAINING_JOB_NAME': 'aaronmossjob-2019-04-25-16-30-31-576', 'HOME': '/root', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '2', 'HOSTNAME': 'aws', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/8a0b9660-9885-40d2-9ce0-d34b77909648', 'DMLC_ROLE': 'worker', 'PWD': '/', 'DMLC_NUM_SERVER': '3', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[32mProcess 40 is a shell:server.\u001b[0m\n",
      "\u001b[32mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:33:45 INFO 140590214039360] Using default worker.\u001b[0m\n",
      "\u001b[32m[2019-04-25 16:33:45.760] [tensorio] [info] batch={\"data_pipeline\": \"/opt/ml/input/data/train\", \"num_examples\": 128, \"features\": [{\"name\": \"values\", \"shape\": [2000], \"storage_type\": \"CSR\"}]}\u001b[0m\n",
      "\u001b[32m[2019-04-25 16:33:45.764] [tensorio] [warning] TensorIO is already initialized; ignoring the initialization routine.\u001b[0m\n",
      "\u001b[32m[2019-04-25 16:33:45.767] [tensorio] [info] batch={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"num_examples\": 128, \"features\": [{\"name\": \"values\", \"shape\": [2000], \"storage_type\": \"CSR\"}]}\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:33:45 INFO 140590214039360] Initializing\u001b[0m\n",
      "\u001b[32m/opt/amazon/lib/python2.7/site-packages/ai_algorithms_sdk/config/config_helper.py:122: DeprecationWarning: deprecated\n",
      "  warnings.warn(\"deprecated\", DeprecationWarning)\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:33:45 INFO 140590214039360] None\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:33:45 INFO 140590214039360] vocab.txt\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:33:45 INFO 140590214039360] Vocab file is not provided\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:33:45 INFO 140590214039360] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:33:45 INFO 140590214039360] Create Store: dist_async\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2019-04-25 16:33:46 Training - Training image download completed. Training in progress.\u001b[32mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:33:48 INFO 139904211511104] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/default-input.json: {u'num_patience_epochs': u'3', u'clip_gradient': u'Inf', u'encoder_layers': u'auto', u'optimizer': u'adadelta', u'_kvstore': u'auto_gpu', u'rescale_gradient': u'1.0', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'learning_rate': u'0.01', u'_data_format': u'record', u'sub_sample': u'1.0', u'epochs': u'50', u'weight_decay': u'0.0', u'_num_kv_servers': u'auto', u'encoder_layers_activation': u'sigmoid', u'mini_batch_size': u'256', u'tolerance': u'0.001', u'batch_norm': u'false'}\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:33:48 INFO 139904211511104] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'num_patience_epochs': u'5', u'num_topics': u'10', u'epochs': u'25', u'feature_dim': u'2000', u'mini_batch_size': u'128', u'tolerance': u'0.001'}\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:33:48 INFO 139904211511104] Final configuration: {u'optimizer': u'adadelta', u'rescale_gradient': u'1.0', u'_tuning_objective_metric': u'', u'learning_rate': u'0.01', u'clip_gradient': u'Inf', u'feature_dim': u'2000', u'encoder_layers_activation': u'sigmoid', u'_num_kv_servers': u'auto', u'weight_decay': u'0.0', u'num_patience_epochs': u'5', u'epochs': u'25', u'mini_batch_size': u'128', u'num_topics': u'10', u'_num_gpus': u'auto', u'_data_format': u'record', u'sub_sample': u'1.0', u'_kvstore': u'auto_gpu', u'encoder_layers': u'auto', u'tolerance': u'0.001', u'batch_norm': u'false'}\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:33:48 INFO 139904211511104] nvidia-smi took: 0.0252349376678 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:33:48 INFO 139904211511104] Launching parameter server for role server\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:33:48 INFO 139904211511104] {'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/eb4d93dc-cc41-47d8-bfbb-14bf68096ee1', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python2.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'ethwe', 'SHLVL': '1', 'AWS_REGION': 'us-east-1', 'NVIDIA_VISIBLE_DEVICES': 'all', 'TRAINING_JOB_NAME': 'aaronmossjob-2019-04-25-16-30-31-576', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '2', 'HOSTNAME': 'aws', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/1cff5b1f-e404-4064-b8b1-7f0d1a15b09f', 'PWD': '/', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:33:48 INFO 139904211511104] envs={'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/eb4d93dc-cc41-47d8-bfbb-14bf68096ee1', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'DMLC_NUM_WORKER': '3', 'DMLC_PS_ROOT_PORT': '9000', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python2.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'ethwe', 'SHLVL': '1', 'DMLC_PS_ROOT_URI': '10.40.0.4', 'AWS_REGION': 'us-east-1', 'NVIDIA_VISIBLE_DEVICES': 'all', 'TRAINING_JOB_NAME': 'aaronmossjob-2019-04-25-16-30-31-576', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '2', 'HOSTNAME': 'aws', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/1cff5b1f-e404-4064-b8b1-7f0d1a15b09f', 'DMLC_ROLE': 'server', 'PWD': '/', 'DMLC_NUM_SERVER': '3', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:33:48 INFO 139904211511104] Environment: {'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/eb4d93dc-cc41-47d8-bfbb-14bf68096ee1', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'DMLC_PS_ROOT_PORT': '9000', 'DMLC_NUM_WORKER': '3', 'SAGEMAKER_HTTP_PORT': '8080', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python2.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'ethwe', 'SHLVL': '1', 'DMLC_PS_ROOT_URI': '10.40.0.4', 'AWS_REGION': 'us-east-1', 'NVIDIA_VISIBLE_DEVICES': 'all', 'TRAINING_JOB_NAME': 'aaronmossjob-2019-04-25-16-30-31-576', 'HOME': '/root', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '2', 'HOSTNAME': 'aws', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/1cff5b1f-e404-4064-b8b1-7f0d1a15b09f', 'DMLC_ROLE': 'worker', 'PWD': '/', 'DMLC_NUM_SERVER': '3', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[32mProcess 40 is a shell:server.\u001b[0m\n",
      "\u001b[32mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:33:48 INFO 139904211511104] Using default worker.\u001b[0m\n",
      "\u001b[32m[2019-04-25 16:33:48.510] [tensorio] [info] batch={\"data_pipeline\": \"/opt/ml/input/data/train\", \"num_examples\": 128, \"features\": [{\"name\": \"values\", \"shape\": [2000], \"storage_type\": \"CSR\"}]}\u001b[0m\n",
      "\u001b[32m[2019-04-25 16:33:48.514] [tensorio] [warning] TensorIO is already initialized; ignoring the initialization routine.\u001b[0m\n",
      "\u001b[32m[2019-04-25 16:33:48.516] [tensorio] [info] batch={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"num_examples\": 128, \"features\": [{\"name\": \"values\", \"shape\": [2000], \"storage_type\": \"CSR\"}]}\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:33:48 INFO 139904211511104] Initializing\u001b[0m\n",
      "\u001b[32m/opt/amazon/lib/python2.7/site-packages/ai_algorithms_sdk/config/config_helper.py:122: DeprecationWarning: deprecated\n",
      "  warnings.warn(\"deprecated\", DeprecationWarning)\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:33:48 INFO 139904211511104] None\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:33:48 INFO 139904211511104] vocab.txt\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:33:48 INFO 139904211511104] Vocab file is not provided\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:33:48 INFO 139904211511104] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:33:48 INFO 139904211511104] Create Store: dist_async\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}}, \"EndTime\": 1556210029.063696, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"init_train_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\"}, \"StartTime\": 1556210029.063661}\n",
      "\u001b[0m\n",
      "\u001b[31m[2019-04-25 16:33:49.067] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 0, \"duration\": 3063, \"num_examples\": 1}\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:33:49 INFO 139933765801792] \u001b[0m\n",
      "\u001b[31m[04/25/2019 16:33:49 INFO 139933765801792] # Starting training for epoch 1\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}}, \"EndTime\": 1556210029.070565, \"Dimensions\": {\"Host\": \"algo-3\", \"Meta\": \"init_train_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\"}, \"StartTime\": 1556210029.070529}\n",
      "\u001b[0m\n",
      "\u001b[33m[2019-04-25 16:33:49.072] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 0, \"duration\": 3312, \"num_examples\": 1}\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:33:49 INFO 140590214039360] \u001b[0m\n",
      "\u001b[33m[04/25/2019 16:33:49 INFO 140590214039360] # Starting training for epoch 1\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}}, \"EndTime\": 1556210029.063691, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"init_train_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\"}, \"StartTime\": 1556210029.063654}\n",
      "\u001b[0m\n",
      "\u001b[32m[2019-04-25 16:33:49.067] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 0, \"duration\": 557, \"num_examples\": 1}\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:33:49 INFO 139904211511104] \u001b[0m\n",
      "\u001b[32m[04/25/2019 16:33:49 INFO 139904211511104] # Starting training for epoch 1\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m[04/25/2019 16:35:18 INFO 140590214039360] # Finished training epoch 1 on 360000 examples from 2813 batches, each of size 128.\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:35:18 INFO 140590214039360] Metrics for Training:\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:35:18 INFO 140590214039360] Loss (name: value) total: 6.66461337092\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:35:18 INFO 140590214039360] Loss (name: value) kld: 0.0756919756483\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:35:18 INFO 140590214039360] Loss (name: value) recons: 6.58892139717\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:35:18 INFO 140590214039360] Loss (name: value) logppx: 6.66461337092\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:35:18 INFO 140590214039360] #quality_metric: host=algo-3, epoch=1, train total_loss <loss>=6.66461337092\u001b[0m\n",
      "\u001b[33m[2019-04-25 16:35:18.954] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 0, \"duration\": 93190, \"num_examples\": 1}\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:35:30 INFO 140590214039360] Finished scoring on 179968 examples from 1406 batches, each of size 128.\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:35:30 INFO 140590214039360] Metrics for Inference:\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:35:30 INFO 140590214039360] Loss (name: value) total: 6.57286319082\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:35:30 INFO 140590214039360] Loss (name: value) kld: 0.128846513052\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:35:30 INFO 140590214039360] Loss (name: value) recons: 6.44401667467\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:35:30 INFO 140590214039360] Loss (name: value) logppx: 6.57286319082\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:35:30 INFO 140590214039360] #validation_score (1): 6.57286319082\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:35:30 INFO 140590214039360] Timing: train: 89.88s, val: 11.78s, epoch: 101.66s\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:35:30 INFO 140590214039360] #progress_metric: host=algo-3, completed 4 % of epochs\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2813, \"sum\": 2813.0, \"min\": 2813}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2813, \"sum\": 2813.0, \"min\": 2813}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 360000, \"sum\": 360000.0, \"min\": 360000}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2813, \"sum\": 2813.0, \"min\": 2813}, \"Total Records Seen\": {\"count\": 1, \"max\": 360000, \"sum\": 360000.0, \"min\": 360000}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 360000, \"sum\": 360000.0, \"min\": 360000}, \"Reset Count\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}}, \"EndTime\": 1556210130.735181, \"Dimensions\": {\"Host\": \"algo-3\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 0}, \"StartTime\": 1556210029.073067}\n",
      "\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:35:30 INFO 140590214039360] #throughput_metric: host=algo-3, train throughput=3541.13677702 records/second\u001b[0m\n",
      "\u001b[33m[2019-04-25 16:35:30.735] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 1, \"duration\": 101662, \"num_examples\": 2813}\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:35:30 INFO 140590214039360] \u001b[0m\n",
      "\u001b[33m[04/25/2019 16:35:30 INFO 140590214039360] # Starting training for epoch 2\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:36:20 INFO 139933765801792] # Finished training epoch 1 on 540000 examples from 4219 batches, each of size 128.\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:36:20 INFO 139933765801792] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:36:20 INFO 139933765801792] Loss (name: value) total: 6.62219223826\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:36:20 INFO 139933765801792] Loss (name: value) kld: 0.105432273214\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:36:20 INFO 139933765801792] Loss (name: value) recons: 6.51675996172\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:36:20 INFO 139933765801792] Loss (name: value) logppx: 6.62219223826\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:36:20 INFO 139933765801792] #quality_metric: host=algo-1, epoch=1, train total_loss <loss>=6.62219223826\u001b[0m\n",
      "\u001b[31m[2019-04-25 16:36:20.248] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 0, \"duration\": 154209, \"num_examples\": 1}\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:36:21 INFO 139904211511104] # Finished training epoch 1 on 540000 examples from 4219 batches, each of size 128.\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:36:21 INFO 139904211511104] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:36:21 INFO 139904211511104] Loss (name: value) total: 6.62172186943\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:36:21 INFO 139904211511104] Loss (name: value) kld: 0.106396982081\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:36:21 INFO 139904211511104] Loss (name: value) recons: 6.51532488362\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:36:21 INFO 139904211511104] Loss (name: value) logppx: 6.62172186943\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:36:21 INFO 139904211511104] #quality_metric: host=algo-2, epoch=1, train total_loss <loss>=6.62172186943\u001b[0m\n",
      "\u001b[32m[2019-04-25 16:36:21.168] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 0, \"duration\": 152654, \"num_examples\": 1}\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:36:35 INFO 139933765801792] Finished scoring on 179968 examples from 1406 batches, each of size 128.\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:36:35 INFO 139933765801792] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:36:35 INFO 139933765801792] Loss (name: value) total: 6.55312740277\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:36:35 INFO 139933765801792] Loss (name: value) kld: 0.162598662576\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:36:35 INFO 139933765801792] Loss (name: value) recons: 6.39052874062\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:36:35 INFO 139933765801792] Loss (name: value) logppx: 6.55312740277\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:36:35 INFO 139933765801792] #validation_score (1): 6.55312740277\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:36:35 INFO 139933765801792] Timing: train: 151.18s, val: 15.38s, epoch: 166.56s\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:36:35 INFO 139933765801792] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 4219, \"sum\": 4219.0, \"min\": 4219}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 4219, \"sum\": 4219.0, \"min\": 4219}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 540000, \"sum\": 540000.0, \"min\": 540000}, \"Total Batches Seen\": {\"count\": 1, \"max\": 4219, \"sum\": 4219.0, \"min\": 4219}, \"Total Records Seen\": {\"count\": 1, \"max\": 540000, \"sum\": 540000.0, \"min\": 540000}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 540000, \"sum\": 540000.0, \"min\": 540000}, \"Reset Count\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}}, \"EndTime\": 1556210195.626702, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 0}, \"StartTime\": 1556210029.067248}\n",
      "\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:36:35 INFO 139933765801792] #throughput_metric: host=algo-1, train throughput=3242.08286949 records/second\u001b[0m\n",
      "\u001b[31m[2019-04-25 16:36:35.626] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 1, \"duration\": 166559, \"num_examples\": 4219}\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:36:35 INFO 139933765801792] \u001b[0m\n",
      "\u001b[31m[04/25/2019 16:36:35 INFO 139933765801792] # Starting training for epoch 2\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:36:35 INFO 139904211511104] Finished scoring on 179968 examples from 1406 batches, each of size 128.\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:36:35 INFO 139904211511104] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:36:35 INFO 139904211511104] Loss (name: value) total: 6.54013416438\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:36:35 INFO 139904211511104] Loss (name: value) kld: 0.161322022633\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:36:35 INFO 139904211511104] Loss (name: value) recons: 6.37881213808\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:36:35 INFO 139904211511104] Loss (name: value) logppx: 6.54013416438\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:36:35 INFO 139904211511104] #validation_score (1): 6.54013416438\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:36:35 INFO 139904211511104] Timing: train: 152.10s, val: 14.37s, epoch: 166.47s\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:36:35 INFO 139904211511104] #progress_metric: host=algo-2, completed 4 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 4219, \"sum\": 4219.0, \"min\": 4219}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 4219, \"sum\": 4219.0, \"min\": 4219}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 540000, \"sum\": 540000.0, \"min\": 540000}, \"Total Batches Seen\": {\"count\": 1, \"max\": 4219, \"sum\": 4219.0, \"min\": 4219}, \"Total Records Seen\": {\"count\": 1, \"max\": 540000, \"sum\": 540000.0, \"min\": 540000}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 540000, \"sum\": 540000.0, \"min\": 540000}, \"Reset Count\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}}, \"EndTime\": 1556210195.537964, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 0}, \"StartTime\": 1556210029.067919}\n",
      "\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:36:35 INFO 139904211511104] #throughput_metric: host=algo-2, train throughput=3243.8238369 records/second\u001b[0m\n",
      "\u001b[32m[2019-04-25 16:36:35.538] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 1, \"duration\": 166469, \"num_examples\": 4219}\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:36:35 INFO 139904211511104] \u001b[0m\n",
      "\u001b[32m[04/25/2019 16:36:35 INFO 139904211511104] # Starting training for epoch 2\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m[04/25/2019 16:36:57 INFO 140590214039360] # Finished training epoch 2 on 360000 examples from 2813 batches, each of size 128.\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:36:57 INFO 140590214039360] Metrics for Training:\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:36:57 INFO 140590214039360] Loss (name: value) total: 6.54893315436\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:36:57 INFO 140590214039360] Loss (name: value) kld: 0.157861502464\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:36:57 INFO 140590214039360] Loss (name: value) recons: 6.39107165242\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:36:57 INFO 140590214039360] Loss (name: value) logppx: 6.54893315436\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:36:57 INFO 140590214039360] #quality_metric: host=algo-3, epoch=2, train total_loss <loss>=6.54893315436\u001b[0m\n",
      "\u001b[33m[2019-04-25 16:36:57.745] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 1, \"duration\": 98790, \"num_examples\": 1407}\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:37:09 INFO 140590214039360] Finished scoring on 179968 examples from 1406 batches, each of size 128.\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:37:09 INFO 140590214039360] Metrics for Inference:\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:37:09 INFO 140590214039360] Loss (name: value) total: 6.53909864127\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:37:09 INFO 140590214039360] Loss (name: value) kld: 0.165852054053\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:37:09 INFO 140590214039360] Loss (name: value) recons: 6.37324659177\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:37:09 INFO 140590214039360] Loss (name: value) logppx: 6.53909864127\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:37:09 INFO 140590214039360] #validation_score (2): 6.53909864127\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:37:09 INFO 140590214039360] Timing: train: 87.01s, val: 11.66s, epoch: 98.67s\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:37:09 INFO 140590214039360] #progress_metric: host=algo-3, completed 8 % of epochs\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2813, \"sum\": 2813.0, \"min\": 2813}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2813, \"sum\": 2813.0, \"min\": 2813}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 360000, \"sum\": 360000.0, \"min\": 360000}, \"Total Batches Seen\": {\"count\": 1, \"max\": 5626, \"sum\": 5626.0, \"min\": 5626}, \"Total Records Seen\": {\"count\": 1, \"max\": 720000, \"sum\": 720000.0, \"min\": 720000}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 360000, \"sum\": 360000.0, \"min\": 360000}, \"Reset Count\": {\"count\": 1, \"max\": 4, \"sum\": 4.0, \"min\": 4}}, \"EndTime\": 1556210229.40317, \"Dimensions\": {\"Host\": \"algo-3\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 1}, \"StartTime\": 1556210130.735547}\n",
      "\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:37:09 INFO 140590214039360] #throughput_metric: host=algo-3, train throughput=3648.60667031 records/second\u001b[0m\n",
      "\u001b[33m[2019-04-25 16:37:09.403] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 2, \"duration\": 98667, \"num_examples\": 2813}\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:37:09 INFO 140590214039360] \u001b[0m\n",
      "\u001b[33m[04/25/2019 16:37:09 INFO 140590214039360] # Starting training for epoch 3\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:38:37 INFO 140590214039360] # Finished training epoch 3 on 360000 examples from 2813 batches, each of size 128.\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:38:37 INFO 140590214039360] Metrics for Training:\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:38:37 INFO 140590214039360] Loss (name: value) total: 6.53464115213\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:38:37 INFO 140590214039360] Loss (name: value) kld: 0.180394164741\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:38:37 INFO 140590214039360] Loss (name: value) recons: 6.35424698514\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:38:37 INFO 140590214039360] Loss (name: value) logppx: 6.53464115213\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:38:37 INFO 140590214039360] #quality_metric: host=algo-3, epoch=3, train total_loss <loss>=6.53464115213\u001b[0m\n",
      "\u001b[33m[2019-04-25 16:38:37.734] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 2, \"duration\": 99988, \"num_examples\": 1407}\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:38:49 INFO 140590214039360] Finished scoring on 179968 examples from 1406 batches, each of size 128.\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:38:49 INFO 140590214039360] Metrics for Inference:\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:38:49 INFO 140590214039360] Loss (name: value) total: 6.52896038329\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:38:49 INFO 140590214039360] Loss (name: value) kld: 0.174864543165\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:38:49 INFO 140590214039360] Loss (name: value) recons: 6.35409584188\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:38:49 INFO 140590214039360] Loss (name: value) logppx: 6.52896038329\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:38:49 INFO 140590214039360] #validation_score (3): 6.52896038329\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:38:49 INFO 140590214039360] Timing: train: 88.33s, val: 11.59s, epoch: 99.92s\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:38:49 INFO 140590214039360] #progress_metric: host=algo-3, completed 12 % of epochs\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2813, \"sum\": 2813.0, \"min\": 2813}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2813, \"sum\": 2813.0, \"min\": 2813}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 360000, \"sum\": 360000.0, \"min\": 360000}, \"Total Batches Seen\": {\"count\": 1, \"max\": 8439, \"sum\": 8439.0, \"min\": 8439}, \"Total Records Seen\": {\"count\": 1, \"max\": 1080000, \"sum\": 1080000.0, \"min\": 1080000}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 360000, \"sum\": 360000.0, \"min\": 360000}, \"Reset Count\": {\"count\": 1, \"max\": 6, \"sum\": 6.0, \"min\": 6}}, \"EndTime\": 1556210329.327801, \"Dimensions\": {\"Host\": \"algo-3\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 2}, \"StartTime\": 1556210229.403531}\n",
      "\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:38:49 INFO 140590214039360] #throughput_metric: host=algo-3, train throughput=3602.72254691 records/second\u001b[0m\n",
      "\u001b[33m[2019-04-25 16:38:49.328] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 3, \"duration\": 99924, \"num_examples\": 2813}\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:38:49 INFO 140590214039360] \u001b[0m\n",
      "\u001b[33m[04/25/2019 16:38:49 INFO 140590214039360] # Starting training for epoch 4\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:39:02 INFO 139933765801792] # Finished training epoch 2 on 540000 examples from 4219 batches, each of size 128.\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:39:02 INFO 139933765801792] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:39:02 INFO 139933765801792] Loss (name: value) total: 6.53532509601\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:39:02 INFO 139933765801792] Loss (name: value) kld: 0.178604359956\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:39:02 INFO 139933765801792] Loss (name: value) recons: 6.3567207362\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:39:02 INFO 139933765801792] Loss (name: value) logppx: 6.53532509601\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:39:02 INFO 139933765801792] #quality_metric: host=algo-1, epoch=2, train total_loss <loss>=6.53532509601\u001b[0m\n",
      "\u001b[31m[2019-04-25 16:39:02.933] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 1, \"duration\": 162684, \"num_examples\": 1407}\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:39:06 INFO 139904211511104] # Finished training epoch 2 on 540000 examples from 4219 batches, each of size 128.\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:39:06 INFO 139904211511104] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:39:06 INFO 139904211511104] Loss (name: value) total: 6.53629623349\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:39:06 INFO 139904211511104] Loss (name: value) kld: 0.17889285622\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:39:06 INFO 139904211511104] Loss (name: value) recons: 6.35740337578\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:39:06 INFO 139904211511104] Loss (name: value) logppx: 6.53629623349\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:39:06 INFO 139904211511104] #quality_metric: host=algo-2, epoch=2, train total_loss <loss>=6.53629623349\u001b[0m\n",
      "\u001b[32m[2019-04-25 16:39:06.955] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 1, \"duration\": 165786, \"num_examples\": 1407}\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:39:18 INFO 139933765801792] Finished scoring on 179968 examples from 1406 batches, each of size 128.\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:39:18 INFO 139933765801792] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:39:18 INFO 139933765801792] Loss (name: value) total: 6.52529416654\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:39:18 INFO 139933765801792] Loss (name: value) kld: 0.187944246899\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:39:18 INFO 139933765801792] Loss (name: value) recons: 6.3373499171\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:39:18 INFO 139933765801792] Loss (name: value) logppx: 6.52529416654\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:39:18 INFO 139933765801792] #validation_score (2): 6.52529416654\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:39:18 INFO 139933765801792] Timing: train: 147.31s, val: 15.30s, epoch: 162.61s\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:39:18 INFO 139933765801792] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 4219, \"sum\": 4219.0, \"min\": 4219}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 4219, \"sum\": 4219.0, \"min\": 4219}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 540000, \"sum\": 540000.0, \"min\": 540000}, \"Total Batches Seen\": {\"count\": 1, \"max\": 8438, \"sum\": 8438.0, \"min\": 8438}, \"Total Records Seen\": {\"count\": 1, \"max\": 1080000, \"sum\": 1080000.0, \"min\": 1080000}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 540000, \"sum\": 540000.0, \"min\": 540000}, \"Reset Count\": {\"count\": 1, \"max\": 4, \"sum\": 4.0, \"min\": 4}}, \"EndTime\": 1556210358.235436, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 1}, \"StartTime\": 1556210195.626967}\n",
      "\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:39:18 INFO 139933765801792] #throughput_metric: host=algo-1, train throughput=3320.85750714 records/second\u001b[0m\n",
      "\u001b[31m[2019-04-25 16:39:18.235] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 2, \"duration\": 162608, \"num_examples\": 4219}\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:39:18 INFO 139933765801792] \u001b[0m\n",
      "\u001b[31m[04/25/2019 16:39:18 INFO 139933765801792] # Starting training for epoch 3\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:39:21 INFO 139904211511104] Finished scoring on 179968 examples from 1406 batches, each of size 128.\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:39:21 INFO 139904211511104] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:39:21 INFO 139904211511104] Loss (name: value) total: 6.52869113919\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:39:21 INFO 139904211511104] Loss (name: value) kld: 0.186389992037\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:39:21 INFO 139904211511104] Loss (name: value) recons: 6.34230114623\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:39:21 INFO 139904211511104] Loss (name: value) logppx: 6.52869113919\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:39:21 INFO 139904211511104] #validation_score (2): 6.52869113919\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:39:21 INFO 139904211511104] Timing: train: 151.42s, val: 15.01s, epoch: 166.43s\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:39:21 INFO 139904211511104] #progress_metric: host=algo-2, completed 8 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 4219, \"sum\": 4219.0, \"min\": 4219}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 4219, \"sum\": 4219.0, \"min\": 4219}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 540000, \"sum\": 540000.0, \"min\": 540000}, \"Total Batches Seen\": {\"count\": 1, \"max\": 8438, \"sum\": 8438.0, \"min\": 8438}, \"Total Records Seen\": {\"count\": 1, \"max\": 1080000, \"sum\": 1080000.0, \"min\": 1080000}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 540000, \"sum\": 540000.0, \"min\": 540000}, \"Reset Count\": {\"count\": 1, \"max\": 4, \"sum\": 4.0, \"min\": 4}}, \"EndTime\": 1556210361.96649, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 1}, \"StartTime\": 1556210195.538258}\n",
      "\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:39:21 INFO 139904211511104] #throughput_metric: host=algo-2, train throughput=3244.63863209 records/second\u001b[0m\n",
      "\u001b[32m[2019-04-25 16:39:21.966] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 2, \"duration\": 166428, \"num_examples\": 4219}\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:39:21 INFO 139904211511104] \u001b[0m\n",
      "\u001b[32m[04/25/2019 16:39:21 INFO 139904211511104] # Starting training for epoch 3\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m[04/25/2019 16:40:15 INFO 140590214039360] # Finished training epoch 4 on 360000 examples from 2813 batches, each of size 128.\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:40:15 INFO 140590214039360] Metrics for Training:\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:40:15 INFO 140590214039360] Loss (name: value) total: 6.52853242113\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:40:15 INFO 140590214039360] Loss (name: value) kld: 0.187616210604\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:40:15 INFO 140590214039360] Loss (name: value) recons: 6.34091620957\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:40:15 INFO 140590214039360] Loss (name: value) logppx: 6.52853242113\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:40:15 INFO 140590214039360] #quality_metric: host=algo-3, epoch=4, train total_loss <loss>=6.52853242113\u001b[0m\n",
      "\u001b[33m[2019-04-25 16:40:15.445] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 3, \"duration\": 97710, \"num_examples\": 1407}\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:40:27 INFO 140590214039360] Finished scoring on 179968 examples from 1406 batches, each of size 128.\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:40:27 INFO 140590214039360] Metrics for Inference:\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:40:27 INFO 140590214039360] Loss (name: value) total: 6.52170948114\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:40:27 INFO 140590214039360] Loss (name: value) kld: 0.183584788615\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:40:27 INFO 140590214039360] Loss (name: value) recons: 6.33812469303\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:40:27 INFO 140590214039360] Loss (name: value) logppx: 6.52170948114\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:40:27 INFO 140590214039360] #validation_score (4): 6.52170948114\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:40:27 INFO 140590214039360] Timing: train: 86.12s, val: 11.71s, epoch: 97.83s\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:40:27 INFO 140590214039360] #progress_metric: host=algo-3, completed 16 % of epochs\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2813, \"sum\": 2813.0, \"min\": 2813}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2813, \"sum\": 2813.0, \"min\": 2813}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 360000, \"sum\": 360000.0, \"min\": 360000}, \"Total Batches Seen\": {\"count\": 1, \"max\": 11252, \"sum\": 11252.0, \"min\": 11252}, \"Total Records Seen\": {\"count\": 1, \"max\": 1440000, \"sum\": 1440000.0, \"min\": 1440000}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 360000, \"sum\": 360000.0, \"min\": 360000}, \"Reset Count\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}}, \"EndTime\": 1556210427.153867, \"Dimensions\": {\"Host\": \"algo-3\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 3}, \"StartTime\": 1556210329.328119}\n",
      "\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:40:27 INFO 140590214039360] #throughput_metric: host=algo-3, train throughput=3680.00711069 records/second\u001b[0m\n",
      "\u001b[33m[2019-04-25 16:40:27.154] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 4, \"duration\": 97825, \"num_examples\": 2813}\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:40:27 INFO 140590214039360] \u001b[0m\n",
      "\u001b[33m[04/25/2019 16:40:27 INFO 140590214039360] # Starting training for epoch 5\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:41:48 INFO 139933765801792] # Finished training epoch 3 on 540000 examples from 4219 batches, each of size 128.\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:41:48 INFO 139933765801792] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:41:48 INFO 139933765801792] Loss (name: value) total: 6.52832950475\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:41:48 INFO 139933765801792] Loss (name: value) kld: 0.18879722744\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:41:48 INFO 139933765801792] Loss (name: value) recons: 6.33953227791\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:41:48 INFO 139933765801792] Loss (name: value) logppx: 6.52832950475\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:41:48 INFO 139933765801792] #quality_metric: host=algo-1, epoch=3, train total_loss <loss>=6.52832950475\u001b[0m\n",
      "\u001b[31m[2019-04-25 16:41:48.809] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 2, \"duration\": 165875, \"num_examples\": 1407}\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:41:56 INFO 139904211511104] # Finished training epoch 3 on 540000 examples from 4219 batches, each of size 128.\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:41:56 INFO 139904211511104] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:41:56 INFO 139904211511104] Loss (name: value) total: 6.52907811818\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:41:56 INFO 139904211511104] Loss (name: value) kld: 0.189088777619\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:41:56 INFO 139904211511104] Loss (name: value) recons: 6.33998933757\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:41:56 INFO 139904211511104] Loss (name: value) logppx: 6.52907811818\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:41:56 INFO 139904211511104] #quality_metric: host=algo-2, epoch=3, train total_loss <loss>=6.52907811818\u001b[0m\n",
      "\u001b[32m[2019-04-25 16:41:56.208] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 2, \"duration\": 169252, \"num_examples\": 1407}\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:41:58 INFO 140590214039360] # Finished training epoch 5 on 360000 examples from 2813 batches, each of size 128.\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:41:58 INFO 140590214039360] Metrics for Training:\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:41:58 INFO 140590214039360] Loss (name: value) total: 6.52695750588\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:41:58 INFO 140590214039360] Loss (name: value) kld: 0.189200741225\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:41:58 INFO 140590214039360] Loss (name: value) recons: 6.33775676801\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:41:58 INFO 140590214039360] Loss (name: value) logppx: 6.52695750588\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:41:58 INFO 140590214039360] #quality_metric: host=algo-3, epoch=5, train total_loss <loss>=6.52695750588\u001b[0m\n",
      "\u001b[33m[2019-04-25 16:41:58.293] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 4, \"duration\": 102848, \"num_examples\": 1407}\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:42:02 INFO 139933765801792] Finished scoring on 179968 examples from 1406 batches, each of size 128.\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:42:02 INFO 139933765801792] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:42:02 INFO 139933765801792] Loss (name: value) total: 6.52568745579\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:42:02 INFO 139933765801792] Loss (name: value) kld: 0.182133688717\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:42:02 INFO 139933765801792] Loss (name: value) recons: 6.3435537676\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:42:02 INFO 139933765801792] Loss (name: value) logppx: 6.52568745579\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:42:02 INFO 139933765801792] #validation_score (3): 6.52568745579\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:42:02 INFO 139933765801792] Timing: train: 150.57s, val: 14.01s, epoch: 164.59s\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:42:02 INFO 139933765801792] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 4219, \"sum\": 4219.0, \"min\": 4219}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 4219, \"sum\": 4219.0, \"min\": 4219}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 540000, \"sum\": 540000.0, \"min\": 540000}, \"Total Batches Seen\": {\"count\": 1, \"max\": 12657, \"sum\": 12657.0, \"min\": 12657}, \"Total Records Seen\": {\"count\": 1, \"max\": 1620000, \"sum\": 1620000.0, \"min\": 1620000}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 540000, \"sum\": 540000.0, \"min\": 540000}, \"Reset Count\": {\"count\": 1, \"max\": 6, \"sum\": 6.0, \"min\": 6}}, \"EndTime\": 1556210522.82283, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 2}, \"StartTime\": 1556210358.235697}\n",
      "\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:42:02 INFO 139933765801792] #throughput_metric: host=algo-1, train throughput=3280.93420019 records/second\u001b[0m\n",
      "\u001b[31m[2019-04-25 16:42:02.823] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 3, \"duration\": 164587, \"num_examples\": 4219}\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:42:02 INFO 139933765801792] \u001b[0m\n",
      "\u001b[31m[04/25/2019 16:42:02 INFO 139933765801792] # Starting training for epoch 4\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:42:08 INFO 139904211511104] Finished scoring on 179968 examples from 1406 batches, each of size 128.\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:42:08 INFO 139904211511104] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:42:08 INFO 139904211511104] Loss (name: value) total: 6.52619998519\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:42:08 INFO 139904211511104] Loss (name: value) kld: 0.193106730465\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:42:08 INFO 139904211511104] Loss (name: value) recons: 6.33309325215\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:42:08 INFO 139904211511104] Loss (name: value) logppx: 6.52619998519\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:42:08 INFO 139904211511104] #validation_score (3): 6.52619998519\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:42:08 INFO 139904211511104] Timing: train: 154.24s, val: 12.63s, epoch: 166.87s\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:42:08 INFO 139904211511104] #progress_metric: host=algo-2, completed 12 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 4219, \"sum\": 4219.0, \"min\": 4219}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 4219, \"sum\": 4219.0, \"min\": 4219}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 540000, \"sum\": 540000.0, \"min\": 540000}, \"Total Batches Seen\": {\"count\": 1, \"max\": 12657, \"sum\": 12657.0, \"min\": 12657}, \"Total Records Seen\": {\"count\": 1, \"max\": 1620000, \"sum\": 1620000.0, \"min\": 1620000}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 540000, \"sum\": 540000.0, \"min\": 540000}, \"Reset Count\": {\"count\": 1, \"max\": 6, \"sum\": 6.0, \"min\": 6}}, \"EndTime\": 1556210528.835558, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 2}, \"StartTime\": 1556210361.96682}\n",
      "\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:42:08 INFO 139904211511104] #throughput_metric: host=algo-2, train throughput=3236.07332893 records/second\u001b[0m\n",
      "\u001b[32m[2019-04-25 16:42:08.835] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 3, \"duration\": 166868, \"num_examples\": 4219}\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:42:08 INFO 139904211511104] \u001b[0m\n",
      "\u001b[32m[04/25/2019 16:42:08 INFO 139904211511104] # Starting training for epoch 4\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:42:09 INFO 140590214039360] Finished scoring on 179968 examples from 1406 batches, each of size 128.\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:42:09 INFO 140590214039360] Metrics for Inference:\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:42:09 INFO 140590214039360] Loss (name: value) total: 6.52002149151\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:42:09 INFO 140590214039360] Loss (name: value) kld: 0.193190079188\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:42:09 INFO 140590214039360] Loss (name: value) recons: 6.32683141506\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:42:09 INFO 140590214039360] Loss (name: value) logppx: 6.52002149151\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:42:09 INFO 140590214039360] #validation_score (5): 6.52002149151\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:42:09 INFO 140590214039360] Timing: train: 91.14s, val: 11.17s, epoch: 102.31s\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:42:09 INFO 140590214039360] #progress_metric: host=algo-3, completed 20 % of epochs\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2813, \"sum\": 2813.0, \"min\": 2813}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2813, \"sum\": 2813.0, \"min\": 2813}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 360000, \"sum\": 360000.0, \"min\": 360000}, \"Total Batches Seen\": {\"count\": 1, \"max\": 14065, \"sum\": 14065.0, \"min\": 14065}, \"Total Records Seen\": {\"count\": 1, \"max\": 1800000, \"sum\": 1800000.0, \"min\": 1800000}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 360000, \"sum\": 360000.0, \"min\": 360000}, \"Reset Count\": {\"count\": 1, \"max\": 10, \"sum\": 10.0, \"min\": 10}}, \"EndTime\": 1556210529.464078, \"Dimensions\": {\"Host\": \"algo-3\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 4}, \"StartTime\": 1556210427.154189}\n",
      "\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:42:09 INFO 140590214039360] #throughput_metric: host=algo-3, train throughput=3518.70940044 records/second\u001b[0m\n",
      "\u001b[33m[2019-04-25 16:42:09.464] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 5, \"duration\": 102310, \"num_examples\": 2813}\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:42:09 INFO 140590214039360] \u001b[0m\n",
      "\u001b[33m[04/25/2019 16:42:09 INFO 140590214039360] # Starting training for epoch 6\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m[04/25/2019 16:43:38 INFO 140590214039360] # Finished training epoch 6 on 360000 examples from 2813 batches, each of size 128.\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:43:38 INFO 140590214039360] Metrics for Training:\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:43:38 INFO 140590214039360] Loss (name: value) total: 6.52607285353\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:43:38 INFO 140590214039360] Loss (name: value) kld: 0.191044585792\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:43:38 INFO 140590214039360] Loss (name: value) recons: 6.33502826545\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:43:38 INFO 140590214039360] Loss (name: value) logppx: 6.52607285353\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:43:38 INFO 140590214039360] #quality_metric: host=algo-3, epoch=6, train total_loss <loss>=6.52607285353\u001b[0m\n",
      "\u001b[33m[2019-04-25 16:43:38.425] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 5, \"duration\": 100131, \"num_examples\": 1407}\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:43:50 INFO 140590214039360] Finished scoring on 179968 examples from 1406 batches, each of size 128.\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:43:50 INFO 140590214039360] Metrics for Inference:\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:43:50 INFO 140590214039360] Loss (name: value) total: 6.52756081028\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:43:50 INFO 140590214039360] Loss (name: value) kld: 0.196946100609\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:43:50 INFO 140590214039360] Loss (name: value) recons: 6.33061471026\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:43:50 INFO 140590214039360] Loss (name: value) logppx: 6.52756081028\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:43:50 INFO 140590214039360] #validation_score (6): 6.52756081028\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:43:50 INFO 140590214039360] patience losses:[6.5728631908150863, 6.5390986412721199, 6.5289603832944865, 6.5217094811402889, 6.5200214915051742] min patience loss:6.52002149151 current loss:6.52756081028 absolute loss difference:0.00753931877114\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:43:50 INFO 140590214039360] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:43:50 INFO 140590214039360] Timing: train: 88.96s, val: 11.81s, epoch: 100.77s\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:43:50 INFO 140590214039360] #progress_metric: host=algo-3, completed 24 % of epochs\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2813, \"sum\": 2813.0, \"min\": 2813}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2813, \"sum\": 2813.0, \"min\": 2813}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 360000, \"sum\": 360000.0, \"min\": 360000}, \"Total Batches Seen\": {\"count\": 1, \"max\": 16878, \"sum\": 16878.0, \"min\": 16878}, \"Total Records Seen\": {\"count\": 1, \"max\": 2160000, \"sum\": 2160000.0, \"min\": 2160000}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 360000, \"sum\": 360000.0, \"min\": 360000}, \"Reset Count\": {\"count\": 1, \"max\": 12, \"sum\": 12.0, \"min\": 12}}, \"EndTime\": 1556210630.236147, \"Dimensions\": {\"Host\": \"algo-3\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 5}, \"StartTime\": 1556210529.464736}\n",
      "\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:43:50 INFO 140590214039360] #throughput_metric: host=algo-3, train throughput=3572.43218302 records/second\u001b[0m\n",
      "\u001b[33m[2019-04-25 16:43:50.236] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 6, \"duration\": 100771, \"num_examples\": 2813}\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:43:50 INFO 140590214039360] \u001b[0m\n",
      "\u001b[33m[04/25/2019 16:43:50 INFO 140590214039360] # Starting training for epoch 7\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:44:30 INFO 139933765801792] # Finished training epoch 4 on 540000 examples from 4219 batches, each of size 128.\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:44:30 INFO 139933765801792] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:44:30 INFO 139933765801792] Loss (name: value) total: 6.52446185393\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:44:30 INFO 139933765801792] Loss (name: value) kld: 0.193082004277\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:44:30 INFO 139933765801792] Loss (name: value) recons: 6.33137984602\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:44:30 INFO 139933765801792] Loss (name: value) logppx: 6.52446185393\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:44:30 INFO 139933765801792] #quality_metric: host=algo-1, epoch=4, train total_loss <loss>=6.52446185393\u001b[0m\n",
      "\u001b[31m[2019-04-25 16:44:30.855] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 3, \"duration\": 162045, \"num_examples\": 1407}\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:44:40 INFO 139904211511104] # Finished training epoch 4 on 540000 examples from 4219 batches, each of size 128.\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:44:40 INFO 139904211511104] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:44:40 INFO 139904211511104] Loss (name: value) total: 6.52493528586\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:44:40 INFO 139904211511104] Loss (name: value) kld: 0.194028478694\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:44:40 INFO 139904211511104] Loss (name: value) recons: 6.33090680378\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:44:40 INFO 139904211511104] Loss (name: value) logppx: 6.52493528586\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:44:40 INFO 139904211511104] #quality_metric: host=algo-2, epoch=4, train total_loss <loss>=6.52493528586\u001b[0m\n",
      "\u001b[32m[2019-04-25 16:44:40.517] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 3, \"duration\": 164308, \"num_examples\": 1407}\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:44:46 INFO 139933765801792] Finished scoring on 179968 examples from 1406 batches, each of size 128.\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:44:46 INFO 139933765801792] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:44:46 INFO 139933765801792] Loss (name: value) total: 6.5155224037\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:44:46 INFO 139933765801792] Loss (name: value) kld: 0.199725746017\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:44:46 INFO 139933765801792] Loss (name: value) recons: 6.31579666016\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:44:46 INFO 139933765801792] Loss (name: value) logppx: 6.5155224037\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:44:46 INFO 139933765801792] #validation_score (4): 6.5155224037\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:44:46 INFO 139933765801792] Timing: train: 148.03s, val: 15.60s, epoch: 163.63s\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:44:46 INFO 139933765801792] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 4219, \"sum\": 4219.0, \"min\": 4219}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 4219, \"sum\": 4219.0, \"min\": 4219}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 540000, \"sum\": 540000.0, \"min\": 540000}, \"Total Batches Seen\": {\"count\": 1, \"max\": 16876, \"sum\": 16876.0, \"min\": 16876}, \"Total Records Seen\": {\"count\": 1, \"max\": 2160000, \"sum\": 2160000.0, \"min\": 2160000}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 540000, \"sum\": 540000.0, \"min\": 540000}, \"Reset Count\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}}, \"EndTime\": 1556210686.452689, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 3}, \"StartTime\": 1556210522.823098}\n",
      "\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:44:46 INFO 139933765801792] #throughput_metric: host=algo-1, train throughput=3300.13375999 records/second\u001b[0m\n",
      "\u001b[31m[2019-04-25 16:44:46.452] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 4, \"duration\": 163629, \"num_examples\": 4219}\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:44:46 INFO 139933765801792] \u001b[0m\n",
      "\u001b[31m[04/25/2019 16:44:46 INFO 139933765801792] # Starting training for epoch 5\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:44:55 INFO 139904211511104] Finished scoring on 179968 examples from 1406 batches, each of size 128.\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:44:55 INFO 139904211511104] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:44:55 INFO 139904211511104] Loss (name: value) total: 6.51567248566\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:44:55 INFO 139904211511104] Loss (name: value) kld: 0.198053086974\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:44:55 INFO 139904211511104] Loss (name: value) recons: 6.31761939936\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:44:55 INFO 139904211511104] Loss (name: value) logppx: 6.51567248566\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:44:55 INFO 139904211511104] #validation_score (4): 6.51567248566\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:44:55 INFO 139904211511104] Timing: train: 151.68s, val: 14.71s, epoch: 166.39s\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:44:55 INFO 139904211511104] #progress_metric: host=algo-2, completed 16 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 4219, \"sum\": 4219.0, \"min\": 4219}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 4219, \"sum\": 4219.0, \"min\": 4219}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 540000, \"sum\": 540000.0, \"min\": 540000}, \"Total Batches Seen\": {\"count\": 1, \"max\": 16876, \"sum\": 16876.0, \"min\": 16876}, \"Total Records Seen\": {\"count\": 1, \"max\": 2160000, \"sum\": 2160000.0, \"min\": 2160000}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 540000, \"sum\": 540000.0, \"min\": 540000}, \"Reset Count\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}}, \"EndTime\": 1556210695.227226, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 3}, \"StartTime\": 1556210528.835882}\n",
      "\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:44:55 INFO 139904211511104] #throughput_metric: host=algo-2, train throughput=3245.35793282 records/second\u001b[0m\n",
      "\u001b[32m[2019-04-25 16:44:55.227] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 4, \"duration\": 166391, \"num_examples\": 4219}\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:44:55 INFO 139904211511104] \u001b[0m\n",
      "\u001b[32m[04/25/2019 16:44:55 INFO 139904211511104] # Starting training for epoch 5\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m[04/25/2019 16:45:13 INFO 140590214039360] # Finished training epoch 7 on 360000 examples from 2813 batches, each of size 128.\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:45:13 INFO 140590214039360] Metrics for Training:\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:45:13 INFO 140590214039360] Loss (name: value) total: 6.51912659629\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:45:13 INFO 140590214039360] Loss (name: value) kld: 0.200189670284\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:45:13 INFO 140590214039360] Loss (name: value) recons: 6.31893692235\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:45:13 INFO 140590214039360] Loss (name: value) logppx: 6.51912659629\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:45:13 INFO 140590214039360] #quality_metric: host=algo-3, epoch=7, train total_loss <loss>=6.51912659629\u001b[0m\n",
      "\u001b[33m[2019-04-25 16:45:13.655] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 6, \"duration\": 95230, \"num_examples\": 1407}\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:45:25 INFO 140590214039360] Finished scoring on 179968 examples from 1406 batches, each of size 128.\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:45:25 INFO 140590214039360] Metrics for Inference:\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:45:25 INFO 140590214039360] Loss (name: value) total: 6.51838584374\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:45:25 INFO 140590214039360] Loss (name: value) kld: 0.206371592848\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:45:25 INFO 140590214039360] Loss (name: value) recons: 6.31201425318\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:45:25 INFO 140590214039360] Loss (name: value) logppx: 6.51838584374\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:45:25 INFO 140590214039360] #validation_score (7): 6.51838584374\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:45:25 INFO 140590214039360] patience losses:[6.5390986412721199, 6.5289603832944865, 6.5217094811402889, 6.5200214915051742, 6.5275608102763192] min patience loss:6.52002149151 current loss:6.51838584374 absolute loss difference:0.00163564776967\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:45:25 INFO 140590214039360] Timing: train: 83.42s, val: 11.83s, epoch: 95.25s\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:45:25 INFO 140590214039360] #progress_metric: host=algo-3, completed 28 % of epochs\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2813, \"sum\": 2813.0, \"min\": 2813}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2813, \"sum\": 2813.0, \"min\": 2813}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 360000, \"sum\": 360000.0, \"min\": 360000}, \"Total Batches Seen\": {\"count\": 1, \"max\": 19691, \"sum\": 19691.0, \"min\": 19691}, \"Total Records Seen\": {\"count\": 1, \"max\": 2520000, \"sum\": 2520000.0, \"min\": 2520000}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 360000, \"sum\": 360000.0, \"min\": 360000}, \"Reset Count\": {\"count\": 1, \"max\": 14, \"sum\": 14.0, \"min\": 14}}, \"EndTime\": 1556210725.49067, \"Dimensions\": {\"Host\": \"algo-3\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 6}, \"StartTime\": 1556210630.236623}\n",
      "\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:45:25 INFO 140590214039360] #throughput_metric: host=algo-3, train throughput=3779.35917022 records/second\u001b[0m\n",
      "\u001b[33m[2019-04-25 16:45:25.490] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 7, \"duration\": 95254, \"num_examples\": 2813}\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:45:25 INFO 140590214039360] \u001b[0m\n",
      "\u001b[33m[04/25/2019 16:45:25 INFO 140590214039360] # Starting training for epoch 8\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:46:54 INFO 140590214039360] # Finished training epoch 8 on 360000 examples from 2813 batches, each of size 128.\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:46:54 INFO 140590214039360] Metrics for Training:\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:46:54 INFO 140590214039360] Loss (name: value) total: 6.51830338389\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:46:54 INFO 140590214039360] Loss (name: value) kld: 0.206647323052\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:46:54 INFO 140590214039360] Loss (name: value) recons: 6.31165605908\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:46:54 INFO 140590214039360] Loss (name: value) logppx: 6.51830338389\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:46:54 INFO 140590214039360] #quality_metric: host=algo-3, epoch=8, train total_loss <loss>=6.51830338389\u001b[0m\n",
      "\u001b[33m[2019-04-25 16:46:54.887] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 7, \"duration\": 101231, \"num_examples\": 1407}\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:47:06 INFO 140590214039360] Finished scoring on 179968 examples from 1406 batches, each of size 128.\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:47:06 INFO 140590214039360] Metrics for Inference:\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:47:06 INFO 140590214039360] Loss (name: value) total: 6.51895543895\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:47:06 INFO 140590214039360] Loss (name: value) kld: 0.204101163179\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:47:06 INFO 140590214039360] Loss (name: value) recons: 6.31485427698\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:47:06 INFO 140590214039360] Loss (name: value) logppx: 6.51895543895\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:47:06 INFO 140590214039360] #validation_score (8): 6.51895543895\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:47:06 INFO 140590214039360] patience losses:[6.5289603832944865, 6.5217094811402889, 6.5200214915051742, 6.5275608102763192, 6.5183858437355005] min patience loss:6.51838584374 current loss:6.51895543895 absolute loss difference:0.000569595212109\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:47:06 INFO 140590214039360] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:47:06 INFO 140590214039360] Timing: train: 89.40s, val: 11.57s, epoch: 100.96s\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:47:06 INFO 140590214039360] #progress_metric: host=algo-3, completed 32 % of epochs\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2813, \"sum\": 2813.0, \"min\": 2813}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2813, \"sum\": 2813.0, \"min\": 2813}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 360000, \"sum\": 360000.0, \"min\": 360000}, \"Total Batches Seen\": {\"count\": 1, \"max\": 22504, \"sum\": 22504.0, \"min\": 22504}, \"Total Records Seen\": {\"count\": 1, \"max\": 2880000, \"sum\": 2880000.0, \"min\": 2880000}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 360000, \"sum\": 360000.0, \"min\": 360000}, \"Reset Count\": {\"count\": 1, \"max\": 16, \"sum\": 16.0, \"min\": 16}}, \"EndTime\": 1556210826.454244, \"Dimensions\": {\"Host\": \"algo-3\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 7}, \"StartTime\": 1556210725.491014}\n",
      "\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:47:06 INFO 140590214039360] #throughput_metric: host=algo-3, train throughput=3565.64892484 records/second\u001b[0m\n",
      "\u001b[33m[2019-04-25 16:47:06.454] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 8, \"duration\": 100963, \"num_examples\": 2813}\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:47:06 INFO 140590214039360] \u001b[0m\n",
      "\u001b[33m[04/25/2019 16:47:06 INFO 140590214039360] # Starting training for epoch 9\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:47:13 INFO 139933765801792] # Finished training epoch 5 on 540000 examples from 4219 batches, each of size 128.\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:47:13 INFO 139933765801792] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:47:13 INFO 139933765801792] Loss (name: value) total: 6.51796439147\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:47:13 INFO 139933765801792] Loss (name: value) kld: 0.206347259555\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:47:13 INFO 139933765801792] Loss (name: value) recons: 6.31161712476\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:47:13 INFO 139933765801792] Loss (name: value) logppx: 6.51796439147\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:47:13 INFO 139933765801792] #quality_metric: host=algo-1, epoch=5, train total_loss <loss>=6.51796439147\u001b[0m\n",
      "\u001b[31m[2019-04-25 16:47:13.902] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 4, \"duration\": 163046, \"num_examples\": 1407}\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:47:26 INFO 139904211511104] # Finished training epoch 5 on 540000 examples from 4219 batches, each of size 128.\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:47:26 INFO 139904211511104] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:47:26 INFO 139904211511104] Loss (name: value) total: 6.51833360922\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:47:26 INFO 139904211511104] Loss (name: value) kld: 0.20698666669\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:47:26 INFO 139904211511104] Loss (name: value) recons: 6.31134694412\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:47:26 INFO 139904211511104] Loss (name: value) logppx: 6.51833360922\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:47:26 INFO 139904211511104] #quality_metric: host=algo-2, epoch=5, train total_loss <loss>=6.51833360922\u001b[0m\n",
      "\u001b[32m[2019-04-25 16:47:26.570] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 4, \"duration\": 166052, \"num_examples\": 1407}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[04/25/2019 16:47:29 INFO 139933765801792] Finished scoring on 179968 examples from 1406 batches, each of size 128.\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:47:29 INFO 139933765801792] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:47:29 INFO 139933765801792] Loss (name: value) total: 6.51310268846\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:47:29 INFO 139933765801792] Loss (name: value) kld: 0.209255355107\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:47:29 INFO 139933765801792] Loss (name: value) recons: 6.30384732649\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:47:29 INFO 139933765801792] Loss (name: value) logppx: 6.51310268846\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:47:29 INFO 139933765801792] #validation_score (5): 6.51310268846\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:47:29 INFO 139933765801792] Timing: train: 147.45s, val: 15.89s, epoch: 163.34s\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:47:29 INFO 139933765801792] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 4219, \"sum\": 4219.0, \"min\": 4219}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 4219, \"sum\": 4219.0, \"min\": 4219}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 540000, \"sum\": 540000.0, \"min\": 540000}, \"Total Batches Seen\": {\"count\": 1, \"max\": 21095, \"sum\": 21095.0, \"min\": 21095}, \"Total Records Seen\": {\"count\": 1, \"max\": 2700000, \"sum\": 2700000.0, \"min\": 2700000}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 540000, \"sum\": 540000.0, \"min\": 540000}, \"Reset Count\": {\"count\": 1, \"max\": 10, \"sum\": 10.0, \"min\": 10}}, \"EndTime\": 1556210849.795866, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 4}, \"StartTime\": 1556210686.452961}\n",
      "\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:47:29 INFO 139933765801792] #throughput_metric: host=algo-1, train throughput=3305.92598273 records/second\u001b[0m\n",
      "\u001b[31m[2019-04-25 16:47:29.796] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 5, \"duration\": 163342, \"num_examples\": 4219}\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:47:29 INFO 139933765801792] \u001b[0m\n",
      "\u001b[31m[04/25/2019 16:47:29 INFO 139933765801792] # Starting training for epoch 6\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:47:41 INFO 139904211511104] Finished scoring on 179968 examples from 1406 batches, each of size 128.\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:47:41 INFO 139904211511104] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:47:41 INFO 139904211511104] Loss (name: value) total: 6.51043439861\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:47:41 INFO 139904211511104] Loss (name: value) kld: 0.206569466692\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:47:41 INFO 139904211511104] Loss (name: value) recons: 6.30386493623\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:47:41 INFO 139904211511104] Loss (name: value) logppx: 6.51043439861\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:47:41 INFO 139904211511104] #validation_score (5): 6.51043439861\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:47:41 INFO 139904211511104] Timing: train: 151.34s, val: 14.93s, epoch: 166.27s\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:47:41 INFO 139904211511104] #progress_metric: host=algo-2, completed 20 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 4219, \"sum\": 4219.0, \"min\": 4219}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 4219, \"sum\": 4219.0, \"min\": 4219}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 540000, \"sum\": 540000.0, \"min\": 540000}, \"Total Batches Seen\": {\"count\": 1, \"max\": 21095, \"sum\": 21095.0, \"min\": 21095}, \"Total Records Seen\": {\"count\": 1, \"max\": 2700000, \"sum\": 2700000.0, \"min\": 2700000}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 540000, \"sum\": 540000.0, \"min\": 540000}, \"Reset Count\": {\"count\": 1, \"max\": 10, \"sum\": 10.0, \"min\": 10}}, \"EndTime\": 1556210861.499871, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 4}, \"StartTime\": 1556210695.227557}\n",
      "\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:47:41 INFO 139904211511104] #throughput_metric: host=algo-2, train throughput=3247.68117872 records/second\u001b[0m\n",
      "\u001b[32m[2019-04-25 16:47:41.500] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 5, \"duration\": 166272, \"num_examples\": 4219}\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:47:41 INFO 139904211511104] \u001b[0m\n",
      "\u001b[32m[04/25/2019 16:47:41 INFO 139904211511104] # Starting training for epoch 6\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:48:33 INFO 140590214039360] # Finished training epoch 9 on 360000 examples from 2813 batches, each of size 128.\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:48:33 INFO 140590214039360] Metrics for Training:\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:48:33 INFO 140590214039360] Loss (name: value) total: 6.51581939802\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:48:33 INFO 140590214039360] Loss (name: value) kld: 0.208565528754\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:48:33 INFO 140590214039360] Loss (name: value) recons: 6.30725386505\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:48:33 INFO 140590214039360] Loss (name: value) logppx: 6.51581939802\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:48:33 INFO 140590214039360] #quality_metric: host=algo-3, epoch=9, train total_loss <loss>=6.51581939802\u001b[0m\n",
      "\u001b[33m[2019-04-25 16:48:33.373] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 8, \"duration\": 98486, \"num_examples\": 1407}\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:48:44 INFO 140590214039360] Finished scoring on 179968 examples from 1406 batches, each of size 128.\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:48:44 INFO 140590214039360] Metrics for Inference:\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:48:44 INFO 140590214039360] Loss (name: value) total: 6.51606084069\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:48:44 INFO 140590214039360] Loss (name: value) kld: 0.212867036949\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:48:44 INFO 140590214039360] Loss (name: value) recons: 6.30319380285\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:48:44 INFO 140590214039360] Loss (name: value) logppx: 6.51606084069\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:48:44 INFO 140590214039360] #validation_score (9): 6.51606084069\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:48:44 INFO 140590214039360] patience losses:[6.5217094811402889, 6.5200214915051742, 6.5275608102763192, 6.5183858437355005, 6.5189554389476099] min patience loss:6.51838584374 current loss:6.51606084069 absolute loss difference:0.00232500304199\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:48:44 INFO 140590214039360] Timing: train: 86.92s, val: 11.59s, epoch: 98.51s\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:48:44 INFO 140590214039360] #progress_metric: host=algo-3, completed 36 % of epochs\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2813, \"sum\": 2813.0, \"min\": 2813}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2813, \"sum\": 2813.0, \"min\": 2813}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 360000, \"sum\": 360000.0, \"min\": 360000}, \"Total Batches Seen\": {\"count\": 1, \"max\": 25317, \"sum\": 25317.0, \"min\": 25317}, \"Total Records Seen\": {\"count\": 1, \"max\": 3240000, \"sum\": 3240000.0, \"min\": 3240000}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 360000, \"sum\": 360000.0, \"min\": 360000}, \"Reset Count\": {\"count\": 1, \"max\": 18, \"sum\": 18.0, \"min\": 18}}, \"EndTime\": 1556210924.96628, \"Dimensions\": {\"Host\": \"algo-3\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 8}, \"StartTime\": 1556210826.454553}\n",
      "\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:48:44 INFO 140590214039360] #throughput_metric: host=algo-3, train throughput=3654.38132966 records/second\u001b[0m\n",
      "\u001b[33m[2019-04-25 16:48:44.966] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 9, \"duration\": 98511, \"num_examples\": 2813}\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:48:44 INFO 140590214039360] \u001b[0m\n",
      "\u001b[33m[04/25/2019 16:48:44 INFO 140590214039360] # Starting training for epoch 10\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:49:57 INFO 139933765801792] # Finished training epoch 6 on 540000 examples from 4219 batches, each of size 128.\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:49:57 INFO 139933765801792] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:49:57 INFO 139933765801792] Loss (name: value) total: 6.51772954086\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:49:57 INFO 139933765801792] Loss (name: value) kld: 0.208218386647\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:49:57 INFO 139933765801792] Loss (name: value) recons: 6.30951115146\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:49:57 INFO 139933765801792] Loss (name: value) logppx: 6.51772954086\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:49:57 INFO 139933765801792] #quality_metric: host=algo-1, epoch=6, train total_loss <loss>=6.51772954086\u001b[0m\n",
      "\u001b[31m[2019-04-25 16:49:57.540] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 5, \"duration\": 163637, \"num_examples\": 1407}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m[04/25/2019 16:50:12 INFO 140590214039360] # Finished training epoch 10 on 360000 examples from 2813 batches, each of size 128.\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:50:12 INFO 140590214039360] Metrics for Training:\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:50:12 INFO 140590214039360] Loss (name: value) total: 6.51677309971\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:50:12 INFO 140590214039360] Loss (name: value) kld: 0.207705555704\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:50:12 INFO 140590214039360] Loss (name: value) recons: 6.30906753806\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:50:12 INFO 140590214039360] Loss (name: value) logppx: 6.51677309971\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:50:12 INFO 140590214039360] #quality_metric: host=algo-3, epoch=10, train total_loss <loss>=6.51677309971\u001b[0m\n",
      "\u001b[33m[2019-04-25 16:50:12.849] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 9, \"duration\": 99475, \"num_examples\": 1407}\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:50:13 INFO 139933765801792] Finished scoring on 179968 examples from 1406 batches, each of size 128.\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:50:13 INFO 139933765801792] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:50:13 INFO 139933765801792] Loss (name: value) total: 6.5155481892\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:50:13 INFO 139933765801792] Loss (name: value) kld: 0.20578874566\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:50:13 INFO 139933765801792] Loss (name: value) recons: 6.30975945135\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:50:13 INFO 139933765801792] Loss (name: value) logppx: 6.5155481892\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:50:13 INFO 139933765801792] #validation_score (6): 6.5155481892\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:50:13 INFO 139933765801792] patience losses:[6.553127402770909, 6.5252941665405233, 6.5256874557918367, 6.5155224037034749, 6.513102688457006] min patience loss:6.51310268846 current loss:6.5155481892 absolute loss difference:0.00244550074147\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:50:13 INFO 139933765801792] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:50:13 INFO 139933765801792] Timing: train: 147.74s, val: 15.51s, epoch: 163.25s\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:50:13 INFO 139933765801792] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 4219, \"sum\": 4219.0, \"min\": 4219}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 4219, \"sum\": 4219.0, \"min\": 4219}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 540000, \"sum\": 540000.0, \"min\": 540000}, \"Total Batches Seen\": {\"count\": 1, \"max\": 25314, \"sum\": 25314.0, \"min\": 25314}, \"Total Records Seen\": {\"count\": 1, \"max\": 3240000, \"sum\": 3240000.0, \"min\": 3240000}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 540000, \"sum\": 540000.0, \"min\": 540000}, \"Reset Count\": {\"count\": 1, \"max\": 12, \"sum\": 12.0, \"min\": 12}}, \"EndTime\": 1556211013.045818, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 5}, \"StartTime\": 1556210849.796155}\n",
      "\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:50:13 INFO 139933765801792] #throughput_metric: host=algo-1, train throughput=3307.81403955 records/second\u001b[0m\n",
      "\u001b[31m[2019-04-25 16:50:13.046] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 6, \"duration\": 163249, \"num_examples\": 4219}\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:50:13 INFO 139933765801792] \u001b[0m\n",
      "\u001b[31m[04/25/2019 16:50:13 INFO 139933765801792] # Starting training for epoch 7\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:50:14 INFO 139904211511104] # Finished training epoch 6 on 540000 examples from 4219 batches, each of size 128.\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:50:14 INFO 139904211511104] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:50:14 INFO 139904211511104] Loss (name: value) total: 6.51828253961\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:50:14 INFO 139904211511104] Loss (name: value) kld: 0.208301198054\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:50:14 INFO 139904211511104] Loss (name: value) recons: 6.30998133924\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:50:14 INFO 139904211511104] Loss (name: value) logppx: 6.51828253961\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:50:14 INFO 139904211511104] #quality_metric: host=algo-2, epoch=6, train total_loss <loss>=6.51828253961\u001b[0m\n",
      "\u001b[32m[2019-04-25 16:50:14.248] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 5, \"duration\": 167677, \"num_examples\": 1407}\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:50:24 INFO 140590214039360] Finished scoring on 179968 examples from 1406 batches, each of size 128.\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:50:24 INFO 140590214039360] Metrics for Inference:\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:50:24 INFO 140590214039360] Loss (name: value) total: 6.51140997759\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:50:24 INFO 140590214039360] Loss (name: value) kld: 0.209277852914\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:50:24 INFO 140590214039360] Loss (name: value) recons: 6.30213212119\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:50:24 INFO 140590214039360] Loss (name: value) logppx: 6.51140997759\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:50:24 INFO 140590214039360] #validation_score (10): 6.51140997759\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:50:24 INFO 140590214039360] patience losses:[6.5200214915051742, 6.5275608102763192, 6.5183858437355005, 6.5189554389476099, 6.5160608406935108] min patience loss:6.51606084069 current loss:6.51140997759 absolute loss difference:0.00465086310212\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:50:24 INFO 140590214039360] Timing: train: 87.88s, val: 11.42s, epoch: 99.31s\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:50:24 INFO 140590214039360] #progress_metric: host=algo-3, completed 40 % of epochs\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2813, \"sum\": 2813.0, \"min\": 2813}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2813, \"sum\": 2813.0, \"min\": 2813}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 360000, \"sum\": 360000.0, \"min\": 360000}, \"Total Batches Seen\": {\"count\": 1, \"max\": 28130, \"sum\": 28130.0, \"min\": 28130}, \"Total Records Seen\": {\"count\": 1, \"max\": 3600000, \"sum\": 3600000.0, \"min\": 3600000}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 360000, \"sum\": 360000.0, \"min\": 360000}, \"Reset Count\": {\"count\": 1, \"max\": 20, \"sum\": 20.0, \"min\": 20}}, \"EndTime\": 1556211024.274377, \"Dimensions\": {\"Host\": \"algo-3\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 9}, \"StartTime\": 1556210924.966558}\n",
      "\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:50:24 INFO 140590214039360] #throughput_metric: host=algo-3, train throughput=3625.08693808 records/second\u001b[0m\n",
      "\u001b[33m[2019-04-25 16:50:24.274] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 10, \"duration\": 99307, \"num_examples\": 2813}\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:50:24 INFO 140590214039360] \u001b[0m\n",
      "\u001b[33m[04/25/2019 16:50:24 INFO 140590214039360] # Starting training for epoch 11\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:50:29 INFO 139904211511104] Finished scoring on 179968 examples from 1406 batches, each of size 128.\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:50:29 INFO 139904211511104] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:50:29 INFO 139904211511104] Loss (name: value) total: 6.51337599687\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:50:29 INFO 139904211511104] Loss (name: value) kld: 0.21377349399\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:50:29 INFO 139904211511104] Loss (name: value) recons: 6.29960250583\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:50:29 INFO 139904211511104] Loss (name: value) logppx: 6.51337599687\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:50:29 INFO 139904211511104] #validation_score (6): 6.51337599687\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:50:29 INFO 139904211511104] patience losses:[6.5401341643815023, 6.5286911391940601, 6.5261999851948502, 6.5156724856554362, 6.5104343986104256] min patience loss:6.51043439861 current loss:6.51337599687 absolute loss difference:0.00294159825462\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:50:29 INFO 139904211511104] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:50:29 INFO 139904211511104] Timing: train: 152.75s, val: 15.41s, epoch: 168.16s\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:50:29 INFO 139904211511104] #progress_metric: host=algo-2, completed 24 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 4219, \"sum\": 4219.0, \"min\": 4219}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 4219, \"sum\": 4219.0, \"min\": 4219}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 540000, \"sum\": 540000.0, \"min\": 540000}, \"Total Batches Seen\": {\"count\": 1, \"max\": 25314, \"sum\": 25314.0, \"min\": 25314}, \"Total Records Seen\": {\"count\": 1, \"max\": 3240000, \"sum\": 3240000.0, \"min\": 3240000}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 540000, \"sum\": 540000.0, \"min\": 540000}, \"Reset Count\": {\"count\": 1, \"max\": 12, \"sum\": 12.0, \"min\": 12}}, \"EndTime\": 1556211029.662926, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 5}, \"StartTime\": 1556210861.500201}\n",
      "\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:50:29 INFO 139904211511104] #throughput_metric: host=algo-2, train throughput=3211.17248465 records/second\u001b[0m\n",
      "\u001b[32m[2019-04-25 16:50:29.663] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 6, \"duration\": 168162, \"num_examples\": 4219}\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:50:29 INFO 139904211511104] \u001b[0m\n",
      "\u001b[32m[04/25/2019 16:50:29 INFO 139904211511104] # Starting training for epoch 7\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m[04/25/2019 16:51:54 INFO 140590214039360] # Finished training epoch 11 on 360000 examples from 2813 batches, each of size 128.\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:51:54 INFO 140590214039360] Metrics for Training:\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:51:54 INFO 140590214039360] Loss (name: value) total: 6.5182072117\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:51:54 INFO 140590214039360] Loss (name: value) kld: 0.208104959447\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:51:54 INFO 140590214039360] Loss (name: value) recons: 6.31010225105\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:51:54 INFO 140590214039360] Loss (name: value) logppx: 6.5182072117\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:51:54 INFO 140590214039360] #quality_metric: host=algo-3, epoch=11, train total_loss <loss>=6.5182072117\u001b[0m\n",
      "\u001b[33m[2019-04-25 16:51:54.165] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 10, \"duration\": 101315, \"num_examples\": 1407}\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:52:06 INFO 140590214039360] Finished scoring on 179968 examples from 1406 batches, each of size 128.\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:52:06 INFO 140590214039360] Metrics for Inference:\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:52:06 INFO 140590214039360] Loss (name: value) total: 6.51170618232\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:52:06 INFO 140590214039360] Loss (name: value) kld: 0.195098992676\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:52:06 INFO 140590214039360] Loss (name: value) recons: 6.31660718599\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:52:06 INFO 140590214039360] Loss (name: value) logppx: 6.51170618232\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:52:06 INFO 140590214039360] #validation_score (11): 6.51170618232\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:52:06 INFO 140590214039360] patience losses:[6.5275608102763192, 6.5183858437355005, 6.5189554389476099, 6.5160608406935108, 6.5114099775913941] min patience loss:6.51140997759 current loss:6.51170618232 absolute loss difference:0.000296204731101\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:52:06 INFO 140590214039360] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:52:06 INFO 140590214039360] Timing: train: 89.89s, val: 11.96s, epoch: 101.85s\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:52:06 INFO 140590214039360] #progress_metric: host=algo-3, completed 44 % of epochs\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2813, \"sum\": 2813.0, \"min\": 2813}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2813, \"sum\": 2813.0, \"min\": 2813}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 360000, \"sum\": 360000.0, \"min\": 360000}, \"Total Batches Seen\": {\"count\": 1, \"max\": 30943, \"sum\": 30943.0, \"min\": 30943}, \"Total Records Seen\": {\"count\": 1, \"max\": 3960000, \"sum\": 3960000.0, \"min\": 3960000}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 360000, \"sum\": 360000.0, \"min\": 360000}, \"Reset Count\": {\"count\": 1, \"max\": 22, \"sum\": 22.0, \"min\": 22}}, \"EndTime\": 1556211126.129945, \"Dimensions\": {\"Host\": \"algo-3\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 10}, \"StartTime\": 1556211024.274678}\n",
      "\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:52:06 INFO 140590214039360] #throughput_metric: host=algo-3, train throughput=3534.42124964 records/second\u001b[0m\n",
      "\u001b[33m[2019-04-25 16:52:06.130] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 11, \"duration\": 101855, \"num_examples\": 2813}\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:52:06 INFO 140590214039360] \u001b[0m\n",
      "\u001b[33m[04/25/2019 16:52:06 INFO 140590214039360] # Starting training for epoch 12\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:52:39 INFO 139933765801792] # Finished training epoch 7 on 540000 examples from 4219 batches, each of size 128.\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:52:39 INFO 139933765801792] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:52:39 INFO 139933765801792] Loss (name: value) total: 6.51767227146\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:52:39 INFO 139933765801792] Loss (name: value) kld: 0.208553663745\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:52:39 INFO 139933765801792] Loss (name: value) recons: 6.30911860677\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:52:39 INFO 139933765801792] Loss (name: value) logppx: 6.51767227146\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:52:39 INFO 139933765801792] #quality_metric: host=algo-1, epoch=7, train total_loss <loss>=6.51767227146\u001b[0m\n",
      "\u001b[31m[2019-04-25 16:52:39.132] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 6, \"duration\": 161592, \"num_examples\": 1407}\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:52:54 INFO 139933765801792] Finished scoring on 179968 examples from 1406 batches, each of size 128.\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:52:54 INFO 139933765801792] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:52:54 INFO 139933765801792] Loss (name: value) total: 6.51972500935\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:52:54 INFO 139933765801792] Loss (name: value) kld: 0.214483511333\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:52:54 INFO 139933765801792] Loss (name: value) recons: 6.30524150203\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:52:54 INFO 139933765801792] Loss (name: value) logppx: 6.51972500935\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:52:54 INFO 139933765801792] #validation_score (7): 6.51972500935\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:52:54 INFO 139933765801792] patience losses:[6.5252941665405233, 6.5256874557918367, 6.5155224037034749, 6.513102688457006, 6.515548189198479] min patience loss:6.51310268846 current loss:6.51972500935 absolute loss difference:0.00662232089687\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:52:54 INFO 139933765801792] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:52:54 INFO 139933765801792] Timing: train: 146.09s, val: 15.79s, epoch: 161.88s\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:52:54 INFO 139933765801792] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 4219, \"sum\": 4219.0, \"min\": 4219}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 4219, \"sum\": 4219.0, \"min\": 4219}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 540000, \"sum\": 540000.0, \"min\": 540000}, \"Total Batches Seen\": {\"count\": 1, \"max\": 29533, \"sum\": 29533.0, \"min\": 29533}, \"Total Records Seen\": {\"count\": 1, \"max\": 3780000, \"sum\": 3780000.0, \"min\": 3780000}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 540000, \"sum\": 540000.0, \"min\": 540000}, \"Reset Count\": {\"count\": 1, \"max\": 14, \"sum\": 14.0, \"min\": 14}}, \"EndTime\": 1556211174.925296, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 6}, \"StartTime\": 1556211013.046153}\n",
      "\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:52:54 INFO 139933765801792] #throughput_metric: host=algo-1, train throughput=3335.81917869 records/second\u001b[0m\n",
      "\u001b[31m[2019-04-25 16:52:54.925] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 7, \"duration\": 161878, \"num_examples\": 4219}\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:52:54 INFO 139933765801792] \u001b[0m\n",
      "\u001b[31m[04/25/2019 16:52:54 INFO 139933765801792] # Starting training for epoch 8\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:53:01 INFO 139904211511104] # Finished training epoch 7 on 540000 examples from 4219 batches, each of size 128.\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:53:01 INFO 139904211511104] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:53:01 INFO 139904211511104] Loss (name: value) total: 6.51896462155\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:53:01 INFO 139904211511104] Loss (name: value) kld: 0.208746003399\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:53:01 INFO 139904211511104] Loss (name: value) recons: 6.31021861958\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:53:01 INFO 139904211511104] Loss (name: value) logppx: 6.51896462155\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:53:01 INFO 139904211511104] #quality_metric: host=algo-2, epoch=7, train total_loss <loss>=6.51896462155\u001b[0m\n",
      "\u001b[32m[2019-04-25 16:53:01.864] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 6, \"duration\": 167615, \"num_examples\": 1407}\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:53:16 INFO 139904211511104] Finished scoring on 179968 examples from 1406 batches, each of size 128.\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:53:16 INFO 139904211511104] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:53:16 INFO 139904211511104] Loss (name: value) total: 6.51816064128\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:53:16 INFO 139904211511104] Loss (name: value) kld: 0.209099162693\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:53:16 INFO 139904211511104] Loss (name: value) recons: 6.30906147435\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:53:16 INFO 139904211511104] Loss (name: value) logppx: 6.51816064128\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:53:16 INFO 139904211511104] #validation_score (7): 6.51816064128\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:53:16 INFO 139904211511104] patience losses:[6.5286911391940601, 6.5261999851948502, 6.5156724856554362, 6.5104343986104256, 6.5133759968650455] min patience loss:6.51043439861 current loss:6.51816064128 absolute loss difference:0.00772624266775\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:53:16 INFO 139904211511104] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:53:16 INFO 139904211511104] Timing: train: 152.20s, val: 14.85s, epoch: 167.05s\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:53:16 INFO 139904211511104] #progress_metric: host=algo-2, completed 28 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 4219, \"sum\": 4219.0, \"min\": 4219}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 4219, \"sum\": 4219.0, \"min\": 4219}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 540000, \"sum\": 540000.0, \"min\": 540000}, \"Total Batches Seen\": {\"count\": 1, \"max\": 29533, \"sum\": 29533.0, \"min\": 29533}, \"Total Records Seen\": {\"count\": 1, \"max\": 3780000, \"sum\": 3780000.0, \"min\": 3780000}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 540000, \"sum\": 540000.0, \"min\": 540000}, \"Reset Count\": {\"count\": 1, \"max\": 14, \"sum\": 14.0, \"min\": 14}}, \"EndTime\": 1556211196.714066, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 6}, \"StartTime\": 1556211029.663205}\n",
      "\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:53:16 INFO 139904211511104] #throughput_metric: host=algo-2, train throughput=3232.54534583 records/second\u001b[0m\n",
      "\u001b[32m[2019-04-25 16:53:16.714] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 7, \"duration\": 167050, \"num_examples\": 4219}\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:53:16 INFO 139904211511104] \u001b[0m\n",
      "\u001b[32m[04/25/2019 16:53:16 INFO 139904211511104] # Starting training for epoch 8\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m[04/25/2019 16:53:34 INFO 140590214039360] # Finished training epoch 12 on 360000 examples from 2813 batches, each of size 128.\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:53:34 INFO 140590214039360] Metrics for Training:\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:53:34 INFO 140590214039360] Loss (name: value) total: 6.51599918758\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:53:34 INFO 140590214039360] Loss (name: value) kld: 0.209243660555\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:53:34 INFO 140590214039360] Loss (name: value) recons: 6.30675552089\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:53:34 INFO 140590214039360] Loss (name: value) logppx: 6.51599918758\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:53:34 INFO 140590214039360] #quality_metric: host=algo-3, epoch=12, train total_loss <loss>=6.51599918758\u001b[0m\n",
      "\u001b[33m[2019-04-25 16:53:34.917] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 11, \"duration\": 100751, \"num_examples\": 1407}\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:53:46 INFO 140590214039360] Finished scoring on 179968 examples from 1406 batches, each of size 128.\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:53:46 INFO 140590214039360] Metrics for Inference:\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:53:46 INFO 140590214039360] Loss (name: value) total: 6.51435464714\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:53:46 INFO 140590214039360] Loss (name: value) kld: 0.20552552891\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:53:46 INFO 140590214039360] Loss (name: value) recons: 6.30882911831\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:53:46 INFO 140590214039360] Loss (name: value) logppx: 6.51435464714\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:53:46 INFO 140590214039360] #validation_score (12): 6.51435464714\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:53:46 INFO 140590214039360] patience losses:[6.5183858437355005, 6.5189554389476099, 6.5160608406935108, 6.5114099775913941, 6.5117061823224951] min patience loss:6.51140997759 current loss:6.51435464714 absolute loss difference:0.00294466954716\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:53:46 INFO 140590214039360] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:53:46 INFO 140590214039360] Timing: train: 88.79s, val: 11.95s, epoch: 100.73s\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:53:46 INFO 140590214039360] #progress_metric: host=algo-3, completed 48 % of epochs\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2813, \"sum\": 2813.0, \"min\": 2813}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2813, \"sum\": 2813.0, \"min\": 2813}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 360000, \"sum\": 360000.0, \"min\": 360000}, \"Total Batches Seen\": {\"count\": 1, \"max\": 33756, \"sum\": 33756.0, \"min\": 33756}, \"Total Records Seen\": {\"count\": 1, \"max\": 4320000, \"sum\": 4320000.0, \"min\": 4320000}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 360000, \"sum\": 360000.0, \"min\": 360000}, \"Reset Count\": {\"count\": 1, \"max\": 24, \"sum\": 24.0, \"min\": 24}}, \"EndTime\": 1556211226.862896, \"Dimensions\": {\"Host\": \"algo-3\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 11}, \"StartTime\": 1556211126.130251}\n",
      "\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:53:46 INFO 140590214039360] #throughput_metric: host=algo-3, train throughput=3573.81174632 records/second\u001b[0m\n",
      "\u001b[33m[2019-04-25 16:53:46.863] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 12, \"duration\": 100732, \"num_examples\": 2813}\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:53:46 INFO 140590214039360] \u001b[0m\n",
      "\u001b[33m[04/25/2019 16:53:46 INFO 140590214039360] # Starting training for epoch 13\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:55:16 INFO 140590214039360] # Finished training epoch 13 on 360000 examples from 2813 batches, each of size 128.\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:55:16 INFO 140590214039360] Metrics for Training:\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:55:16 INFO 140590214039360] Loss (name: value) total: 6.51813627052\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:55:16 INFO 140590214039360] Loss (name: value) kld: 0.208910732158\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:55:16 INFO 140590214039360] Loss (name: value) recons: 6.30922554016\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:55:16 INFO 140590214039360] Loss (name: value) logppx: 6.51813627052\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:55:16 INFO 140590214039360] #quality_metric: host=algo-3, epoch=13, train total_loss <loss>=6.51813627052\u001b[0m\n",
      "\u001b[33m[2019-04-25 16:55:16.136] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 12, \"duration\": 101219, \"num_examples\": 1407}\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:55:22 INFO 139933765801792] # Finished training epoch 8 on 540000 examples from 4219 batches, each of size 128.\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:55:22 INFO 139933765801792] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:55:22 INFO 139933765801792] Loss (name: value) total: 6.51740765792\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:55:22 INFO 139933765801792] Loss (name: value) kld: 0.209322370506\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:55:22 INFO 139933765801792] Loss (name: value) recons: 6.30808528618\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:55:22 INFO 139933765801792] Loss (name: value) logppx: 6.51740765792\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:55:22 INFO 139933765801792] #quality_metric: host=algo-1, epoch=8, train total_loss <loss>=6.51740765792\u001b[0m\n",
      "\u001b[31m[2019-04-25 16:55:22.037] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 7, \"duration\": 162904, \"num_examples\": 1407}\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:55:27 INFO 140590214039360] Finished scoring on 179968 examples from 1406 batches, each of size 128.\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:55:27 INFO 140590214039360] Metrics for Inference:\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:55:27 INFO 140590214039360] Loss (name: value) total: 6.51518830466\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:55:27 INFO 140590214039360] Loss (name: value) kld: 0.2176140855\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:55:27 INFO 140590214039360] Loss (name: value) recons: 6.29757422166\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:55:27 INFO 140590214039360] Loss (name: value) logppx: 6.51518830466\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:55:27 INFO 140590214039360] #validation_score (13): 6.51518830466\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:55:27 INFO 140590214039360] patience losses:[6.5189554389476099, 6.5160608406935108, 6.5114099775913941, 6.5117061823224951, 6.5143546471385498] min patience loss:6.51140997759 current loss:6.51518830466 absolute loss difference:0.00377832707097\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:55:27 INFO 140590214039360] Bad epoch: loss has not improved (enough). Bad count:3\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:55:27 INFO 140590214039360] Timing: train: 89.27s, val: 11.68s, epoch: 100.96s\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:55:27 INFO 140590214039360] #progress_metric: host=algo-3, completed 52 % of epochs\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2813, \"sum\": 2813.0, \"min\": 2813}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2813, \"sum\": 2813.0, \"min\": 2813}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 360000, \"sum\": 360000.0, \"min\": 360000}, \"Total Batches Seen\": {\"count\": 1, \"max\": 36569, \"sum\": 36569.0, \"min\": 36569}, \"Total Records Seen\": {\"count\": 1, \"max\": 4680000, \"sum\": 4680000.0, \"min\": 4680000}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 360000, \"sum\": 360000.0, \"min\": 360000}, \"Reset Count\": {\"count\": 1, \"max\": 26, \"sum\": 26.0, \"min\": 26}}, \"EndTime\": 1556211327.820758, \"Dimensions\": {\"Host\": \"algo-3\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 12}, \"StartTime\": 1556211226.863206}\n",
      "\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:55:27 INFO 140590214039360] #throughput_metric: host=algo-3, train throughput=3565.84946807 records/second\u001b[0m\n",
      "\u001b[33m[2019-04-25 16:55:27.821] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 13, \"duration\": 100957, \"num_examples\": 2813}\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:55:27 INFO 140590214039360] \u001b[0m\n",
      "\u001b[33m[04/25/2019 16:55:27 INFO 140590214039360] # Starting training for epoch 14\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:55:37 INFO 139933765801792] Finished scoring on 179968 examples from 1406 batches, each of size 128.\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:55:37 INFO 139933765801792] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:55:37 INFO 139933765801792] Loss (name: value) total: 6.51192891072\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:55:37 INFO 139933765801792] Loss (name: value) kld: 0.214395271627\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:55:37 INFO 139933765801792] Loss (name: value) recons: 6.29753363556\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:55:37 INFO 139933765801792] Loss (name: value) logppx: 6.51192891072\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:55:37 INFO 139933765801792] #validation_score (8): 6.51192891072\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:55:37 INFO 139933765801792] patience losses:[6.5256874557918367, 6.5155224037034749, 6.513102688457006, 6.515548189198479, 6.5197250093538761] min patience loss:6.51310268846 current loss:6.51192891072 absolute loss difference:0.00117377773627\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:55:37 INFO 139933765801792] Timing: train: 147.11s, val: 15.33s, epoch: 162.44s\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:55:37 INFO 139933765801792] #progress_metric: host=algo-1, completed 32 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 4219, \"sum\": 4219.0, \"min\": 4219}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 4219, \"sum\": 4219.0, \"min\": 4219}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 540000, \"sum\": 540000.0, \"min\": 540000}, \"Total Batches Seen\": {\"count\": 1, \"max\": 33752, \"sum\": 33752.0, \"min\": 33752}, \"Total Records Seen\": {\"count\": 1, \"max\": 4320000, \"sum\": 4320000.0, \"min\": 4320000}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 540000, \"sum\": 540000.0, \"min\": 540000}, \"Reset Count\": {\"count\": 1, \"max\": 16, \"sum\": 16.0, \"min\": 16}}, \"EndTime\": 1556211337.368124, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 7}, \"StartTime\": 1556211174.925636}\n",
      "\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:55:37 INFO 139933765801792] #throughput_metric: host=algo-1, train throughput=3324.24351983 records/second\u001b[0m\n",
      "\u001b[31m[2019-04-25 16:55:37.368] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 8, \"duration\": 162442, \"num_examples\": 4219}\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:55:37 INFO 139933765801792] \u001b[0m\n",
      "\u001b[31m[04/25/2019 16:55:37 INFO 139933765801792] # Starting training for epoch 9\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[04/25/2019 16:55:48 INFO 139904211511104] # Finished training epoch 8 on 540000 examples from 4219 batches, each of size 128.\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:55:48 INFO 139904211511104] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:55:48 INFO 139904211511104] Loss (name: value) total: 6.51861211219\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:55:48 INFO 139904211511104] Loss (name: value) kld: 0.209564624161\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:55:48 INFO 139904211511104] Loss (name: value) recons: 6.30904748582\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:55:48 INFO 139904211511104] Loss (name: value) logppx: 6.51861211219\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:55:48 INFO 139904211511104] #quality_metric: host=algo-2, epoch=8, train total_loss <loss>=6.51861211219\u001b[0m\n",
      "\u001b[32m[2019-04-25 16:55:48.404] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 7, \"duration\": 166539, \"num_examples\": 1407}\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:56:04 INFO 139904211511104] Finished scoring on 179968 examples from 1406 batches, each of size 128.\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:56:04 INFO 139904211511104] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:56:04 INFO 139904211511104] Loss (name: value) total: 6.51774774235\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:56:04 INFO 139904211511104] Loss (name: value) kld: 0.220273745221\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:56:04 INFO 139904211511104] Loss (name: value) recons: 6.29747399531\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:56:04 INFO 139904211511104] Loss (name: value) logppx: 6.51774774235\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:56:04 INFO 139904211511104] #validation_score (8): 6.51774774235\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:56:04 INFO 139904211511104] patience losses:[6.5261999851948502, 6.5156724856554362, 6.5104343986104256, 6.5133759968650455, 6.5181606412781763] min patience loss:6.51043439861 current loss:6.51774774235 absolute loss difference:0.00731334374266\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:56:04 INFO 139904211511104] Bad epoch: loss has not improved (enough). Bad count:3\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:56:04 INFO 139904211511104] Timing: train: 151.69s, val: 15.64s, epoch: 167.33s\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:56:04 INFO 139904211511104] #progress_metric: host=algo-2, completed 32 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 4219, \"sum\": 4219.0, \"min\": 4219}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 4219, \"sum\": 4219.0, \"min\": 4219}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 540000, \"sum\": 540000.0, \"min\": 540000}, \"Total Batches Seen\": {\"count\": 1, \"max\": 33752, \"sum\": 33752.0, \"min\": 33752}, \"Total Records Seen\": {\"count\": 1, \"max\": 4320000, \"sum\": 4320000.0, \"min\": 4320000}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 540000, \"sum\": 540000.0, \"min\": 540000}, \"Reset Count\": {\"count\": 1, \"max\": 16, \"sum\": 16.0, \"min\": 16}}, \"EndTime\": 1556211364.042463, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 7}, \"StartTime\": 1556211196.714395}\n",
      "\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:56:04 INFO 139904211511104] #throughput_metric: host=algo-2, train throughput=3227.1900168 records/second\u001b[0m\n",
      "\u001b[32m[2019-04-25 16:56:04.042] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 8, \"duration\": 167327, \"num_examples\": 4219}\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:56:04 INFO 139904211511104] \u001b[0m\n",
      "\u001b[32m[04/25/2019 16:56:04 INFO 139904211511104] # Starting training for epoch 9\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:56:55 INFO 140590214039360] # Finished training epoch 14 on 360000 examples from 2813 batches, each of size 128.\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:56:55 INFO 140590214039360] Metrics for Training:\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:56:55 INFO 140590214039360] Loss (name: value) total: 6.51710478784\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:56:55 INFO 140590214039360] Loss (name: value) kld: 0.209279263741\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:56:55 INFO 140590214039360] Loss (name: value) recons: 6.30782552169\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:56:55 INFO 140590214039360] Loss (name: value) logppx: 6.51710478784\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:56:55 INFO 140590214039360] #quality_metric: host=algo-3, epoch=14, train total_loss <loss>=6.51710478784\u001b[0m\n",
      "\u001b[33m[2019-04-25 16:56:55.425] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 13, \"duration\": 99289, \"num_examples\": 1407}\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:57:07 INFO 140590214039360] Finished scoring on 179968 examples from 1406 batches, each of size 128.\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:57:07 INFO 140590214039360] Metrics for Inference:\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:57:07 INFO 140590214039360] Loss (name: value) total: 6.51779618168\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:57:07 INFO 140590214039360] Loss (name: value) kld: 0.194221890806\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:57:07 INFO 140590214039360] Loss (name: value) recons: 6.32357429068\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:57:07 INFO 140590214039360] Loss (name: value) logppx: 6.51779618168\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:57:07 INFO 140590214039360] #validation_score (14): 6.51779618168\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:57:07 INFO 140590214039360] patience losses:[6.5160608406935108, 6.5114099775913941, 6.5117061823224951, 6.5143546471385498, 6.5151883046623658] min patience loss:6.51140997759 current loss:6.51779618168 absolute loss difference:0.00638620409145\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:57:07 INFO 140590214039360] Bad epoch: loss has not improved (enough). Bad count:4\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:57:07 INFO 140590214039360] Timing: train: 87.60s, val: 11.82s, epoch: 99.42s\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:57:07 INFO 140590214039360] #progress_metric: host=algo-3, completed 56 % of epochs\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2813, \"sum\": 2813.0, \"min\": 2813}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2813, \"sum\": 2813.0, \"min\": 2813}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 360000, \"sum\": 360000.0, \"min\": 360000}, \"Total Batches Seen\": {\"count\": 1, \"max\": 39382, \"sum\": 39382.0, \"min\": 39382}, \"Total Records Seen\": {\"count\": 1, \"max\": 5040000, \"sum\": 5040000.0, \"min\": 5040000}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 360000, \"sum\": 360000.0, \"min\": 360000}, \"Reset Count\": {\"count\": 1, \"max\": 28, \"sum\": 28.0, \"min\": 28}}, \"EndTime\": 1556211427.243784, \"Dimensions\": {\"Host\": \"algo-3\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 13}, \"StartTime\": 1556211327.821091}\n",
      "\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:57:07 INFO 140590214039360] #throughput_metric: host=algo-3, train throughput=3620.89753919 records/second\u001b[0m\n",
      "\u001b[33m[2019-04-25 16:57:07.244] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 14, \"duration\": 99422, \"num_examples\": 2813}\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:57:07 INFO 140590214039360] \u001b[0m\n",
      "\u001b[33m[04/25/2019 16:57:07 INFO 140590214039360] # Starting training for epoch 15\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:58:04 INFO 139933765801792] # Finished training epoch 9 on 540000 examples from 4219 batches, each of size 128.\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:58:04 INFO 139933765801792] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:58:04 INFO 139933765801792] Loss (name: value) total: 6.51825425613\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:58:04 INFO 139933765801792] Loss (name: value) kld: 0.209238252582\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:58:04 INFO 139933765801792] Loss (name: value) recons: 6.30901600236\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:58:04 INFO 139933765801792] Loss (name: value) logppx: 6.51825425613\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:58:04 INFO 139933765801792] #quality_metric: host=algo-1, epoch=9, train total_loss <loss>=6.51825425613\u001b[0m\n",
      "\u001b[31m[2019-04-25 16:58:04.012] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 8, \"duration\": 161975, \"num_examples\": 1407}\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:58:19 INFO 139933765801792] Finished scoring on 179968 examples from 1406 batches, each of size 128.\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:58:19 INFO 139933765801792] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:58:19 INFO 139933765801792] Loss (name: value) total: 6.51912636099\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:58:19 INFO 139933765801792] Loss (name: value) kld: 0.21697649471\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:58:19 INFO 139933765801792] Loss (name: value) recons: 6.30214987167\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:58:19 INFO 139933765801792] Loss (name: value) logppx: 6.51912636099\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:58:19 INFO 139933765801792] #validation_score (9): 6.51912636099\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:58:19 INFO 139933765801792] patience losses:[6.5155224037034749, 6.513102688457006, 6.515548189198479, 6.5197250093538761, 6.5119289107207381] min patience loss:6.51192891072 current loss:6.51912636099 absolute loss difference:0.00719745026883\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:58:19 INFO 139933765801792] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:58:19 INFO 139933765801792] Timing: train: 146.64s, val: 14.99s, epoch: 161.64s\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:58:19 INFO 139933765801792] #progress_metric: host=algo-1, completed 36 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 4219, \"sum\": 4219.0, \"min\": 4219}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 4219, \"sum\": 4219.0, \"min\": 4219}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 540000, \"sum\": 540000.0, \"min\": 540000}, \"Total Batches Seen\": {\"count\": 1, \"max\": 37971, \"sum\": 37971.0, \"min\": 37971}, \"Total Records Seen\": {\"count\": 1, \"max\": 4860000, \"sum\": 4860000.0, \"min\": 4860000}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 540000, \"sum\": 540000.0, \"min\": 540000}, \"Reset Count\": {\"count\": 1, \"max\": 18, \"sum\": 18.0, \"min\": 18}}, \"EndTime\": 1556211499.006961, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 8}, \"StartTime\": 1556211337.36909}\n",
      "\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:58:19 INFO 139933765801792] #throughput_metric: host=algo-1, train throughput=3340.79755924 records/second\u001b[0m\n",
      "\u001b[31m[2019-04-25 16:58:19.007] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 9, \"duration\": 161636, \"num_examples\": 4219}\u001b[0m\n",
      "\u001b[31m[04/25/2019 16:58:19 INFO 139933765801792] \u001b[0m\n",
      "\u001b[31m[04/25/2019 16:58:19 INFO 139933765801792] # Starting training for epoch 10\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m[04/25/2019 16:58:31 INFO 140590214039360] # Finished training epoch 15 on 360000 examples from 2813 batches, each of size 128.\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:58:31 INFO 140590214039360] Metrics for Training:\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:58:31 INFO 140590214039360] Loss (name: value) total: 6.51687300226\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:58:31 INFO 140590214039360] Loss (name: value) kld: 0.20893855224\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:58:31 INFO 140590214039360] Loss (name: value) recons: 6.3079344516\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:58:31 INFO 140590214039360] Loss (name: value) logppx: 6.51687300226\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:58:31 INFO 140590214039360] #quality_metric: host=algo-3, epoch=15, train total_loss <loss>=6.51687300226\u001b[0m\n",
      "\u001b[33m[2019-04-25 16:58:31.725] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 14, \"duration\": 96299, \"num_examples\": 1407}\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:58:33 INFO 139904211511104] # Finished training epoch 9 on 540000 examples from 4219 batches, each of size 128.\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:58:33 INFO 139904211511104] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:58:33 INFO 139904211511104] Loss (name: value) total: 6.51859922086\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:58:33 INFO 139904211511104] Loss (name: value) kld: 0.209233428434\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:58:33 INFO 139904211511104] Loss (name: value) recons: 6.30936579197\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:58:33 INFO 139904211511104] Loss (name: value) logppx: 6.51859922086\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:58:33 INFO 139904211511104] #quality_metric: host=algo-2, epoch=9, train total_loss <loss>=6.51859922086\u001b[0m\n",
      "\u001b[32m[2019-04-25 16:58:33.532] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 8, \"duration\": 165126, \"num_examples\": 1407}\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:58:42 INFO 140590214039360] Finished scoring on 179968 examples from 1406 batches, each of size 128.\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:58:42 INFO 140590214039360] Metrics for Inference:\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:58:42 INFO 140590214039360] Loss (name: value) total: 6.52176022597\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:58:42 INFO 140590214039360] Loss (name: value) kld: 0.213275619519\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:58:42 INFO 140590214039360] Loss (name: value) recons: 6.30848460584\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:58:42 INFO 140590214039360] Loss (name: value) logppx: 6.52176022597\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:58:42 INFO 140590214039360] #validation_score (15): 6.52176022597\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:58:42 INFO 140590214039360] patience losses:[6.5114099775913941, 6.5117061823224951, 6.5143546471385498, 6.5151883046623658, 6.5177961816828418] min patience loss:6.51140997759 current loss:6.52176022597 absolute loss difference:0.0103502483829\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:58:42 INFO 140590214039360] Bad epoch: loss has not improved (enough). Bad count:5\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:58:42 INFO 140590214039360] Timing: train: 84.48s, val: 11.27s, epoch: 95.76s\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:58:43 INFO 140590214039360] #progress_metric: host=algo-3, completed 60 % of epochs\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2813, \"sum\": 2813.0, \"min\": 2813}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2813, \"sum\": 2813.0, \"min\": 2813}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 360000, \"sum\": 360000.0, \"min\": 360000}, \"Total Batches Seen\": {\"count\": 1, \"max\": 42195, \"sum\": 42195.0, \"min\": 42195}, \"Total Records Seen\": {\"count\": 1, \"max\": 5400000, \"sum\": 5400000.0, \"min\": 5400000}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 360000, \"sum\": 360000.0, \"min\": 360000}, \"Reset Count\": {\"count\": 1, \"max\": 30, \"sum\": 30.0, \"min\": 30}}, \"EndTime\": 1556211523.000145, \"Dimensions\": {\"Host\": \"algo-3\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 14}, \"StartTime\": 1556211427.244087}\n",
      "\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:58:43 INFO 140590214039360] #throughput_metric: host=algo-3, train throughput=3759.54752256 records/second\u001b[0m\n",
      "\u001b[33m[2019-04-25 16:58:43.000] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 15, \"duration\": 95755, \"num_examples\": 2813}\u001b[0m\n",
      "\u001b[33m[04/25/2019 16:58:43 INFO 140590214039360] \u001b[0m\n",
      "\u001b[33m[04/25/2019 16:58:43 INFO 140590214039360] # Starting training for epoch 16\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:58:47 INFO 139904211511104] Finished scoring on 179968 examples from 1406 batches, each of size 128.\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:58:47 INFO 139904211511104] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:58:47 INFO 139904211511104] Loss (name: value) total: 6.51373446988\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:58:47 INFO 139904211511104] Loss (name: value) kld: 0.205313401183\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:58:47 INFO 139904211511104] Loss (name: value) recons: 6.30842106475\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:58:47 INFO 139904211511104] Loss (name: value) logppx: 6.51373446988\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:58:47 INFO 139904211511104] #validation_score (9): 6.51373446988\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:58:47 INFO 139904211511104] patience losses:[6.5156724856554362, 6.5104343986104256, 6.5133759968650455, 6.5181606412781763, 6.5177477423530892] min patience loss:6.51043439861 current loss:6.51373446988 absolute loss difference:0.00330007127135\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:58:47 INFO 139904211511104] Bad epoch: loss has not improved (enough). Bad count:4\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:58:47 INFO 139904211511104] Timing: train: 149.49s, val: 14.41s, epoch: 163.90s\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:58:47 INFO 139904211511104] #progress_metric: host=algo-2, completed 36 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 4219, \"sum\": 4219.0, \"min\": 4219}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 4219, \"sum\": 4219.0, \"min\": 4219}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 540000, \"sum\": 540000.0, \"min\": 540000}, \"Total Batches Seen\": {\"count\": 1, \"max\": 37971, \"sum\": 37971.0, \"min\": 37971}, \"Total Records Seen\": {\"count\": 1, \"max\": 4860000, \"sum\": 4860000.0, \"min\": 4860000}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 540000, \"sum\": 540000.0, \"min\": 540000}, \"Reset Count\": {\"count\": 1, \"max\": 18, \"sum\": 18.0, \"min\": 18}}, \"EndTime\": 1556211527.939616, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 8}, \"StartTime\": 1556211364.042757}\n",
      "\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:58:47 INFO 139904211511104] #throughput_metric: host=algo-2, train throughput=3294.75174949 records/second\u001b[0m\n",
      "\u001b[32m[2019-04-25 16:58:47.939] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 9, \"duration\": 163896, \"num_examples\": 4219}\u001b[0m\n",
      "\u001b[32m[04/25/2019 16:58:47 INFO 139904211511104] \u001b[0m\n",
      "\u001b[32m[04/25/2019 16:58:47 INFO 139904211511104] # Starting training for epoch 10\u001b[0m\n",
      "\u001b[33m[04/25/2019 17:00:13 INFO 140590214039360] # Finished training epoch 16 on 360000 examples from 2813 batches, each of size 128.\u001b[0m\n",
      "\u001b[33m[04/25/2019 17:00:13 INFO 140590214039360] Metrics for Training:\u001b[0m\n",
      "\u001b[33m[04/25/2019 17:00:13 INFO 140590214039360] Loss (name: value) total: 6.51713271663\u001b[0m\n",
      "\u001b[33m[04/25/2019 17:00:13 INFO 140590214039360] Loss (name: value) kld: 0.208890345238\u001b[0m\n",
      "\u001b[33m[04/25/2019 17:00:13 INFO 140590214039360] Loss (name: value) recons: 6.30824237305\u001b[0m\n",
      "\u001b[33m[04/25/2019 17:00:13 INFO 140590214039360] Loss (name: value) logppx: 6.51713271663\u001b[0m\n",
      "\u001b[33m[04/25/2019 17:00:13 INFO 140590214039360] #quality_metric: host=algo-3, epoch=16, train total_loss <loss>=6.51713271663\u001b[0m\n",
      "\u001b[33m[2019-04-25 17:00:13.065] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 15, \"duration\": 101339, \"num_examples\": 1407}\u001b[0m\n",
      "\u001b[33m[04/25/2019 17:00:24 INFO 140590214039360] Finished scoring on 179968 examples from 1406 batches, each of size 128.\u001b[0m\n",
      "\u001b[33m[04/25/2019 17:00:24 INFO 140590214039360] Metrics for Inference:\u001b[0m\n",
      "\u001b[33m[04/25/2019 17:00:24 INFO 140590214039360] Loss (name: value) total: 6.51152048681\u001b[0m\n",
      "\u001b[33m[04/25/2019 17:00:24 INFO 140590214039360] Loss (name: value) kld: 0.201042412154\u001b[0m\n",
      "\u001b[33m[04/25/2019 17:00:24 INFO 140590214039360] Loss (name: value) recons: 6.31047808124\u001b[0m\n",
      "\u001b[33m[04/25/2019 17:00:24 INFO 140590214039360] Loss (name: value) logppx: 6.51152048681\u001b[0m\n",
      "\u001b[33m[04/25/2019 17:00:24 INFO 140590214039360] #validation_score (16): 6.51152048681\u001b[0m\n",
      "\u001b[33m[04/25/2019 17:00:24 INFO 140590214039360] patience losses:[6.5117061823224951, 6.5143546471385498, 6.5151883046623658, 6.5177961816828418, 6.5217602259743099] min patience loss:6.51170618232 current loss:6.51152048681 absolute loss difference:0.000185695515248\u001b[0m\n",
      "\u001b[33m[04/25/2019 17:00:24 INFO 140590214039360] Bad epoch: loss has not improved (enough). Bad count:6\u001b[0m\n",
      "\u001b[33m[04/25/2019 17:00:24 INFO 140590214039360] Bad epochs exceeded patience. Stopping training early!\u001b[0m\n",
      "\u001b[33m[04/25/2019 17:00:24 INFO 140590214039360] Timing: train: 90.06s, val: 11.43s, epoch: 101.50s\u001b[0m\n",
      "\u001b[33m[04/25/2019 17:00:24 INFO 140590214039360] Early stop condition met. Stopping training.\u001b[0m\n",
      "\u001b[33m[04/25/2019 17:00:24 INFO 140590214039360] #progress_metric: host=algo-3, completed 100 % epochs\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2813, \"sum\": 2813.0, \"min\": 2813}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2813, \"sum\": 2813.0, \"min\": 2813}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 360000, \"sum\": 360000.0, \"min\": 360000}, \"Total Batches Seen\": {\"count\": 1, \"max\": 45008, \"sum\": 45008.0, \"min\": 45008}, \"Total Records Seen\": {\"count\": 1, \"max\": 5760000, \"sum\": 5760000.0, \"min\": 5760000}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 360000, \"sum\": 360000.0, \"min\": 360000}, \"Reset Count\": {\"count\": 1, \"max\": 32, \"sum\": 32.0, \"min\": 32}}, \"EndTime\": 1556211624.499316, \"Dimensions\": {\"Host\": \"algo-3\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 15}, \"StartTime\": 1556211523.000452}\n",
      "\u001b[0m\n",
      "\u001b[33m[04/25/2019 17:00:24 INFO 140590214039360] #throughput_metric: host=algo-3, train throughput=3546.83213805 records/second\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[04/25/2019 17:00:41 INFO 139933765801792] # Finished training epoch 10 on 540000 examples from 4219 batches, each of size 128.\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:00:41 INFO 139933765801792] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:00:41 INFO 139933765801792] Loss (name: value) total: 6.51582853284\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:00:41 INFO 139933765801792] Loss (name: value) kld: 0.2091993313\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:00:41 INFO 139933765801792] Loss (name: value) recons: 6.30662920374\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:00:41 INFO 139933765801792] Loss (name: value) logppx: 6.51582853284\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:00:41 INFO 139933765801792] #quality_metric: host=algo-1, epoch=10, train total_loss <loss>=6.51582853284\u001b[0m\n",
      "\u001b[31m[2019-04-25 17:00:41.382] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 9, \"duration\": 157369, \"num_examples\": 1407}\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:00:56 INFO 139933765801792] Finished scoring on 179968 examples from 1406 batches, each of size 128.\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:00:56 INFO 139933765801792] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:00:56 INFO 139933765801792] Loss (name: value) total: 6.51136975098\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:00:56 INFO 139933765801792] Loss (name: value) kld: 0.208427754693\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:00:56 INFO 139933765801792] Loss (name: value) recons: 6.30294200095\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:00:56 INFO 139933765801792] Loss (name: value) logppx: 6.51136975098\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:00:56 INFO 139933765801792] #validation_score (10): 6.51136975098\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:00:56 INFO 139933765801792] patience losses:[6.513102688457006, 6.515548189198479, 6.5197250093538761, 6.5119289107207381, 6.5191263609895662] min patience loss:6.51192891072 current loss:6.51136975098 absolute loss difference:0.000559159736036\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:00:56 INFO 139933765801792] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:00:56 INFO 139933765801792] Timing: train: 142.38s, val: 14.66s, epoch: 157.03s\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:00:56 INFO 139933765801792] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 4219, \"sum\": 4219.0, \"min\": 4219}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 4219, \"sum\": 4219.0, \"min\": 4219}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 540000, \"sum\": 540000.0, \"min\": 540000}, \"Total Batches Seen\": {\"count\": 1, \"max\": 42190, \"sum\": 42190.0, \"min\": 42190}, \"Total Records Seen\": {\"count\": 1, \"max\": 5400000, \"sum\": 5400000.0, \"min\": 5400000}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 540000, \"sum\": 540000.0, \"min\": 540000}, \"Reset Count\": {\"count\": 1, \"max\": 20, \"sum\": 20.0, \"min\": 20}}, \"EndTime\": 1556211656.04022, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 9}, \"StartTime\": 1556211499.007257}\n",
      "\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:00:56 INFO 139933765801792] #throughput_metric: host=algo-1, train throughput=3438.76526175 records/second\u001b[0m\n",
      "\u001b[31m[2019-04-25 17:00:56.040] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 10, \"duration\": 157032, \"num_examples\": 4219}\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:00:56 INFO 139933765801792] \u001b[0m\n",
      "\u001b[31m[04/25/2019 17:00:56 INFO 139933765801792] # Starting training for epoch 11\u001b[0m\n",
      "\u001b[32m[04/25/2019 17:01:12 INFO 139904211511104] # Finished training epoch 10 on 540000 examples from 4219 batches, each of size 128.\u001b[0m\n",
      "\u001b[32m[04/25/2019 17:01:12 INFO 139904211511104] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[04/25/2019 17:01:12 INFO 139904211511104] Loss (name: value) total: 6.51639447638\u001b[0m\n",
      "\u001b[32m[04/25/2019 17:01:12 INFO 139904211511104] Loss (name: value) kld: 0.209630783237\u001b[0m\n",
      "\u001b[32m[04/25/2019 17:01:12 INFO 139904211511104] Loss (name: value) recons: 6.30676369804\u001b[0m\n",
      "\u001b[32m[04/25/2019 17:01:12 INFO 139904211511104] Loss (name: value) logppx: 6.51639447638\u001b[0m\n",
      "\u001b[32m[04/25/2019 17:01:12 INFO 139904211511104] #quality_metric: host=algo-2, epoch=10, train total_loss <loss>=6.51639447638\u001b[0m\n",
      "\u001b[32m[2019-04-25 17:01:12.072] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 9, \"duration\": 158539, \"num_examples\": 1407}\u001b[0m\n",
      "\u001b[32m[04/25/2019 17:01:26 INFO 139904211511104] Finished scoring on 179968 examples from 1406 batches, each of size 128.\u001b[0m\n",
      "\u001b[32m[04/25/2019 17:01:26 INFO 139904211511104] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[04/25/2019 17:01:26 INFO 139904211511104] Loss (name: value) total: 6.51067318482\u001b[0m\n",
      "\u001b[32m[04/25/2019 17:01:26 INFO 139904211511104] Loss (name: value) kld: 0.210015536245\u001b[0m\n",
      "\u001b[32m[04/25/2019 17:01:26 INFO 139904211511104] Loss (name: value) recons: 6.30065764947\u001b[0m\n",
      "\u001b[32m[04/25/2019 17:01:26 INFO 139904211511104] Loss (name: value) logppx: 6.51067318482\u001b[0m\n",
      "\u001b[32m[04/25/2019 17:01:26 INFO 139904211511104] #validation_score (10): 6.51067318482\u001b[0m\n",
      "\u001b[32m[04/25/2019 17:01:26 INFO 139904211511104] patience losses:[6.5104343986104256, 6.5133759968650455, 6.5181606412781763, 6.5177477423530892, 6.5137344698817765] min patience loss:6.51043439861 current loss:6.51067318482 absolute loss difference:0.000238786211733\u001b[0m\n",
      "\u001b[32m[04/25/2019 17:01:26 INFO 139904211511104] Bad epoch: loss has not improved (enough). Bad count:5\u001b[0m\n",
      "\u001b[32m[04/25/2019 17:01:26 INFO 139904211511104] Timing: train: 144.13s, val: 14.75s, epoch: 158.88s\u001b[0m\n",
      "\u001b[32m[04/25/2019 17:01:26 INFO 139904211511104] #progress_metric: host=algo-2, completed 40 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 4219, \"sum\": 4219.0, \"min\": 4219}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 4219, \"sum\": 4219.0, \"min\": 4219}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 540000, \"sum\": 540000.0, \"min\": 540000}, \"Total Batches Seen\": {\"count\": 1, \"max\": 42190, \"sum\": 42190.0, \"min\": 42190}, \"Total Records Seen\": {\"count\": 1, \"max\": 5400000, \"sum\": 5400000.0, \"min\": 5400000}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 540000, \"sum\": 540000.0, \"min\": 540000}, \"Reset Count\": {\"count\": 1, \"max\": 20, \"sum\": 20.0, \"min\": 20}}, \"EndTime\": 1556211686.818795, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 9}, \"StartTime\": 1556211527.939943}\n",
      "\u001b[0m\n",
      "\u001b[32m[04/25/2019 17:01:26 INFO 139904211511104] #throughput_metric: host=algo-2, train throughput=3398.81254659 records/second\u001b[0m\n",
      "\u001b[32m[2019-04-25 17:01:26.819] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 10, \"duration\": 158878, \"num_examples\": 4219}\u001b[0m\n",
      "\u001b[32m[04/25/2019 17:01:26 INFO 139904211511104] \u001b[0m\n",
      "\u001b[32m[04/25/2019 17:01:26 INFO 139904211511104] # Starting training for epoch 11\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:03:06 INFO 139933765801792] # Finished training epoch 11 on 540000 examples from 4219 batches, each of size 128.\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:03:06 INFO 139933765801792] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:03:06 INFO 139933765801792] Loss (name: value) total: 6.51201715173\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:03:06 INFO 139933765801792] Loss (name: value) kld: 0.210930181407\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:03:06 INFO 139933765801792] Loss (name: value) recons: 6.30108697382\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:03:06 INFO 139933765801792] Loss (name: value) logppx: 6.51201715173\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:03:06 INFO 139933765801792] #quality_metric: host=algo-1, epoch=11, train total_loss <loss>=6.51201715173\u001b[0m\n",
      "\u001b[31m[2019-04-25 17:03:06.420] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 10, \"duration\": 145038, \"num_examples\": 1407}\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:03:21 INFO 139933765801792] Finished scoring on 179968 examples from 1406 batches, each of size 128.\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:03:21 INFO 139933765801792] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:03:21 INFO 139933765801792] Loss (name: value) total: 6.5138138731\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:03:21 INFO 139933765801792] Loss (name: value) kld: 0.205282829734\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:03:21 INFO 139933765801792] Loss (name: value) recons: 6.30853104184\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:03:21 INFO 139933765801792] Loss (name: value) logppx: 6.5138138731\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:03:21 INFO 139933765801792] #validation_score (11): 6.5138138731\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:03:21 INFO 139933765801792] patience losses:[6.515548189198479, 6.5197250093538761, 6.5119289107207381, 6.5191263609895662, 6.5113697509847022] min patience loss:6.51136975098 current loss:6.5138138731 absolute loss difference:0.00244412211911\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:03:21 INFO 139933765801792] Bad epoch: loss has not improved (enough). Bad count:3\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:03:21 INFO 139933765801792] Timing: train: 130.38s, val: 14.88s, epoch: 145.26s\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:03:21 INFO 139933765801792] #progress_metric: host=algo-1, completed 44 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 4219, \"sum\": 4219.0, \"min\": 4219}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 4219, \"sum\": 4219.0, \"min\": 4219}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 540000, \"sum\": 540000.0, \"min\": 540000}, \"Total Batches Seen\": {\"count\": 1, \"max\": 46409, \"sum\": 46409.0, \"min\": 46409}, \"Total Records Seen\": {\"count\": 1, \"max\": 5940000, \"sum\": 5940000.0, \"min\": 5940000}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 540000, \"sum\": 540000.0, \"min\": 540000}, \"Reset Count\": {\"count\": 1, \"max\": 22, \"sum\": 22.0, \"min\": 22}}, \"EndTime\": 1556211801.298922, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 10}, \"StartTime\": 1556211656.040511}\n",
      "\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:03:21 INFO 139933765801792] #throughput_metric: host=algo-1, train throughput=3717.50927815 records/second\u001b[0m\n",
      "\u001b[31m[2019-04-25 17:03:21.299] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 11, \"duration\": 145258, \"num_examples\": 4219}\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:03:21 INFO 139933765801792] \u001b[0m\n",
      "\u001b[31m[04/25/2019 17:03:21 INFO 139933765801792] # Starting training for epoch 12\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[04/25/2019 17:03:39 INFO 139904211511104] # Finished training epoch 11 on 540000 examples from 4219 batches, each of size 128.\u001b[0m\n",
      "\u001b[32m[04/25/2019 17:03:39 INFO 139904211511104] Metrics for Training:\u001b[0m\n",
      "\u001b[32m[04/25/2019 17:03:39 INFO 139904211511104] Loss (name: value) total: 6.5135724058\u001b[0m\n",
      "\u001b[32m[04/25/2019 17:03:39 INFO 139904211511104] Loss (name: value) kld: 0.211448285924\u001b[0m\n",
      "\u001b[32m[04/25/2019 17:03:39 INFO 139904211511104] Loss (name: value) recons: 6.30212411487\u001b[0m\n",
      "\u001b[32m[04/25/2019 17:03:39 INFO 139904211511104] Loss (name: value) logppx: 6.5135724058\u001b[0m\n",
      "\u001b[32m[04/25/2019 17:03:39 INFO 139904211511104] #quality_metric: host=algo-2, epoch=11, train total_loss <loss>=6.5135724058\u001b[0m\n",
      "\u001b[32m[2019-04-25 17:03:39.852] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 10, \"duration\": 147780, \"num_examples\": 1407}\u001b[0m\n",
      "\u001b[32m[04/25/2019 17:03:54 INFO 139904211511104] Finished scoring on 179968 examples from 1406 batches, each of size 128.\u001b[0m\n",
      "\u001b[32m[04/25/2019 17:03:54 INFO 139904211511104] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[04/25/2019 17:03:54 INFO 139904211511104] Loss (name: value) total: 6.51140493078\u001b[0m\n",
      "\u001b[32m[04/25/2019 17:03:54 INFO 139904211511104] Loss (name: value) kld: 0.211432102331\u001b[0m\n",
      "\u001b[32m[04/25/2019 17:03:54 INFO 139904211511104] Loss (name: value) recons: 6.2999728177\u001b[0m\n",
      "\u001b[32m[04/25/2019 17:03:54 INFO 139904211511104] Loss (name: value) logppx: 6.51140493078\u001b[0m\n",
      "\u001b[32m[04/25/2019 17:03:54 INFO 139904211511104] #validation_score (11): 6.51140493078\u001b[0m\n",
      "\u001b[32m[04/25/2019 17:03:54 INFO 139904211511104] patience losses:[6.5133759968650455, 6.5181606412781763, 6.5177477423530892, 6.5137344698817765, 6.5106731848221582] min patience loss:6.51067318482 current loss:6.51140493078 absolute loss difference:0.000731745960024\u001b[0m\n",
      "\u001b[32m[04/25/2019 17:03:54 INFO 139904211511104] Bad epoch: loss has not improved (enough). Bad count:6\u001b[0m\n",
      "\u001b[32m[04/25/2019 17:03:54 INFO 139904211511104] Bad epochs exceeded patience. Stopping training early!\u001b[0m\n",
      "\u001b[32m[04/25/2019 17:03:54 INFO 139904211511104] Timing: train: 133.03s, val: 14.89s, epoch: 147.93s\u001b[0m\n",
      "\u001b[32m[04/25/2019 17:03:54 INFO 139904211511104] Early stop condition met. Stopping training.\u001b[0m\n",
      "\u001b[32m[04/25/2019 17:03:54 INFO 139904211511104] #progress_metric: host=algo-2, completed 100 % epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 4219, \"sum\": 4219.0, \"min\": 4219}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 4219, \"sum\": 4219.0, \"min\": 4219}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 540000, \"sum\": 540000.0, \"min\": 540000}, \"Total Batches Seen\": {\"count\": 1, \"max\": 46409, \"sum\": 46409.0, \"min\": 46409}, \"Total Records Seen\": {\"count\": 1, \"max\": 5940000, \"sum\": 5940000.0, \"min\": 5940000}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 540000, \"sum\": 540000.0, \"min\": 540000}, \"Reset Count\": {\"count\": 1, \"max\": 22, \"sum\": 22.0, \"min\": 22}}, \"EndTime\": 1556211834.746632, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 10}, \"StartTime\": 1556211686.819107}\n",
      "\u001b[0m\n",
      "\u001b[32m[04/25/2019 17:03:54 INFO 139904211511104] #throughput_metric: host=algo-2, train throughput=3650.43228005 records/second\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:05:13 INFO 139933765801792] # Finished training epoch 12 on 540000 examples from 4219 batches, each of size 128.\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:05:13 INFO 139933765801792] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:05:13 INFO 139933765801792] Loss (name: value) total: 6.51087962894\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:05:13 INFO 139933765801792] Loss (name: value) kld: 0.211822982679\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:05:13 INFO 139933765801792] Loss (name: value) recons: 6.29905664856\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:05:13 INFO 139933765801792] Loss (name: value) logppx: 6.51087962894\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:05:13 INFO 139933765801792] #quality_metric: host=algo-1, epoch=12, train total_loss <loss>=6.51087962894\u001b[0m\n",
      "\u001b[31m[2019-04-25 17:05:13.332] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 11, \"duration\": 126911, \"num_examples\": 1407}\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:05:23 INFO 139933765801792] Finished scoring on 179968 examples from 1406 batches, each of size 128.\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:05:23 INFO 139933765801792] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:05:23 INFO 139933765801792] Loss (name: value) total: 6.50857037365\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:05:23 INFO 139933765801792] Loss (name: value) kld: 0.213907480378\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:05:23 INFO 139933765801792] Loss (name: value) recons: 6.29466289341\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:05:23 INFO 139933765801792] Loss (name: value) logppx: 6.50857037365\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:05:23 INFO 139933765801792] #validation_score (12): 6.50857037365\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:05:23 INFO 139933765801792] patience losses:[6.5197250093538761, 6.5119289107207381, 6.5191263609895662, 6.5113697509847022, 6.5138138731038078] min patience loss:6.51136975098 current loss:6.50857037365 absolute loss difference:0.00279937733288\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:05:23 INFO 139933765801792] Timing: train: 112.03s, val: 10.09s, epoch: 122.12s\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:05:23 INFO 139933765801792] #progress_metric: host=algo-1, completed 48 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 4219, \"sum\": 4219.0, \"min\": 4219}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 4219, \"sum\": 4219.0, \"min\": 4219}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 540000, \"sum\": 540000.0, \"min\": 540000}, \"Total Batches Seen\": {\"count\": 1, \"max\": 50628, \"sum\": 50628.0, \"min\": 50628}, \"Total Records Seen\": {\"count\": 1, \"max\": 6480000, \"sum\": 6480000.0, \"min\": 6480000}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 540000, \"sum\": 540000.0, \"min\": 540000}, \"Reset Count\": {\"count\": 1, \"max\": 24, \"sum\": 24.0, \"min\": 24}}, \"EndTime\": 1556211923.420532, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 11}, \"StartTime\": 1556211801.299178}\n",
      "\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:05:23 INFO 139933765801792] #throughput_metric: host=algo-1, train throughput=4421.82616665 records/second\u001b[0m\n",
      "\u001b[31m[2019-04-25 17:05:23.420] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 12, \"duration\": 122121, \"num_examples\": 4219}\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:05:23 INFO 139933765801792] \u001b[0m\n",
      "\u001b[31m[04/25/2019 17:05:23 INFO 139933765801792] # Starting training for epoch 13\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:07:09 INFO 139933765801792] # Finished training epoch 13 on 540000 examples from 4219 batches, each of size 128.\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:07:09 INFO 139933765801792] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:07:09 INFO 139933765801792] Loss (name: value) total: 6.51054573409\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:07:09 INFO 139933765801792] Loss (name: value) kld: 0.212001560582\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:07:09 INFO 139933765801792] Loss (name: value) recons: 6.29854417542\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:07:09 INFO 139933765801792] Loss (name: value) logppx: 6.51054573409\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:07:09 INFO 139933765801792] #quality_metric: host=algo-1, epoch=13, train total_loss <loss>=6.51054573409\u001b[0m\n",
      "\u001b[31m[2019-04-25 17:07:09.878] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 12, \"duration\": 116545, \"num_examples\": 1407}\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:07:19 INFO 139933765801792] Finished scoring on 179968 examples from 1406 batches, each of size 128.\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:07:19 INFO 139933765801792] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:07:19 INFO 139933765801792] Loss (name: value) total: 6.51047441936\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:07:19 INFO 139933765801792] Loss (name: value) kld: 0.216664599067\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:07:19 INFO 139933765801792] Loss (name: value) recons: 6.29380982224\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:07:19 INFO 139933765801792] Loss (name: value) logppx: 6.51047441936\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:07:19 INFO 139933765801792] #validation_score (13): 6.51047441936\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:07:19 INFO 139933765801792] patience losses:[6.5119289107207381, 6.5191263609895662, 6.5113697509847022, 6.5138138731038078, 6.5085703736518221] min patience loss:6.50857037365 current loss:6.51047441936 absolute loss difference:0.00190404570459\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:07:19 INFO 139933765801792] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:07:19 INFO 139933765801792] Timing: train: 106.46s, val: 10.07s, epoch: 116.53s\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:07:19 INFO 139933765801792] #progress_metric: host=algo-1, completed 52 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 4219, \"sum\": 4219.0, \"min\": 4219}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 4219, \"sum\": 4219.0, \"min\": 4219}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 540000, \"sum\": 540000.0, \"min\": 540000}, \"Total Batches Seen\": {\"count\": 1, \"max\": 54847, \"sum\": 54847.0, \"min\": 54847}, \"Total Records Seen\": {\"count\": 1, \"max\": 7020000, \"sum\": 7020000.0, \"min\": 7020000}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 540000, \"sum\": 540000.0, \"min\": 540000}, \"Reset Count\": {\"count\": 1, \"max\": 26, \"sum\": 26.0, \"min\": 26}}, \"EndTime\": 1556212039.946807, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 12}, \"StartTime\": 1556211923.4208}\n",
      "\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:07:19 INFO 139933765801792] #throughput_metric: host=algo-1, train throughput=4634.15309643 records/second\u001b[0m\n",
      "\u001b[31m[2019-04-25 17:07:19.947] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 13, \"duration\": 116525, \"num_examples\": 4219}\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:07:19 INFO 139933765801792] \u001b[0m\n",
      "\u001b[31m[04/25/2019 17:07:19 INFO 139933765801792] # Starting training for epoch 14\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[04/25/2019 17:09:04 INFO 139933765801792] # Finished training epoch 14 on 540000 examples from 4219 batches, each of size 128.\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:09:04 INFO 139933765801792] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:09:04 INFO 139933765801792] Loss (name: value) total: 6.51065680336\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:09:04 INFO 139933765801792] Loss (name: value) kld: 0.212145554808\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:09:04 INFO 139933765801792] Loss (name: value) recons: 6.29851125015\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:09:04 INFO 139933765801792] Loss (name: value) logppx: 6.51065680336\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:09:04 INFO 139933765801792] #quality_metric: host=algo-1, epoch=14, train total_loss <loss>=6.51065680336\u001b[0m\n",
      "\u001b[31m[2019-04-25 17:09:04.336] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 13, \"duration\": 114457, \"num_examples\": 1407}\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:09:14 INFO 139933765801792] Finished scoring on 179968 examples from 1406 batches, each of size 128.\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:09:14 INFO 139933765801792] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:09:14 INFO 139933765801792] Loss (name: value) total: 6.51088157148\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:09:14 INFO 139933765801792] Loss (name: value) kld: 0.21585571289\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:09:14 INFO 139933765801792] Loss (name: value) recons: 6.29502585568\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:09:14 INFO 139933765801792] Loss (name: value) logppx: 6.51088157148\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:09:14 INFO 139933765801792] #validation_score (14): 6.51088157148\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:09:14 INFO 139933765801792] patience losses:[6.5191263609895662, 6.5113697509847022, 6.5138138731038078, 6.5085703736518221, 6.5104744193564095] min patience loss:6.50857037365 current loss:6.51088157148 absolute loss difference:0.00231119782623\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:09:14 INFO 139933765801792] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:09:14 INFO 139933765801792] Timing: train: 104.39s, val: 9.84s, epoch: 114.23s\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:09:14 INFO 139933765801792] #progress_metric: host=algo-1, completed 56 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 4219, \"sum\": 4219.0, \"min\": 4219}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 4219, \"sum\": 4219.0, \"min\": 4219}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 540000, \"sum\": 540000.0, \"min\": 540000}, \"Total Batches Seen\": {\"count\": 1, \"max\": 59066, \"sum\": 59066.0, \"min\": 59066}, \"Total Records Seen\": {\"count\": 1, \"max\": 7560000, \"sum\": 7560000.0, \"min\": 7560000}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 540000, \"sum\": 540000.0, \"min\": 540000}, \"Reset Count\": {\"count\": 1, \"max\": 28, \"sum\": 28.0, \"min\": 28}}, \"EndTime\": 1556212154.178888, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 13}, \"StartTime\": 1556212039.947074}\n",
      "\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:09:14 INFO 139933765801792] #throughput_metric: host=algo-1, train throughput=4727.22373327 records/second\u001b[0m\n",
      "\u001b[31m[2019-04-25 17:09:14.179] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 14, \"duration\": 114231, \"num_examples\": 4219}\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:09:14 INFO 139933765801792] \u001b[0m\n",
      "\u001b[31m[04/25/2019 17:09:14 INFO 139933765801792] # Starting training for epoch 15\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:10:59 INFO 139933765801792] # Finished training epoch 15 on 540000 examples from 4219 batches, each of size 128.\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:10:59 INFO 139933765801792] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:10:59 INFO 139933765801792] Loss (name: value) total: 6.51075326574\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:10:59 INFO 139933765801792] Loss (name: value) kld: 0.212253554246\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:10:59 INFO 139933765801792] Loss (name: value) recons: 6.29849971089\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:10:59 INFO 139933765801792] Loss (name: value) logppx: 6.51075326574\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:10:59 INFO 139933765801792] #quality_metric: host=algo-1, epoch=15, train total_loss <loss>=6.51075326574\u001b[0m\n",
      "\u001b[31m[2019-04-25 17:10:59.728] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 14, \"duration\": 115391, \"num_examples\": 1407}\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:11:09 INFO 139933765801792] Finished scoring on 179968 examples from 1406 batches, each of size 128.\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:11:09 INFO 139933765801792] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:11:09 INFO 139933765801792] Loss (name: value) total: 6.51043969944\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:11:09 INFO 139933765801792] Loss (name: value) kld: 0.219926659394\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:11:09 INFO 139933765801792] Loss (name: value) recons: 6.29051303897\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:11:09 INFO 139933765801792] Loss (name: value) logppx: 6.51043969944\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:11:09 INFO 139933765801792] #validation_score (15): 6.51043969944\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:11:09 INFO 139933765801792] patience losses:[6.5113697509847022, 6.5138138731038078, 6.5085703736518221, 6.5104744193564095, 6.5108815714780501] min patience loss:6.50857037365 current loss:6.51043969944 absolute loss difference:0.00186932578704\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:11:09 INFO 139933765801792] Bad epoch: loss has not improved (enough). Bad count:3\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:11:09 INFO 139933765801792] Timing: train: 105.55s, val: 9.93s, epoch: 115.48s\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:11:09 INFO 139933765801792] #progress_metric: host=algo-1, completed 60 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 4219, \"sum\": 4219.0, \"min\": 4219}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 4219, \"sum\": 4219.0, \"min\": 4219}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 540000, \"sum\": 540000.0, \"min\": 540000}, \"Total Batches Seen\": {\"count\": 1, \"max\": 63285, \"sum\": 63285.0, \"min\": 63285}, \"Total Records Seen\": {\"count\": 1, \"max\": 8100000, \"sum\": 8100000.0, \"min\": 8100000}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 540000, \"sum\": 540000.0, \"min\": 540000}, \"Reset Count\": {\"count\": 1, \"max\": 30, \"sum\": 30.0, \"min\": 30}}, \"EndTime\": 1556212269.658019, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 14}, \"StartTime\": 1556212154.179182}\n",
      "\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:11:09 INFO 139933765801792] #throughput_metric: host=algo-1, train throughput=4676.17529072 records/second\u001b[0m\n",
      "\u001b[31m[2019-04-25 17:11:09.658] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 15, \"duration\": 115478, \"num_examples\": 4219}\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:11:09 INFO 139933765801792] \u001b[0m\n",
      "\u001b[31m[04/25/2019 17:11:09 INFO 139933765801792] # Starting training for epoch 16\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:12:56 INFO 139933765801792] # Finished training epoch 16 on 540000 examples from 4219 batches, each of size 128.\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:12:56 INFO 139933765801792] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:12:56 INFO 139933765801792] Loss (name: value) total: 6.51087765989\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:12:56 INFO 139933765801792] Loss (name: value) kld: 0.213056833322\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:12:56 INFO 139933765801792] Loss (name: value) recons: 6.29782082724\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:12:56 INFO 139933765801792] Loss (name: value) logppx: 6.51087765989\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:12:56 INFO 139933765801792] #quality_metric: host=algo-1, epoch=16, train total_loss <loss>=6.51087765989\u001b[0m\n",
      "\u001b[31m[2019-04-25 17:12:56.264] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 15, \"duration\": 116535, \"num_examples\": 1407}\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:13:06 INFO 139933765801792] Finished scoring on 179968 examples from 1406 batches, each of size 128.\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:13:06 INFO 139933765801792] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:13:06 INFO 139933765801792] Loss (name: value) total: 6.51191667303\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:13:06 INFO 139933765801792] Loss (name: value) kld: 0.213443421675\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:13:06 INFO 139933765801792] Loss (name: value) recons: 6.29847325404\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:13:06 INFO 139933765801792] Loss (name: value) logppx: 6.51191667303\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:13:06 INFO 139933765801792] #validation_score (16): 6.51191667303\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:13:06 INFO 139933765801792] patience losses:[6.5138138731038078, 6.5085703736518221, 6.5104744193564095, 6.5108815714780501, 6.5104396994388631] min patience loss:6.50857037365 current loss:6.51191667303 absolute loss difference:0.003346299379\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:13:06 INFO 139933765801792] Bad epoch: loss has not improved (enough). Bad count:4\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:13:06 INFO 139933765801792] Timing: train: 106.61s, val: 10.22s, epoch: 116.83s\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:13:06 INFO 139933765801792] #progress_metric: host=algo-1, completed 64 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 4219, \"sum\": 4219.0, \"min\": 4219}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 4219, \"sum\": 4219.0, \"min\": 4219}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 540000, \"sum\": 540000.0, \"min\": 540000}, \"Total Batches Seen\": {\"count\": 1, \"max\": 67504, \"sum\": 67504.0, \"min\": 67504}, \"Total Records Seen\": {\"count\": 1, \"max\": 8640000, \"sum\": 8640000.0, \"min\": 8640000}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 540000, \"sum\": 540000.0, \"min\": 540000}, \"Reset Count\": {\"count\": 1, \"max\": 32, \"sum\": 32.0, \"min\": 32}}, \"EndTime\": 1556212386.48778, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 15}, \"StartTime\": 1556212269.65833}\n",
      "\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:13:06 INFO 139933765801792] #throughput_metric: host=algo-1, train throughput=4622.11662991 records/second\u001b[0m\n",
      "\u001b[31m[2019-04-25 17:13:06.487] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 16, \"duration\": 116829, \"num_examples\": 4219}\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:13:06 INFO 139933765801792] \u001b[0m\n",
      "\u001b[31m[04/25/2019 17:13:06 INFO 139933765801792] # Starting training for epoch 17\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[04/25/2019 17:14:54 INFO 139933765801792] # Finished training epoch 17 on 540000 examples from 4219 batches, each of size 128.\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:14:54 INFO 139933765801792] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:14:54 INFO 139933765801792] Loss (name: value) total: 6.51077189754\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:14:54 INFO 139933765801792] Loss (name: value) kld: 0.21335203692\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:14:54 INFO 139933765801792] Loss (name: value) recons: 6.29741986495\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:14:54 INFO 139933765801792] Loss (name: value) logppx: 6.51077189754\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:14:54 INFO 139933765801792] #quality_metric: host=algo-1, epoch=17, train total_loss <loss>=6.51077189754\u001b[0m\n",
      "\u001b[31m[2019-04-25 17:14:54.097] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 16, \"duration\": 117832, \"num_examples\": 1407}\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:15:03 INFO 139933765801792] Finished scoring on 179968 examples from 1406 batches, each of size 128.\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:15:03 INFO 139933765801792] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:15:03 INFO 139933765801792] Loss (name: value) total: 6.51099110871\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:15:03 INFO 139933765801792] Loss (name: value) kld: 0.217408475939\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:15:03 INFO 139933765801792] Loss (name: value) recons: 6.29358263714\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:15:03 INFO 139933765801792] Loss (name: value) logppx: 6.51099110871\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:15:03 INFO 139933765801792] #validation_score (17): 6.51099110871\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:15:03 INFO 139933765801792] patience losses:[6.5085703736518221, 6.5104744193564095, 6.5108815714780501, 6.5104396994388631, 6.511916673030826] min patience loss:6.50857037365 current loss:6.51099110871 absolute loss difference:0.00242073505396\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:15:03 INFO 139933765801792] Bad epoch: loss has not improved (enough). Bad count:5\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:15:03 INFO 139933765801792] Timing: train: 107.61s, val: 9.89s, epoch: 117.50s\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:15:03 INFO 139933765801792] #progress_metric: host=algo-1, completed 68 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 4219, \"sum\": 4219.0, \"min\": 4219}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 4219, \"sum\": 4219.0, \"min\": 4219}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 540000, \"sum\": 540000.0, \"min\": 540000}, \"Total Batches Seen\": {\"count\": 1, \"max\": 71723, \"sum\": 71723.0, \"min\": 71723}, \"Total Records Seen\": {\"count\": 1, \"max\": 9180000, \"sum\": 9180000.0, \"min\": 9180000}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 540000, \"sum\": 540000.0, \"min\": 540000}, \"Reset Count\": {\"count\": 1, \"max\": 34, \"sum\": 34.0, \"min\": 34}}, \"EndTime\": 1556212503.991607, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 16}, \"StartTime\": 1556212386.488062}\n",
      "\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:15:03 INFO 139933765801792] #throughput_metric: host=algo-1, train throughput=4595.60028468 records/second\u001b[0m\n",
      "\u001b[31m[2019-04-25 17:15:03.991] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 17, \"duration\": 117503, \"num_examples\": 4219}\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:15:03 INFO 139933765801792] \u001b[0m\n",
      "\u001b[31m[04/25/2019 17:15:03 INFO 139933765801792] # Starting training for epoch 18\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:16:51 INFO 139933765801792] # Finished training epoch 18 on 540000 examples from 4219 batches, each of size 128.\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:16:51 INFO 139933765801792] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:16:51 INFO 139933765801792] Loss (name: value) total: 6.51000903754\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:16:51 INFO 139933765801792] Loss (name: value) kld: 0.214836178208\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:16:51 INFO 139933765801792] Loss (name: value) recons: 6.29517286156\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:16:51 INFO 139933765801792] Loss (name: value) logppx: 6.51000903754\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:16:51 INFO 139933765801792] #quality_metric: host=algo-1, epoch=18, train total_loss <loss>=6.51000903754\u001b[0m\n",
      "\u001b[31m[2019-04-25 17:16:51.482] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 17, \"duration\": 117384, \"num_examples\": 1407}\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:17:01 INFO 139933765801792] Finished scoring on 179968 examples from 1406 batches, each of size 128.\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:17:01 INFO 139933765801792] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:17:01 INFO 139933765801792] Loss (name: value) total: 6.50778665515\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:17:01 INFO 139933765801792] Loss (name: value) kld: 0.221116317603\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:17:01 INFO 139933765801792] Loss (name: value) recons: 6.28667033482\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:17:01 INFO 139933765801792] Loss (name: value) logppx: 6.50778665515\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:17:01 INFO 139933765801792] #validation_score (18): 6.50778665515\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:17:01 INFO 139933765801792] patience losses:[6.5104744193564095, 6.5108815714780501, 6.5104396994388631, 6.511916673030826, 6.5109911087057837] min patience loss:6.51043969944 current loss:6.50778665515 absolute loss difference:0.00265304428415\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:17:01 INFO 139933765801792] Timing: train: 107.49s, val: 10.14s, epoch: 117.63s\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:17:01 INFO 139933765801792] #progress_metric: host=algo-1, completed 72 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 4219, \"sum\": 4219.0, \"min\": 4219}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 4219, \"sum\": 4219.0, \"min\": 4219}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 540000, \"sum\": 540000.0, \"min\": 540000}, \"Total Batches Seen\": {\"count\": 1, \"max\": 75942, \"sum\": 75942.0, \"min\": 75942}, \"Total Records Seen\": {\"count\": 1, \"max\": 9720000, \"sum\": 9720000.0, \"min\": 9720000}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 540000, \"sum\": 540000.0, \"min\": 540000}, \"Reset Count\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}}, \"EndTime\": 1556212621.620673, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 17}, \"StartTime\": 1556212503.991857}\n",
      "\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:17:01 INFO 139933765801792] #throughput_metric: host=algo-1, train throughput=4590.70665709 records/second\u001b[0m\n",
      "\u001b[31m[2019-04-25 17:17:01.620] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 18, \"duration\": 117628, \"num_examples\": 4219}\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:17:01 INFO 139933765801792] \u001b[0m\n",
      "\u001b[31m[04/25/2019 17:17:01 INFO 139933765801792] # Starting training for epoch 19\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:18:50 INFO 139933765801792] # Finished training epoch 19 on 540000 examples from 4219 batches, each of size 128.\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:18:50 INFO 139933765801792] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:18:50 INFO 139933765801792] Loss (name: value) total: 6.50917744676\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:18:50 INFO 139933765801792] Loss (name: value) kld: 0.217148431848\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:18:50 INFO 139933765801792] Loss (name: value) recons: 6.29202901789\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:18:50 INFO 139933765801792] Loss (name: value) logppx: 6.50917744676\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:18:50 INFO 139933765801792] #quality_metric: host=algo-1, epoch=19, train total_loss <loss>=6.50917744676\u001b[0m\n",
      "\u001b[31m[2019-04-25 17:18:50.016] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 18, \"duration\": 118534, \"num_examples\": 1407}\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:18:59 INFO 139933765801792] Finished scoring on 179968 examples from 1406 batches, each of size 128.\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:18:59 INFO 139933765801792] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:18:59 INFO 139933765801792] Loss (name: value) total: 6.50835217758\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:18:59 INFO 139933765801792] Loss (name: value) kld: 0.214177027429\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:18:59 INFO 139933765801792] Loss (name: value) recons: 6.29417514564\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:18:59 INFO 139933765801792] Loss (name: value) logppx: 6.50835217758\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:18:59 INFO 139933765801792] #validation_score (19): 6.50835217758\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:18:59 INFO 139933765801792] patience losses:[6.5108815714780501, 6.5104396994388631, 6.511916673030826, 6.5109911087057837, 6.5077866551547094] min patience loss:6.50778665515 current loss:6.50835217758 absolute loss difference:0.000565522425884\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:18:59 INFO 139933765801792] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:18:59 INFO 139933765801792] Timing: train: 108.40s, val: 9.80s, epoch: 118.20s\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:18:59 INFO 139933765801792] #progress_metric: host=algo-1, completed 76 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 4219, \"sum\": 4219.0, \"min\": 4219}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 4219, \"sum\": 4219.0, \"min\": 4219}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 540000, \"sum\": 540000.0, \"min\": 540000}, \"Total Batches Seen\": {\"count\": 1, \"max\": 80161, \"sum\": 80161.0, \"min\": 80161}, \"Total Records Seen\": {\"count\": 1, \"max\": 10260000, \"sum\": 10260000.0, \"min\": 10260000}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 540000, \"sum\": 540000.0, \"min\": 540000}, \"Reset Count\": {\"count\": 1, \"max\": 38, \"sum\": 38.0, \"min\": 38}}, \"EndTime\": 1556212739.818438, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 18}, \"StartTime\": 1556212621.62094}\n",
      "\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:18:59 INFO 139933765801792] #throughput_metric: host=algo-1, train throughput=4568.61936918 records/second\u001b[0m\n",
      "\u001b[31m[2019-04-25 17:18:59.818] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 19, \"duration\": 118197, \"num_examples\": 4219}\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:18:59 INFO 139933765801792] \u001b[0m\n",
      "\u001b[31m[04/25/2019 17:18:59 INFO 139933765801792] # Starting training for epoch 20\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[04/25/2019 17:20:44 INFO 139933765801792] # Finished training epoch 20 on 540000 examples from 4219 batches, each of size 128.\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:20:44 INFO 139933765801792] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:20:44 INFO 139933765801792] Loss (name: value) total: 6.50910460483\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:20:44 INFO 139933765801792] Loss (name: value) kld: 0.219079769549\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:20:44 INFO 139933765801792] Loss (name: value) recons: 6.290024833\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:20:44 INFO 139933765801792] Loss (name: value) logppx: 6.50910460483\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:20:44 INFO 139933765801792] #quality_metric: host=algo-1, epoch=20, train total_loss <loss>=6.50910460483\u001b[0m\n",
      "\u001b[31m[2019-04-25 17:20:44.999] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 19, \"duration\": 114982, \"num_examples\": 1407}\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:20:55 INFO 139933765801792] Finished scoring on 179968 examples from 1406 batches, each of size 128.\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:20:55 INFO 139933765801792] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:20:55 INFO 139933765801792] Loss (name: value) total: 6.50939694475\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:20:55 INFO 139933765801792] Loss (name: value) kld: 0.214546025753\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:20:55 INFO 139933765801792] Loss (name: value) recons: 6.29485091715\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:20:55 INFO 139933765801792] Loss (name: value) logppx: 6.50939694475\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:20:55 INFO 139933765801792] #validation_score (20): 6.50939694475\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:20:55 INFO 139933765801792] patience losses:[6.5104396994388631, 6.511916673030826, 6.5109911087057837, 6.5077866551547094, 6.5083521775805933] min patience loss:6.50778665515 current loss:6.50939694475 absolute loss difference:0.00161028959673\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:20:55 INFO 139933765801792] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:20:55 INFO 139933765801792] Timing: train: 105.18s, val: 10.01s, epoch: 115.19s\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:20:55 INFO 139933765801792] #progress_metric: host=algo-1, completed 80 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 4219, \"sum\": 4219.0, \"min\": 4219}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 4219, \"sum\": 4219.0, \"min\": 4219}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 540000, \"sum\": 540000.0, \"min\": 540000}, \"Total Batches Seen\": {\"count\": 1, \"max\": 84380, \"sum\": 84380.0, \"min\": 84380}, \"Total Records Seen\": {\"count\": 1, \"max\": 10800000, \"sum\": 10800000.0, \"min\": 10800000}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 540000, \"sum\": 540000.0, \"min\": 540000}, \"Reset Count\": {\"count\": 1, \"max\": 40, \"sum\": 40.0, \"min\": 40}}, \"EndTime\": 1556212855.008802, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 19}, \"StartTime\": 1556212739.818706}\n",
      "\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:20:55 INFO 139933765801792] #throughput_metric: host=algo-1, train throughput=4687.89749307 records/second\u001b[0m\n",
      "\u001b[31m[2019-04-25 17:20:55.009] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 20, \"duration\": 115189, \"num_examples\": 4219}\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:20:55 INFO 139933765801792] \u001b[0m\n",
      "\u001b[31m[04/25/2019 17:20:55 INFO 139933765801792] # Starting training for epoch 21\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:22:41 INFO 139933765801792] # Finished training epoch 21 on 540000 examples from 4219 batches, each of size 128.\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:22:41 INFO 139933765801792] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:22:41 INFO 139933765801792] Loss (name: value) total: 6.50749600355\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:22:41 INFO 139933765801792] Loss (name: value) kld: 0.22343924808\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:22:41 INFO 139933765801792] Loss (name: value) recons: 6.28405675574\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:22:41 INFO 139933765801792] Loss (name: value) logppx: 6.50749600355\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:22:41 INFO 139933765801792] #quality_metric: host=algo-1, epoch=21, train total_loss <loss>=6.50749600355\u001b[0m\n",
      "\u001b[31m[2019-04-25 17:22:41.648] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 20, \"duration\": 116649, \"num_examples\": 1407}\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:22:51 INFO 139933765801792] Finished scoring on 179968 examples from 1406 batches, each of size 128.\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:22:51 INFO 139933765801792] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:22:51 INFO 139933765801792] Loss (name: value) total: 6.50700704065\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:22:51 INFO 139933765801792] Loss (name: value) kld: 0.218297337104\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:22:51 INFO 139933765801792] Loss (name: value) recons: 6.2887097063\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:22:51 INFO 139933765801792] Loss (name: value) logppx: 6.50700704065\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:22:51 INFO 139933765801792] #validation_score (21): 6.50700704065\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:22:51 INFO 139933765801792] patience losses:[6.511916673030826, 6.5109911087057837, 6.5077866551547094, 6.5083521775805933, 6.5093969447514413] min patience loss:6.50778665515 current loss:6.50700704065 absolute loss difference:0.000779614509593\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:22:51 INFO 139933765801792] Bad epoch: loss has not improved (enough). Bad count:3\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:22:51 INFO 139933765801792] Timing: train: 106.64s, val: 10.18s, epoch: 116.82s\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:22:51 INFO 139933765801792] #progress_metric: host=algo-1, completed 84 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 4219, \"sum\": 4219.0, \"min\": 4219}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 4219, \"sum\": 4219.0, \"min\": 4219}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 540000, \"sum\": 540000.0, \"min\": 540000}, \"Total Batches Seen\": {\"count\": 1, \"max\": 88599, \"sum\": 88599.0, \"min\": 88599}, \"Total Records Seen\": {\"count\": 1, \"max\": 11340000, \"sum\": 11340000.0, \"min\": 11340000}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 540000, \"sum\": 540000.0, \"min\": 540000}, \"Reset Count\": {\"count\": 1, \"max\": 42, \"sum\": 42.0, \"min\": 42}}, \"EndTime\": 1556212971.82783, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 20}, \"StartTime\": 1556212855.009066}\n",
      "\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:22:51 INFO 139933765801792] #throughput_metric: host=algo-1, train throughput=4622.5395949 records/second\u001b[0m\n",
      "\u001b[31m[2019-04-25 17:22:51.828] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 21, \"duration\": 116818, \"num_examples\": 4219}\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:22:51 INFO 139933765801792] \u001b[0m\n",
      "\u001b[31m[04/25/2019 17:22:51 INFO 139933765801792] # Starting training for epoch 22\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:24:36 INFO 139933765801792] # Finished training epoch 22 on 540000 examples from 4219 batches, each of size 128.\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:24:36 INFO 139933765801792] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:24:36 INFO 139933765801792] Loss (name: value) total: 6.50611857674\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:24:36 INFO 139933765801792] Loss (name: value) kld: 0.226700102161\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:24:36 INFO 139933765801792] Loss (name: value) recons: 6.27941847062\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:24:36 INFO 139933765801792] Loss (name: value) logppx: 6.50611857674\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:24:36 INFO 139933765801792] #quality_metric: host=algo-1, epoch=22, train total_loss <loss>=6.50611857674\u001b[0m\n",
      "\u001b[31m[2019-04-25 17:24:36.387] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 21, \"duration\": 114738, \"num_examples\": 1407}\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:24:46 INFO 139933765801792] Finished scoring on 179968 examples from 1406 batches, each of size 128.\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:24:46 INFO 139933765801792] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:24:46 INFO 139933765801792] Loss (name: value) total: 6.5073512757\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:24:46 INFO 139933765801792] Loss (name: value) kld: 0.228803195716\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:24:46 INFO 139933765801792] Loss (name: value) recons: 6.27854807753\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:24:46 INFO 139933765801792] Loss (name: value) logppx: 6.5073512757\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:24:46 INFO 139933765801792] #validation_score (22): 6.5073512757\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:24:46 INFO 139933765801792] patience losses:[6.5109911087057837, 6.5077866551547094, 6.5083521775805933, 6.5093969447514413, 6.5070070406451164] min patience loss:6.50700704065 current loss:6.5073512757 absolute loss difference:0.000344235052594\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:24:46 INFO 139933765801792] Bad epoch: loss has not improved (enough). Bad count:4\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:24:46 INFO 139933765801792] Timing: train: 104.56s, val: 9.77s, epoch: 114.33s\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:24:46 INFO 139933765801792] #progress_metric: host=algo-1, completed 88 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 4219, \"sum\": 4219.0, \"min\": 4219}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 4219, \"sum\": 4219.0, \"min\": 4219}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 540000, \"sum\": 540000.0, \"min\": 540000}, \"Total Batches Seen\": {\"count\": 1, \"max\": 92818, \"sum\": 92818.0, \"min\": 92818}, \"Total Records Seen\": {\"count\": 1, \"max\": 11880000, \"sum\": 11880000.0, \"min\": 11880000}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 540000, \"sum\": 540000.0, \"min\": 540000}, \"Reset Count\": {\"count\": 1, \"max\": 44, \"sum\": 44.0, \"min\": 44}}, \"EndTime\": 1556213086.155478, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 21}, \"StartTime\": 1556212971.828118}\n",
      "\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:24:46 INFO 139933765801792] #throughput_metric: host=algo-1, train throughput=4723.27313761 records/second\u001b[0m\n",
      "\u001b[31m[2019-04-25 17:24:46.155] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 22, \"duration\": 114327, \"num_examples\": 4219}\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:24:46 INFO 139933765801792] \u001b[0m\n",
      "\u001b[31m[04/25/2019 17:24:46 INFO 139933765801792] # Starting training for epoch 23\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[04/25/2019 17:26:32 INFO 139933765801792] # Finished training epoch 23 on 540000 examples from 4219 batches, each of size 128.\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:26:32 INFO 139933765801792] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:26:32 INFO 139933765801792] Loss (name: value) total: 6.50571492081\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:26:32 INFO 139933765801792] Loss (name: value) kld: 0.22825256429\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:26:32 INFO 139933765801792] Loss (name: value) recons: 6.27746235643\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:26:32 INFO 139933765801792] Loss (name: value) logppx: 6.50571492081\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:26:32 INFO 139933765801792] #quality_metric: host=algo-1, epoch=23, train total_loss <loss>=6.50571492081\u001b[0m\n",
      "\u001b[31m[2019-04-25 17:26:32.485] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 22, \"duration\": 116096, \"num_examples\": 1407}\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:26:42 INFO 139933765801792] Finished scoring on 179968 examples from 1406 batches, each of size 128.\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:26:42 INFO 139933765801792] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:26:42 INFO 139933765801792] Loss (name: value) total: 6.5062060095\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:26:42 INFO 139933765801792] Loss (name: value) kld: 0.224232491185\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:26:42 INFO 139933765801792] Loss (name: value) recons: 6.2819735234\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:26:42 INFO 139933765801792] Loss (name: value) logppx: 6.5062060095\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:26:42 INFO 139933765801792] #validation_score (23): 6.5062060095\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:26:42 INFO 139933765801792] patience losses:[6.5077866551547094, 6.5083521775805933, 6.5093969447514413, 6.5070070406451164, 6.5073512756977108] min patience loss:6.50700704065 current loss:6.5062060095 absolute loss difference:0.000801031145229\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:26:42 INFO 139933765801792] Bad epoch: loss has not improved (enough). Bad count:5\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:26:42 INFO 139933765801792] Timing: train: 106.33s, val: 9.97s, epoch: 116.30s\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:26:42 INFO 139933765801792] #progress_metric: host=algo-1, completed 92 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 4219, \"sum\": 4219.0, \"min\": 4219}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 4219, \"sum\": 4219.0, \"min\": 4219}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 540000, \"sum\": 540000.0, \"min\": 540000}, \"Total Batches Seen\": {\"count\": 1, \"max\": 97037, \"sum\": 97037.0, \"min\": 97037}, \"Total Records Seen\": {\"count\": 1, \"max\": 12420000, \"sum\": 12420000.0, \"min\": 12420000}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 540000, \"sum\": 540000.0, \"min\": 540000}, \"Reset Count\": {\"count\": 1, \"max\": 46, \"sum\": 46.0, \"min\": 46}}, \"EndTime\": 1556213202.453394, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 22}, \"StartTime\": 1556213086.155749}\n",
      "\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:26:42 INFO 139933765801792] #throughput_metric: host=algo-1, train throughput=4643.25274777 records/second\u001b[0m\n",
      "\u001b[31m[2019-04-25 17:26:42.453] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 23, \"duration\": 116297, \"num_examples\": 4219}\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:26:42 INFO 139933765801792] \u001b[0m\n",
      "\u001b[31m[04/25/2019 17:26:42 INFO 139933765801792] # Starting training for epoch 24\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:28:30 INFO 139933765801792] # Finished training epoch 24 on 540000 examples from 4219 batches, each of size 128.\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:28:30 INFO 139933765801792] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:28:30 INFO 139933765801792] Loss (name: value) total: 6.50539207447\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:28:30 INFO 139933765801792] Loss (name: value) kld: 0.228493634987\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:28:30 INFO 139933765801792] Loss (name: value) recons: 6.27689843428\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:28:30 INFO 139933765801792] Loss (name: value) logppx: 6.50539207447\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:28:30 INFO 139933765801792] #quality_metric: host=algo-1, epoch=24, train total_loss <loss>=6.50539207447\u001b[0m\n",
      "\u001b[31m[2019-04-25 17:28:30.179] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 23, \"duration\": 117692, \"num_examples\": 1407}\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:28:39 INFO 139933765801792] Finished scoring on 179968 examples from 1406 batches, each of size 128.\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:28:39 INFO 139933765801792] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:28:39 INFO 139933765801792] Loss (name: value) total: 6.50480696555\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:28:39 INFO 139933765801792] Loss (name: value) kld: 0.232969379347\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:28:39 INFO 139933765801792] Loss (name: value) recons: 6.27183758382\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:28:39 INFO 139933765801792] Loss (name: value) logppx: 6.50480696555\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:28:39 INFO 139933765801792] #validation_score (24): 6.50480696555\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:28:39 INFO 139933765801792] patience losses:[6.5083521775805933, 6.5093969447514413, 6.5070070406451164, 6.5073512756977108, 6.5062060094998877] min patience loss:6.5062060095 current loss:6.50480696555 absolute loss difference:0.00139904395276\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:28:39 INFO 139933765801792] Timing: train: 107.73s, val: 9.71s, epoch: 117.43s\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:28:39 INFO 139933765801792] #progress_metric: host=algo-1, completed 96 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 4219, \"sum\": 4219.0, \"min\": 4219}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 4219, \"sum\": 4219.0, \"min\": 4219}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 540000, \"sum\": 540000.0, \"min\": 540000}, \"Total Batches Seen\": {\"count\": 1, \"max\": 101256, \"sum\": 101256.0, \"min\": 101256}, \"Total Records Seen\": {\"count\": 1, \"max\": 12960000, \"sum\": 12960000.0, \"min\": 12960000}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 540000, \"sum\": 540000.0, \"min\": 540000}, \"Reset Count\": {\"count\": 1, \"max\": 48, \"sum\": 48.0, \"min\": 48}}, \"EndTime\": 1556213319.887511, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 23}, \"StartTime\": 1556213202.453702}\n",
      "\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:28:39 INFO 139933765801792] #throughput_metric: host=algo-1, train throughput=4598.32972718 records/second\u001b[0m\n",
      "\u001b[31m[2019-04-25 17:28:39.887] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 24, \"duration\": 117433, \"num_examples\": 4219}\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:28:39 INFO 139933765801792] \u001b[0m\n",
      "\u001b[31m[04/25/2019 17:28:39 INFO 139933765801792] # Starting training for epoch 25\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:30:26 INFO 139933765801792] # Finished training epoch 25 on 540000 examples from 4219 batches, each of size 128.\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:30:26 INFO 139933765801792] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:30:26 INFO 139933765801792] Loss (name: value) total: 6.50507815332\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:30:26 INFO 139933765801792] Loss (name: value) kld: 0.22878139617\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:30:26 INFO 139933765801792] Loss (name: value) recons: 6.27629676083\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:30:26 INFO 139933765801792] Loss (name: value) logppx: 6.50507815332\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:30:26 INFO 139933765801792] #quality_metric: host=algo-1, epoch=25, train total_loss <loss>=6.50507815332\u001b[0m\n",
      "\u001b[31m[2019-04-25 17:30:26.872] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 24, \"duration\": 116693, \"num_examples\": 1407}\u001b[0m\n",
      "\u001b[32m[2019-04-25 17:30:36.805] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 11, \"duration\": 1616951, \"num_examples\": 1407}\u001b[0m\n",
      "\u001b[33m[2019-04-25 17:30:36.805] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 16, \"duration\": 1823738, \"num_examples\": 1407}\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:30:36 INFO 139933765801792] Finished scoring on 179968 examples from 1406 batches, each of size 128.\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:30:36 INFO 139933765801792] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:30:36 INFO 139933765801792] Loss (name: value) total: 6.50616756171\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:30:36 INFO 139933765801792] Loss (name: value) kld: 0.237721946446\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:30:36 INFO 139933765801792] Loss (name: value) recons: 6.26844561219\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:30:36 INFO 139933765801792] Loss (name: value) logppx: 6.50616756171\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:30:36 INFO 139933765801792] #validation_score (25): 6.50616756171\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:30:36 INFO 139933765801792] patience losses:[6.5093969447514413, 6.5070070406451164, 6.5073512756977108, 6.5062060094998877, 6.5048069655471306] min patience loss:6.50480696555 current loss:6.50616756171 absolute loss difference:0.00136059615893\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:30:36 INFO 139933765801792] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:30:36 INFO 139933765801792] Timing: train: 106.99s, val: 9.93s, epoch: 116.92s\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:30:36 INFO 139933765801792] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 4219, \"sum\": 4219.0, \"min\": 4219}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 4219, \"sum\": 4219.0, \"min\": 4219}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 540000, \"sum\": 540000.0, \"min\": 540000}, \"Total Batches Seen\": {\"count\": 1, \"max\": 105475, \"sum\": 105475.0, \"min\": 105475}, \"Total Records Seen\": {\"count\": 1, \"max\": 13500000, \"sum\": 13500000.0, \"min\": 13500000}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 540000, \"sum\": 540000.0, \"min\": 540000}, \"Reset Count\": {\"count\": 1, \"max\": 50, \"sum\": 50.0, \"min\": 50}}, \"EndTime\": 1556213436.803911, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 24}, \"StartTime\": 1556213319.8878}\n",
      "\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:30:36 INFO 139933765801792] #throughput_metric: host=algo-1, train throughput=4618.69053206 records/second\u001b[0m\n",
      "\u001b[31m[2019-04-25 17:30:36.805] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 25, \"duration\": 9931, \"num_examples\": 1407}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2019-04-25 17:30:59 Uploading - Uploading generated training model\n",
      "2019-04-25 17:30:59 Completed - Training job completed\n",
      "\u001b[32m[04/25/2019 17:30:46 INFO 139904211511104] Finished scoring on 179968 examples from 1406 batches, each of size 128.\u001b[0m\n",
      "\u001b[32m[04/25/2019 17:30:46 INFO 139904211511104] Metrics for Inference:\u001b[0m\n",
      "\u001b[32m[04/25/2019 17:30:46 INFO 139904211511104] Loss (name: value) total: 6.51353128197\u001b[0m\n",
      "\u001b[32m[04/25/2019 17:30:46 INFO 139904211511104] Loss (name: value) kld: 0.207419769587\u001b[0m\n",
      "\u001b[32m[04/25/2019 17:30:46 INFO 139904211511104] Loss (name: value) recons: 6.30611151347\u001b[0m\n",
      "\u001b[32m[04/25/2019 17:30:46 INFO 139904211511104] Loss (name: value) logppx: 6.51353128197\u001b[0m\n",
      "\u001b[32m[04/25/2019 17:30:46 INFO 139904211511104] #quality_metric: host=algo-2, epoch=11, validation total_loss <loss>=6.51353128197\u001b[0m\n",
      "\u001b[32m[04/25/2019 17:30:46 INFO 139904211511104] Loss of server-side model: 6.51353128197\u001b[0m\n",
      "\u001b[32m[04/25/2019 17:30:46 INFO 139904211511104] Best model based on early stopping at epoch 5. Best loss: 6.51043439861\u001b[0m\n",
      "\u001b[32m[04/25/2019 17:30:46 INFO 139904211511104] Topics from epoch:final (num_topics:10) [, tu 0.72]:\u001b[0m\n",
      "\u001b[32m[04/25/2019 17:30:46 INFO 139904211511104] 1362 1987 577 989 1902 542 1484 1876 1955 1616 933 1291 1085 1335 1122 1233 1986 1320 328 512\u001b[0m\n",
      "\u001b[32m[04/25/2019 17:30:46 INFO 139904211511104] 1993 493 1815 365 1969 1567 1360 708 1116 231 1437 419 163 1525 742 1623 28 455 262 406\u001b[0m\n",
      "\u001b[32m[04/25/2019 17:30:46 INFO 139904211511104] 577 1000 1608 1902 665 1362 542 1987 1746 989 248 495 499 1108 1925 354 1710 1484 546 1377\u001b[0m\n",
      "\u001b[32m[04/25/2019 17:30:46 INFO 139904211511104] 1362 542 933 1633 1431 175 338 1619 1585 1587 12 14 1352 15 1904 667 1072 1624 1377 1586\u001b[0m\n",
      "\u001b[32m[04/25/2019 17:30:46 INFO 139904211511104] 749 303 215 310 44 1524 399 1680 946 1800 1794 940 1922 748 460 272 564 1368 1215 1044\u001b[0m\n",
      "\u001b[32m[04/25/2019 17:30:46 INFO 139904211511104] 577 455 499 1987 187 542 495 390 1746 458 248 1377 665 1871 1753 1035 288 1873 254 1902\u001b[0m\n",
      "\u001b[32m[04/25/2019 17:30:46 INFO 139904211511104] 577 665 869 1987 1608 187 455 1926 458 1108 1746 1437 1925 1902 248 1772 390 1937 547 510\u001b[0m\n",
      "\u001b[32m[04/25/2019 17:30:46 INFO 139904211511104] 642 577 807 1342 1822 998 1163 1479 1333 1135 1553 1218 800 1808 1429 145 1731 268 1430 1853\u001b[0m\n",
      "\u001b[32m[04/25/2019 17:30:46 INFO 139904211511104] 1421 175 1894 923 536 244 1535 1397 1706 1217 1892 547 598 1398 546 1404 411 1562 1255 1190\u001b[0m\n",
      "\u001b[32m[04/25/2019 17:30:46 INFO 139904211511104] 577 1987 1746 455 1608 542 546 390 1000 1362 187 1753 1925 1597 869 760 1785 547 1676 1902\u001b[0m\n",
      "\u001b[32m[04/25/2019 17:30:46 INFO 139904211511104] Serializing model to /opt/ml/model/model_algo-2\u001b[0m\n",
      "\u001b[32m[04/25/2019 17:30:46 INFO 139904211511104] Saved checkpoint to \"/tmp/tmpcrurk0/state-0001.params\"\u001b[0m\n",
      "\u001b[32m[04/25/2019 17:30:46 INFO 139904211511104] Test data is not provided.\u001b[0m\n",
      "\u001b[32m[2019-04-25 17:30:46.862] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 11, \"duration\": 1760043, \"num_examples\": 4219}\u001b[0m\n",
      "\u001b[32m[2019-04-25 17:30:46.862] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"duration\": 3418341, \"num_epochs\": 12, \"num_examples\": 46410}\u001b[0m\n",
      "\u001b[32m[2019-04-25 17:30:46.865] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 12, \"duration\": 10058, \"num_examples\": 1407}\u001b[0m\n",
      "\u001b[32m[2019-04-25 17:30:46.865] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"duration\": 3418337, \"num_epochs\": 13, \"num_examples\": 16885}\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 3418454.955101013, \"sum\": 3418454.955101013, \"min\": 3418454.955101013}, \"finalize.time\": {\"count\": 1, \"max\": 10055.208921432495, \"sum\": 10055.208921432495, \"min\": 10055.208921432495}, \"initialize.time\": {\"count\": 1, \"max\": 546.6270446777344, \"sum\": 546.6270446777344, \"min\": 546.6270446777344}, \"model.serialize.time\": {\"count\": 1, \"max\": 2.490997314453125, \"sum\": 2.490997314453125, \"min\": 2.490997314453125}, \"setuptime\": {\"count\": 1, \"max\": 62.458038330078125, \"sum\": 62.458038330078125, \"min\": 62.458038330078125}, \"early_stop.time\": {\"count\": 11, \"max\": 15637.379884719849, \"sum\": 161596.37022018433, \"min\": 12627.293825149536}, \"update.time\": {\"count\": 11, \"max\": 168162.56189346313, \"sum\": 1805673.3846664429, \"min\": 147927.29878425598}, \"epochs\": {\"count\": 1, \"max\": 25, \"sum\": 25.0, \"min\": 25}, \"model.score.time\": {\"count\": 12, \"max\": 15636.997938156128, \"sum\": 171584.85293388367, \"min\": 10012.353897094727}}, \"EndTime\": 1556213446.866218, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\"}, \"StartTime\": 1556210028.507985}\n",
      "\u001b[0m\n",
      "\u001b[33m[04/25/2019 17:30:46 INFO 140590214039360] Finished scoring on 179968 examples from 1406 batches, each of size 128.\u001b[0m\n",
      "\u001b[33m[04/25/2019 17:30:46 INFO 140590214039360] Metrics for Inference:\u001b[0m\n",
      "\u001b[33m[04/25/2019 17:30:46 INFO 140590214039360] Loss (name: value) total: 6.51403740188\u001b[0m\n",
      "\u001b[33m[04/25/2019 17:30:46 INFO 140590214039360] Loss (name: value) kld: 0.207419769587\u001b[0m\n",
      "\u001b[33m[04/25/2019 17:30:46 INFO 140590214039360] Loss (name: value) recons: 6.30661762897\u001b[0m\n",
      "\u001b[33m[04/25/2019 17:30:46 INFO 140590214039360] Loss (name: value) logppx: 6.51403740188\u001b[0m\n",
      "\u001b[33m[04/25/2019 17:30:46 INFO 140590214039360] #quality_metric: host=algo-3, epoch=16, validation total_loss <loss>=6.51403740188\u001b[0m\n",
      "\u001b[33m[04/25/2019 17:30:46 INFO 140590214039360] Loss of server-side model: 6.51403740188\u001b[0m\n",
      "\u001b[33m[04/25/2019 17:30:46 INFO 140590214039360] Best model based on early stopping at epoch 10. Best loss: 6.51140997759\u001b[0m\n",
      "\u001b[33m[04/25/2019 17:30:46 INFO 140590214039360] Topics from epoch:final (num_topics:10) [, tu 0.70]:\u001b[0m\n",
      "\u001b[33m[04/25/2019 17:30:46 INFO 140590214039360] 1362 1987 577 1902 989 542 1484 1876 1955 933 1291 1986 1085 1335 1616 328 1320 512 1233 1033\u001b[0m\n",
      "\u001b[33m[04/25/2019 17:30:46 INFO 140590214039360] 1993 493 1567 1815 365 1360 1969 1437 708 231 163 1509 1116 28 1525 419 455 406 94 280\u001b[0m\n",
      "\u001b[33m[04/25/2019 17:30:46 INFO 140590214039360] 455 577 542 1608 546 1362 547 1421 665 1746 1987 869 1747 495 390 1902 1701 778 254 92\u001b[0m\n",
      "\u001b[33m[04/25/2019 17:30:46 INFO 140590214039360] 1362 542 933 1633 1431 175 338 1619 14 12 1904 15 1072 1587 1352 1585 1624 1586 667 1377\u001b[0m\n",
      "\u001b[33m[04/25/2019 17:30:46 INFO 140590214039360] 215 44 749 303 1800 399 310 940 784 1524 1680 946 1922 1368 1794 1215 272 171 1044 1666\u001b[0m\n",
      "\u001b[33m[04/25/2019 17:30:46 INFO 140590214039360] 577 542 665 760 1772 1362 844 546 1746 1455 1608 458 736 547 572 1000 778 68 499 495\u001b[0m\n",
      "\u001b[33m[04/25/2019 17:30:46 INFO 140590214039360] 577 542 1362 1608 1000 455 495 869 546 187 1987 1772 760 547 1902 1578 1421 1484 458 786\u001b[0m\n",
      "\u001b[33m[04/25/2019 17:30:46 INFO 140590214039360] 577 642 1342 1822 998 807 1333 1163 1479 800 1135 145 1553 1218 268 451 1429 1900 1731 1808\u001b[0m\n",
      "\u001b[33m[04/25/2019 17:30:46 INFO 140590214039360] 175 1421 923 1894 536 1706 1535 244 1217 598 1892 1562 547 1397 1398 1404 84 411 546 1255\u001b[0m\n",
      "\u001b[33m[04/25/2019 17:30:46 INFO 140590214039360] 577 1362 1421 542 1987 531 455 495 1608 1747 310 869 187 547 546 1753 1746 1000 1377 1902\u001b[0m\n",
      "\u001b[33m[04/25/2019 17:30:46 INFO 140590214039360] Serializing model to /opt/ml/model/model_algo-3\u001b[0m\n",
      "\u001b[33m[04/25/2019 17:30:46 INFO 140590214039360] Saved checkpoint to \"/tmp/tmpsNCV6O/state-0001.params\"\u001b[0m\n",
      "\u001b[33m[04/25/2019 17:30:46 INFO 140590214039360] Test data is not provided.\u001b[0m\n",
      "\u001b[33m[2019-04-25 17:30:46.851] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 16, \"duration\": 1923850, \"num_examples\": 2813}\u001b[0m\n",
      "\u001b[33m[2019-04-25 17:30:46.851] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"duration\": 3421077, \"num_epochs\": 17, \"num_examples\": 45009}\u001b[0m\n",
      "\u001b[33m[2019-04-25 17:30:46.852] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 17, \"duration\": 10047, \"num_examples\": 1407}\u001b[0m\n",
      "\u001b[33m[2019-04-25 17:30:46.852] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"duration\": 3421076, \"num_epochs\": 18, \"num_examples\": 23920}\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 3421682.6441287994, \"sum\": 3421682.6441287994, \"min\": 3421682.6441287994}, \"finalize.time\": {\"count\": 1, \"max\": 10043.990135192871, \"sum\": 10043.990135192871, \"min\": 10043.990135192871}, \"initialize.time\": {\"count\": 1, \"max\": 3301.2208938598633, \"sum\": 3301.2208938598633, \"min\": 3301.2208938598633}, \"model.serialize.time\": {\"count\": 1, \"max\": 2.6350021362304688, \"sum\": 2.6350021362304688, \"min\": 2.6350021362304688}, \"setuptime\": {\"count\": 1, \"max\": 553.6990165710449, \"sum\": 553.6990165710449, \"min\": 553.6990165710449}, \"early_stop.time\": {\"count\": 16, \"max\": 11964.336156845093, \"sum\": 186258.88347625732, \"min\": 11169.774055480957}, \"update.time\": {\"count\": 16, \"max\": 102309.60607528687, \"sum\": 1595417.5350666046, \"min\": 95253.8709640503}, \"epochs\": {\"count\": 1, \"max\": 25, \"sum\": 25.0, \"min\": 25}, \"model.score.time\": {\"count\": 17, \"max\": 11963.92297744751, \"sum\": 196214.15042877197, \"min\": 9998.679161071777}}, \"EndTime\": 1556213446.854863, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\"}, \"StartTime\": 1556210025.757981}\n",
      "\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:30:46 INFO 139933765801792] Finished scoring on 179968 examples from 1406 batches, each of size 128.\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:30:46 INFO 139933765801792] Metrics for Inference:\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:30:46 INFO 139933765801792] Loss (name: value) total: 6.51291723407\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:30:46 INFO 139933765801792] Loss (name: value) kld: 0.207419769587\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:30:46 INFO 139933765801792] Loss (name: value) recons: 6.30549746319\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:30:46 INFO 139933765801792] Loss (name: value) logppx: 6.51291723407\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:30:46 INFO 139933765801792] #quality_metric: host=algo-1, epoch=25, validation total_loss <loss>=6.51291723407\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:30:46 INFO 139933765801792] Loss of server-side model: 6.51291723407\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:30:46 INFO 139933765801792] Best model based on early stopping at epoch 24. Best loss: 6.50480696555\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:30:46 INFO 139933765801792] Topics from epoch:final (num_topics:10) [, tu 0.70]:\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:30:46 INFO 139933765801792] 1362 989 577 1902 542 1987 1484 1986 1955 1876 933 1291 1335 1616 1085 1233 1320 1033 365 1122\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:30:46 INFO 139933765801792] 1993 493 1437 1567 455 1509 1815 1436 1969 1999 708 1360 1929 1120 667 1900 231 12 365 163\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:30:46 INFO 139933765801792] 1362 577 542 785 784 665 1405 1876 272 524 227 891 1969 1639 760 458 141 456 1122 1631\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:30:46 INFO 139933765801792] 1362 542 933 1633 1431 1619 1377 175 1586 1587 1072 1031 1585 1624 902 1904 45 338 1352 12\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:30:46 INFO 139933765801792] 215 44 784 303 1680 749 946 1403 1524 1794 1368 785 989 1484 399 1922 1800 272 1215 460\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:30:46 INFO 139933765801792] 1362 577 542 784 1405 785 891 665 272 141 524 227 1746 590 760 994 19 494 589 413\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:30:46 INFO 139933765801792] 1362 577 784 542 785 272 891 665 1969 1405 590 207 760 227 1639 458 524 274 933 371\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:30:46 INFO 139933765801792] 577 1822 542 1362 642 1342 1218 807 1429 998 1853 1333 1163 933 1479 1771 107 268 155 1430\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:30:46 INFO 139933765801792] 175 1421 923 1706 536 1894 1535 244 1892 1397 598 1562 547 1217 546 1398 84 1404 355 1382\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:30:46 INFO 139933765801792] 577 310 455 461 1377 869 1089 989 665 390 1608 1044 903 1405 531 187 546 1362 1746 1987\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:30:46 INFO 139933765801792] Serializing model to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:30:46 INFO 139933765801792] Saved checkpoint to \"/tmp/tmpjcU2aT/state-0001.params\"\u001b[0m\n",
      "\u001b[31m[04/25/2019 17:30:46 INFO 139933765801792] Test data is not provided.\u001b[0m\n",
      "\u001b[31m[2019-04-25 17:30:46.660] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 25, \"duration\": 126772, \"num_examples\": 4219}\u001b[0m\n",
      "\u001b[31m[2019-04-25 17:30:46.660] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"duration\": 3420631, \"num_epochs\": 26, \"num_examples\": 105476}\u001b[0m\n",
      "\u001b[31m[2019-04-25 17:30:46.663] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"epoch\": 26, \"duration\": 9847, \"num_examples\": 1407}\u001b[0m\n",
      "\u001b[31m[2019-04-25 17:30:46.663] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/validation\", \"duration\": 3420591, \"num_epochs\": 27, \"num_examples\": 36583}\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 3420759.3829631805, \"sum\": 3420759.3829631805, \"min\": 3420759.3829631805}, \"finalize.time\": {\"count\": 1, \"max\": 9853.302001953125, \"sum\": 9853.302001953125, \"min\": 9853.302001953125}, \"initialize.time\": {\"count\": 1, \"max\": 3006.2220096588135, \"sum\": 3006.2220096588135, \"min\": 3006.2220096588135}, \"model.serialize.time\": {\"count\": 1, \"max\": 2.4721622467041016, \"sum\": 2.4721622467041016, \"min\": 2.4721622467041016}, \"setuptime\": {\"count\": 1, \"max\": 53.06506156921387, \"sum\": 53.06506156921387, \"min\": 53.06506156921387}, \"early_stop.time\": {\"count\": 25, \"max\": 15893.362045288086, \"sum\": 306892.7299976349, \"min\": 9708.011865615845}, \"update.time\": {\"count\": 25, \"max\": 166559.27300453186, \"sum\": 3407724.6940135956, \"min\": 114231.66084289551}, \"epochs\": {\"count\": 1, \"max\": 25, \"sum\": 25.0, \"min\": 25}, \"model.score.time\": {\"count\": 26, \"max\": 15888.836145401001, \"sum\": 316643.8126564026, \"min\": 9703.426122665405}}, \"EndTime\": 1556213446.663789, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\"}, \"StartTime\": 1556210026.001557}\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Billable seconds: 10364\n"
     ]
    }
   ],
   "source": [
    "ntm.fit({'train': s3_train, 'validation': s3_val_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training job name: aaronmossjob-2019-04-25-16-30-31-576\n"
     ]
    }
   ],
   "source": [
    "print('Training job name: {}'.format(ntm.latest_training_job.job_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: ntm-2019-04-25-17-32-58-096\n",
      "INFO:sagemaker:Creating endpoint with name aaronmossjob-2019-04-25-16-30-31-576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "ntm_predictor = ntm.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint name: aaronmossjob-2019-04-25-16-30-31-576\n"
     ]
    }
   ],
   "source": [
    "print('Endpoint name: {}'.format(ntm_predictor.endpoint))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = sparse.csr_matrix(vectors, dtype=np.float32)\n",
    "print(type(vectors), vectors.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import csv_serializer, json_deserializer\n",
    "\n",
    "ntm_predictor.content_type = 'text/csv'\n",
    "ntm_predictor.serializer = csv_serializer\n",
    "ntm_predictor.deserializer = json_deserializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = np.array(test_vectors.todense())\n",
    "results = ntm_predictor.predict(test_data[:5])\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_full = np.array(vectors_tst.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sparse matrix length is ambiguous; use getnnz() or shape[0]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-2972a3c25f36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresults_full\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mntm_predictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/predictor.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, initial_args)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \"\"\"\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mrequest_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_request_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_runtime_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mrequest_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/predictor.py\u001b[0m in \u001b[0;36m_create_request_args\u001b[0;34m(self, data, initial_args)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserializer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Body'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/predictor.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    172\u001b[0m         \"\"\"\n\u001b[1;32m    173\u001b[0m         \u001b[0;31m# For inputs which represent multiple \"rows\", the result should be newline-separated CSV rows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0m_is_mutable_sequence_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_is_sequence_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_CsvSerializer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_serialize_row\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_CsvSerializer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_serialize_row\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;31m# non-zeros is more important.  For now, raise an exception!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m         raise TypeError(\"sparse matrix length is ambiguous; use getnnz()\"\n\u001b[0m\u001b[1;32m    297\u001b[0m                         \" or shape[0]\")\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: sparse matrix length is ambiguous; use getnnz() or shape[0]"
     ]
    }
   ],
   "source": [
    "results_full = ntm_predictor.predict(vectors1[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# full test data\n",
    "test_data_full = np.array(vectors_tst.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_full = ntm_predictor.predict(test_data_full[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predictions': [{'topic_weights': [0.1165521964, 0.0913761482, 0.0975406617, 0.0643444285, 0.2140946388, 0.0942866355, 0.0947133526, 0.1093520969, 0.0648432449, 0.052896671]}, {'topic_weights': [0.1189275458, 0.0935707539, 0.0703211725, 0.2002211511, 0.0705644712, 0.0716817304, 0.0696063042, 0.076000385, 0.0924226642, 0.1366838068]}, {'topic_weights': [0.3914327025, 0.0630526543, 0.0668352917, 0.0873404592, 0.0638733879, 0.0651632771, 0.0634139106, 0.0618334673, 0.0648173913, 0.0722375512]}, {'topic_weights': [0.1124764383, 0.068726182, 0.0732384324, 0.0970809534, 0.0599014089, 0.0714360029, 0.0711140931, 0.2476294488, 0.0747735947, 0.1236234605]}, {'topic_weights': [0.1439232677, 0.0523980334, 0.0655551329, 0.2273790985, 0.0502465479, 0.0642922372, 0.0624970198, 0.2140279114, 0.0615230426, 0.058157824]}, {'topic_weights': [0.2086798102, 0.0723338425, 0.0717713162, 0.1732192039, 0.0695170686, 0.0707703605, 0.0698544383, 0.0568669848, 0.074523747, 0.1324631423]}, {'topic_weights': [0.3616770208, 0.0531915724, 0.0545455851, 0.1243114844, 0.0502818301, 0.0530646816, 0.0512155704, 0.1083773971, 0.0554666445, 0.0878683329]}, {'topic_weights': [0.2409458458, 0.0442067161, 0.0728818178, 0.0602493323, 0.1057042927, 0.0694117919, 0.0695305392, 0.0526992455, 0.0401899517, 0.2441804409]}, {'topic_weights': [0.292445004, 0.0616807379, 0.0668206215, 0.1244357526, 0.0728306249, 0.0652338639, 0.0635512918, 0.1276092827, 0.067983821, 0.0574090257]}, {'topic_weights': [0.0659269392, 0.0588622279, 0.0742269829, 0.3751157224, 0.0513772927, 0.0736404136, 0.0729566067, 0.056210693, 0.0995929688, 0.0720901936]}]}\n"
     ]
    }
   ],
   "source": [
    "print(results_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = np.array([prediction['topic_weights'] for prediction in results_full['predictions']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1a = test[test.Label==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2a =test1a[:600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 10)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1165522  0.09137615 0.09754066 ... 0.1093521  0.06484324 0.05289667]\n",
      " [0.11892755 0.09357075 0.07032117 ... 0.07600039 0.09242268 0.13668381]\n",
      " [0.3914327  0.06305265 0.06683528 ... 0.06183347 0.06481739 0.07223755]\n",
      " ...\n",
      " [0.07170653 0.15261699 0.0713416  ... 0.0837216  0.07213472 0.18956518]\n",
      " [0.07909842 0.0752635  0.0783104  ... 0.12010554 0.10210624 0.06920021]\n",
      " [0.06447116 0.07433524 0.12615666 ... 0.16472414 0.06525113 0.07349328]]\n"
     ]
    }
   ],
   "source": [
    "print (pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,0,'Topic ID')"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fs = 12\n",
    "df=pd.DataFrame(pred_test.T)\n",
    "df.plot(kind='bar', figsize=(16,4), fontsize=fs)\n",
    "plt.ylabel('Topic assignment', fontsize=fs+2)\n",
    "plt.xlabel('Topic ID', fontsize=fs+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ability',\n",
       " 'able',\n",
       " 'absolute',\n",
       " 'absolutely',\n",
       " 'abuse',\n",
       " 'accept',\n",
       " 'acceptable',\n",
       " 'access',\n",
       " 'according',\n",
       " 'account']"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_list1[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop / Close the Endpoint\n",
    "Finally, we should delete the endpoint before we close the notebook.\n",
    "\n",
    "To restart the endpoint you can follow the code above using the same endpoint_name we created or you can navigate to the \"Endpoints\" tab in the SageMaker console, select the endpoint with the name stored in the variable endpoint_name, and select \"Delete\" from the \"Actions\" dropdown menu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.Session().delete_endpoint(ntm_predictor.endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you use conda_mxnet_p36 kernel, mxnet is already installed, otherwise, uncomment the following line to install.\n",
    "# !pip install mxnet \n",
    "import mxnet as mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(output_prefix, ntm._current_job_name, 'output/model.tar.gz')\n",
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3.resource('s3').Bucket(bucket).download_file(model_path, 'downloaded_model.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xzvf 'downloaded_model.tar.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use flag -o to overwrite previous unzipped content\n",
    "!unzip -o model_algo-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mx.ndarray.load('params')\n",
    "W = model['arg:projection_weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wordcloud\n",
    "import wordcloud as wc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_id = dict()\n",
    "for i, v in enumerate(vocab_list):\n",
    "    word_to_id[v] = i\n",
    "\n",
    "limit = 24\n",
    "n_col = 4\n",
    "counter = 0\n",
    "\n",
    "plt.figure(figsize=(20,16))\n",
    "for ind in range(num_topics):\n",
    "\n",
    "    if counter >= limit:\n",
    "        break\n",
    "\n",
    "    title_str = 'Topic{}'.format(ind)\n",
    "\n",
    "    #pvals = mx.nd.softmax(W[:, ind]).asnumpy()\n",
    "    pvals = mx.nd.softmax(mx.nd.array(W[:, ind])).asnumpy()\n",
    "\n",
    "    word_freq = dict()\n",
    "    for k in word_to_id.keys():\n",
    "        i = word_to_id[k]\n",
    "        word_freq[k] =pvals[i]\n",
    "\n",
    "    wordcloud = wc.WordCloud(background_color='white').fit_words(word_freq)\n",
    "\n",
    "    plt.subplot(limit // n_col, n_col, counter+1)\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(title_str)\n",
    "    #plt.close()\n",
    "\n",
    "    counter +=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
